{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research_trader.services.arxiv_service import ArxivService\n",
    "from research_trader.services.openai_service import OpenAIService\n",
    "from research_trader.services.cache_service import CacheService\n",
    "from research_trader.services.paper_processing_service import PaperProcessingService\n",
    "from research_trader.models.paper import Paper, PaperMetadata, PaperContent\n",
    "\n",
    "from pydantic import BaseModel, Field, HttpUrl\n",
    "from typing import List, Dict, Union, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_service = CacheService()\n",
    "openai_service = OpenAIService()\n",
    "arxiv_service = ArxivService()\n",
    "paper_processing_service = PaperProcessingService(openai_service, cache_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await arxiv_service.search_papers(\n",
    "    \"momentum based trading\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2504.15268v1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].paper_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaperMetadata(paper_id='2504.15268v1', title='Beating the Correlation Breakdown: Robust Inference, Flexible Scenarios, and Stress Testing for Financial Portfolios', authors=['JD Opdyke'], abstract='We live in a multivariate world, and effective modeling of financial\\nportfolios, including their construction, allocation, forecasting, and risk\\nanalysis, simply is not possible without explicitly modeling the dependence\\nstructure of their assets. Dependence structure can drive portfolio results\\nmore than many other parameters in investment and risk models, sometimes even\\nmore than their combined effects, but the literature provides relatively little\\nto define the finite-sample distributions of dependence measures in useable and\\nuseful ways under challenging, real-world financial data conditions. Yet this\\nis exactly what is needed to make valid inferences about their estimates, and\\nto use these inferences for a myriad of essential purposes, such as hypothesis\\ntesting, dynamic monitoring, realistic and granular scenario and reverse\\nscenario analyses, and mitigating the effects of correlation breakdowns during\\nmarket upheavals (which is when we need valid inferences the most). This work\\ndevelops a new and straightforward method, Nonparametric Angles-based\\nCorrelation (NAbC), for defining the finite-sample distributions of any\\ndependence measure whose matrix of pairwise associations is positive definite\\n(e.g. Pearsons, Kendalls Tau, Spearmans Rho, Chatterjees, Lancasters, Szekelys,\\nand their many variants). The solution remains valid under marginal asset\\ndistributions characterized by notably different and varying degrees of serial\\ncorrelation, non-stationarity, heavy-tailedness, and asymmetry. Notably, NAbCs\\np-values and confidence intervals remain analytically consistent at both the\\nmatrix level and the pairwise cell level. Finally, NAbC maintains validity even\\nwhen selected cells in the matrix are frozen for a given scenario or stress\\ntest, that is, unaffected by the scenario, thus enabling flexible, granular,\\nand realistic scenarios.', published_date=datetime.datetime(2025, 4, 21, 17, 52, 36, tzinfo=datetime.timezone.utc), pdf_url=HttpUrl('http://arxiv.org/pdf/2504.15268v1'), source_url=HttpUrl('http://arxiv.org/abs/2504.15268v1'), tags=['62-07, 62E20, 62F10, 62F12, 60E05, 60G70, 91B30', 'q-fin.PM', 'q-fin.ST', 'stat.AP', 'q-fin.RM', 'G.3'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await arxiv_service.get_paper_by_id(results[0].paper_id)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_processing_service = PaperProcessingService(openai_service, cache_service)\n",
    "processed_papers = await paper_processing_service.process_papers_batch(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'paper_id': '2504.15268v1',\n",
       "  'title': 'Beating the Correlation Breakdown: Robust Inference, Flexible Scenarios, and Stress Testing for Financial Portfolios',\n",
       "  'authors': ['JD Opdyke'],\n",
       "  'abstract': 'We live in a multivariate world, and effective modeling of financial\\nportfolios, including their construction, allocation, forecasting, and risk\\nanalysis, simply is not possible without explicitly modeling the dependence\\nstructure of their assets. Dependence structure can drive portfolio results\\nmore than many other parameters in investment and risk models, sometimes even\\nmore than their combined effects, but the literature provides relatively little\\nto define the finite-sample distributions of dependence measures in useable and\\nuseful ways under challenging, real-world financial data conditions. Yet this\\nis exactly what is needed to make valid inferences about their estimates, and\\nto use these inferences for a myriad of essential purposes, such as hypothesis\\ntesting, dynamic monitoring, realistic and granular scenario and reverse\\nscenario analyses, and mitigating the effects of correlation breakdowns during\\nmarket upheavals (which is when we need valid inferences the most). This work\\ndevelops a new and straightforward method, Nonparametric Angles-based\\nCorrelation (NAbC), for defining the finite-sample distributions of any\\ndependence measure whose matrix of pairwise associations is positive definite\\n(e.g. Pearsons, Kendalls Tau, Spearmans Rho, Chatterjees, Lancasters, Szekelys,\\nand their many variants). The solution remains valid under marginal asset\\ndistributions characterized by notably different and varying degrees of serial\\ncorrelation, non-stationarity, heavy-tailedness, and asymmetry. Notably, NAbCs\\np-values and confidence intervals remain analytically consistent at both the\\nmatrix level and the pairwise cell level. Finally, NAbC maintains validity even\\nwhen selected cells in the matrix are frozen for a given scenario or stress\\ntest, that is, unaffected by the scenario, thus enabling flexible, granular,\\nand realistic scenarios.',\n",
       "  'published_date': datetime.datetime(2025, 4, 21, 17, 52, 36, tzinfo=datetime.timezone.utc),\n",
       "  'pdf_url': HttpUrl('http://arxiv.org/pdf/2504.15268v1'),\n",
       "  'source_url': HttpUrl('http://arxiv.org/abs/2504.15268v1'),\n",
       "  'tags': ['62-07, 62E20, 62F10, 62F12, 60E05, 60G70, 91B30',\n",
       "   'q-fin.PM',\n",
       "   'q-fin.ST',\n",
       "   'stat.AP',\n",
       "   'q-fin.RM',\n",
       "   'G.3']},\n",
       " 'content': {'full_text': 'JD Opdyke, Chief Analytics Officer                  Page 1 of 88                      Beating the Correlation Breakdown  \\n Beating the Correlation Breakdown :  \\nRobust Inference, Flexible Scenarios , and Stress Testing  for Financial Portfolios  \\nJD Opdyke, Chief Analytics Officer, DataMineit , LLC  \\n \\n \\n \\nMonograph: 1st Draft, November, 2021; Current draft, January 202 5 \\n \\nDisclaimer :  The views presented in this monograph are those of the sole author, JD Opdyke, and do not \\nnecessarily reflect those of particular  institution s. \\n \\nAcknowledgements :  This work is dedicated to my family – to my one and only daughter, Nicole, my one \\nand only son, Ryan, and my one and only wife, Toyo, for whom Euler and Gauss will always be close to her \\nheart: I extend my pride, love, and deepest gratitude for your unwavering support.  \\n \\nBiography :  JD Opdyke is  a senior data scientist of over 30 years in the investment and risk analytics \\nspace.  Currently Chief Analytics Officer at DataMineit, LLC, JD has strong and extensive experience \\nacross major financial verticals (capital markets, banking, and insurance) as  well as decades of risk \\nmodeling and data science consulting expertise across related industries. JD has built and led several \\nsenior quant teams, published 14 peer reviewed journal papers and book chapters, several of which were \\nvoted ‘Paper of the Year’  by panels of experts, and is a frequently invited speaker/presenter at top quant \\nand risk conferences globally.  \\nJD earned his Bachelor\\'s, with honors, from Yale University, his Master’s degree from Harvard University \\nwhere he was awarded multiple paid, competitive Fellowships, and he completed a post -graduate \\nfellowship in MIT’s graduate mathematics department as an  Advanced Study Fellow.  He serves as review \\neditor of several journals, including Artificial Intelligence in Finance.  \\n \\n \\n \\n \\n \\nJD Opdyke, Chief Analytics Officer                  Page 2 of 88                      Beating the Correlation Breakdown  \\n  \\n \\n \\n \\nCONTENTS  \\nIntroduction  \\nBackground  \\n Types of Dependence Measures  \\nEstimation  \\nNAbC: (Robust) Statistical Inference  \\n  Brief Literature review of Pearson’s Matrix: Distributional Results and Sampling Algorithms  \\n  NAbC: Pearson’s Correlation, the Gaussian Identity Matrix  \\n    Correlations to Angles, Angles to Correlations  \\n    Fully Analytic Angles Density, and Efficient Sample Generation  \\n    Matrix-level p-values and Confidence Intervals  \\n  NAbC: Pearson’s Correlation, Real -World Financial Data, Any Matrix Values  \\n    Nonparametric Kernel Estimation  \\n  NAbC: Any (PD) Dependence Measure, Any Data, Any Matrix Values  \\n    Spectral and Angles Distributions, Examples from Other Dependence Measures  \\n  NAbC: Fully General Conditions, Statistical Comparison of Two Matrices  \\n NAbC Remains “Estimator Agnostic”  \\nNAbC: Granular, Fully Flexible Scenarios, Reverse Scenarios, and Stress Testing  \\nNAbC Example: Kendall’s Tau p -values & Confidence Intervals, Unrestricted & Scenario -restricted  \\nNAbC: Beyond ‘Distance’ —Generalized Entropy  \\nNAbC: Future Research  and Applications  \\nConclusions  \\nReferences  \\n \\n \\n \\n \\n \\n \\n \\nJD Opdyke, Chief Analytics Officer                  Page 3 of 88                      Beating the Correlation Breakdown  \\n Introduction  \\n \\nWe live in a multivariate world, and effective modeling of financial portfolios, including their construction, \\nallocation, forecasting, and risk analysis, simply is not possible without explicitly modeling the \\ndependence structure of their assets.  Depende nce structure can drive portfolio results more than many \\nother parameters in investment and risk models – sometimes even more than their combined effects . \\n“Correlation is one of the most important, if not the most important, risk factor in finance, driving \\neverything … however, a unified and generally accepted correlation risk management framework \\ndoes not yet exist” (Packham & Woebbeking, 202 3, p.1).  \\nDespite this widely acknowledged yet unacceptable state of affairs , the literature provides relatively little \\nto define the finite -sample distributions of commonly applied  dependence measures , like (Pearson ’s) \\ncorrelation,  in useable and useful ways under challenging, real -world financial data conditions .1  Yet this \\nis exactly what is needed to make valid inferences about their estimates, and to use these inferences for \\na myriad of essential purposes, such as hypothesis testing, dynamic monitoring, realistic and granular \\nscenario and reverse scenario analys es, as well as  mitigating the effects of correlation breakdowns \\nduring, and preferably  before, market upheavals (which is when we need valid inferences the most).  \\nOn the one hand  the dearth of “real-world effective” methodology in this setting , which again simply boils \\ndown to defining the fini te-sample distributions of the relevant dependence measures , is somewhat \\nsurprising.  Financial markets certainly ha ve seen more extreme downtur ns in recent decades  than many \\nwould have expected ex ante  (e.g. Black Monday (1987), Tech Bubble (2000), Housing Bubble (2008), \\nCovid (2020)), during which correlation breakdowns have been well documented, their very material  \\neffects measured  and assessed  (see for example  Feng & Zeng, 2022 , and Packham & Woebbeking, 202 3), \\nand the importan ce of mitigation efforts  widely discussed , considered, and acted upon (see Greenspan, \\n1999; BIS, 2011a; and EBA-CRR, 2013).  What’s more, practitioners, academics, and regulators have a \\nlong history of bringing analytic and probabilistic rigor to bear when analyzing and estimating  the other  \\nparameters of our portfolio risk and investment models.  There is no shortage of empirical research \\ndefining, for example, various estimators  of the tail indices of a portfolio ’s marginal distributions,  and \\nderiving their associated p-values, confidence intervals, and statistical power  and level .  When rigorously \\nand properly estimated , these tools are highly actionable, providing invaluable guidance in decision-\\nmaking and mitigation efforts.  But what do we find when we look for those same tools , say, for an entire \\nmatrix of pairwise Kendall ’s tau values , to make decisions based on answers to questions such as , “Has \\nthis Kendall ’s matrix shifted in the past week ?  What is the probability of observing the movement we \\nobserved, given that our baseline estimate is true?  Does this meet our probabilistically  defined threshold \\n \\n1 I take ‘real -world’ financial returns data to be multivariate with marginal distributions that can vary notably from each other , \\nand change in time,  in their degrees of heavy -tailedness, serial correlation, asymmetry, and (non -)stationarity.  These \\nobviously are not the only defining characteristics of such data, but from a distributional and inferential perspective, they  \\nremain some of the most chall enging, especially when occurring concurrently as they do in non -textbook settings.  \\n \\nJD Opdyke, Chief Analytics Officer                  Page 4 of 88                      Beating the Correlation Breakdown  \\n for a ‘breakdown’?” or “Are the Kendall ’s matrices from these two sectors different only due to sample \\nvariation? Or do they represent  fundamentally distinct dependence structures? ” or “What are the two  \\n(upper and lower ) Kendall’s matrices that capture 95% of the conditional sample variation  in this setting?  \\nHow far beyond these bounds , probabilistically , does each Kendall’s matrix that we’ve defined for each of \\nour scenarios lie?  Given our distributions of losses/returns, d oes a tail dependence m atrix better capture \\nwhat we are trying to measure here, and can we conduct a ceteris paribus analysis, using the exact same \\ndistribution -defining methodology, to compare the statistical power  of these two dependence measures \\nunder the various relevant  data conditions? ”  If we require the p-values and confidence interval s and \\nrigorous, probabilistic  answers to these questions  to be valid under challenging, real -world financial data \\nconditions , the current literature provides relatively little.    \\nOn the other hand, the possible explanations  for this disparity  in useable and useful  methodology are n ot \\nentirely unreasonable.  F irst, depending on how broadly we define it, this arguably is a harder problem  \\nthan, say, that related to univariate tail indices , or many of the challenges  related to mo deling the other \\nparameters of investment and risk portfolios .  Even though each cell value of the dependence matrix is a \\nbivariate association , we are measuring all the p airwise associations in the portfolio simultaneously,  and \\nthe values of the cells  are, in non-trivial cases, all interrelated, making this a complex, multivariate \\nproblem.  Immutable mathematical requirements such as positive definiteness arise, and make  deriving \\nand simulating  the distribution of the all -pairwise matrix a non-trivial task.  This is especially true if we \\nwant to require, as we should, that the p -values and confidence intervals of each and every cell are \\nconsistent with those of the entire matrix.  Additionally , requiring that the finite sample distribution of the \\nmatrix (which makes possible the calculation of the p -values and confidence intervals)  remain valid \\nunder challenging, real -world financial data conditions  adds significantly to the nontrivial nature of the \\nproblem.  Distributions of dependence measures are more readily derived when we can  assume that \\nreturns are , say, multivar iate normal, or at least independent and identically distributed ( iid).  It is another \\nmatter entirely when the portfolio ’s marginal distributions  vary notably from each other , while also  \\nchanging over time in their degrees of heavy -tailedness, serial correlation, asymmetry, and (non -\\n)stationarity .  Yet this is exactly the empirical challenge of actual financial port folios.   In fairness, the \\nliterature does provide many solutions under mathematically convenience conditions , which more \\nnarrowly define and restrict both in the distr ibutional characteristics of the underlying returns data as \\nwell as the assumptions made regarding the values of the all -pairwise matri x.  But these often unrealistic \\nassumptions limit practical, real -world application, which is exactly the motivation for this monograp h.   \\nAnother complicating factor is the requirement that the method defining the finite sample distribution of \\nthe relevant dependence measures is the same across all those in practical usage : in this case, all those \\ndependence me asures for which the all-pairwise matrix is positive definite .2  This arguably covers all  that \\n \\n2 Note that “positive definite” throughout this monograph refers to the dependence measure calculated on the matrix of all \\npairwise associations in the portfolio, that is, calculated on a bivariate basis.  Some of the  dependence measures  addressed \\nin this monograph  (e.g. Szekely’s correlation , variants of Chatterjee’s , and others ) can be applied on a multivariate basis  \\n(sometimes even in arbitrary dimensions ), for example, to test the hypothesis of multivariate independence.  But “positive \\ndefinite” herein is not applied in this sense  (see for example Cardin, 2009) , and I explain below some of the reasons for using \\nJD Opdyke, Chief Analytics Officer                  Page 5 of 88                      Beating the Correlation Breakdown  \\n could conceivably be used and be useful in the financial setting .  This universality  is certainly desirable , \\nbut it also increases the challenge of deriving the methodology.  However, this actually is a crucial \\nrequirement as it provides the ability to conduct all -else-equal analyses comparing the performance of \\ndifferent dependence measures under controlled conditions : we can be certain that material and \\nstatistically significant differences in results are due to the dependence measures themselves , rather \\nthan distinct methodolog ies we typically would have to  use (based on the current literature)  to define \\ntheir distributions .  Yet nothing in the extant literature provides this  broad ceteris paribus capability.    \\nFinally, one of the major uses of dependence measures and their all -pairwise matrices is in de fining \\nscenarios and reverse s cenarios.3  These remain central and  critical for all manner of risk analyses , and \\nfully flexible scenarios require the ability to ‘freeze’ any set of selected cells of the all -pairwise matrix \\nwhile allowing others to vary.  For example, m any of the pairwise cells that will change dram atically, in \\nboth direction and magnitude, unde r a Covid-like scenario  will be completely  unaffected under a housing \\nbubble (see Feng & Zeng, 2022 , and Pramanik, 2024 ), and scenario analytics must be able to  validly \\ndefine the finite sample distribution of the all-pairwise matrix under both types of scenario -restricted \\nconditions.   However, no existing method allows for this without inadvertently  affecting the other \\n‘peripheral ’ cells of the all -pairwise matrix  (see Ng et al. (201 4) and Yu et al. (2014) ), and this can \\ndramatically dist ort the distribution of the matrix , rendering the associated inferences  for the scenario \\nuseless at best, and dangerously misleading at worst.  Granular flexibility in scenario de finition, at the \\nlevel of the pairwise cells , and the valid distribution of the associated , scenario-restricted  all-pairwise \\nmatrix, is a necessity if we are to accurately cap ture the fundamentally different nature of  disparate \\ncorrelation breakdowns , and accurately assess , forecast,  and mitigate their impacts . \\nSo perhaps it is not so surprising that we have comparatively little  in the extant literature in the way of \\nreal-world solutions to this problem .  For purposes of this monograph , I define the problem state ment as \\nfollows:  to define the finite sample distributions of all positive definite  measures of dependence \\nstructure, robustly under real -world data conditions, that remain valid regardless of the estimators used , \\nand even if  the co-movement of selected pairs of variables is ‘frozen’, i.e.  scenario-restricted.   In contrast  \\nto this problem statement , to date financial portfolio analysis in practice very often relies on ad hoc, \\nlargely qualitative, and ‘judgmental’ approaches to specifying and utilizing dependence structure .  When \\nquantitative approaches are used, their application relies on a literature largely restricted to  providing  \\nsolutions valid only in narrowly defined cases and/or requiring unrealistic but mathematically convenient \\nassumptions  for which either i. the distributions derived are only asymptotically valid (i.e. assume \\ninfinitely large sample sizes) , or ii. they require very restrictive and/or unrealistic assumptions about the \\nmarginal returns distribution s of the portfolio (e.g. that they are multivariate Gaussian,  or elliptical ; or \\n \\nthe dependence framework of all pairwise associations, which is highly flexible, and allows for more precise attribution and \\nintervention analyses.  \\n \\n3 Scenarios typically are designed to answer questions of the type, “What loss is associated  with, say, the 99.5%tile of the loss \\ndistribution? ” while reverse scenarios  answer questions of the type, “What percentile of the loss distribution produc es a loss \\nof $X?” The dollar amoun ts referenced in the latter ty pically are associated with specific extreme or catastrophic events , such \\nas insolvency or the failure of a m ajor business line or geography .   \\nJD Opdyke, Chief Analytics Officer                  Page 6 of 88                      Beating the Correlation Breakdown  \\n even that they are  independent and identically distributed (“ iid”), or all symmetric, or all stationary , or not \\nserially correlated, etc .), or iii. they require very restrictive and/or unrealistic assumptions about the \\nvalues of the dependence measures themselves  (e.g. the cells are all zeros, or all have the same value , or \\nfollow very discrete and limited block structures ), or iv. they estimate the all -pairwise matrix in ways that \\ndo not guarantee  its positive definiteness , or violate other fundamental mathematical requirements (e.g. \\nunit diagonals ), or v. most typically, they require multiple of these restrictive and/or unrealistic \\nassumptions combined.   Many of these more narrow solutions are mathe matically elegant, but our goal \\nherein is to obtain an actual solution  that works and remains inferentially valid for messy, challenging, \\nreal-world financial portfolios .  The new and str aightforward method  developed herein – Nonparametric \\nAngles-based Correlation (“NAbC”) – is defined by eight critically important characteristics listed below \\nthat, satisfied simultaneously, distinguish it from any other approach in the lit erature.  Yet its foundations \\nrest squarely on very well established results in the relevant and closely related literatures.  \\n1. NAbC remains valid under challenging, real -world data conditions, with marginal asset distributions \\ncharacterized by notably  different and  varying degrees of serial correlation, (non-)stationarity, heavy -\\ntailedness, and asymmetry  \\n2. NAbC can be applied to ANY positive definite dependence measure , including but not limited to the \\nfoundational Pearson’s product moment correlation matrix (Pearson, 1895), rank -based measures \\nlike Kendall’s Tau (Kendall, 1938) and Spearman’s Rho (Spearman, 1904), measures designed to \\ncapture highly non -linear and/or cyclical dependence such as the tail dependence matrix (see \\nEmbrechts, Hofert, and Wang, 201 6, and Shyamalkumar and Tao, 2020 ), Chatterjee’s correlation \\n(Chatterjee, 2021)  and its variants ( Pascual-Marqui et al., 2024), the improved Chatterjee ’s correlation \\n(Xia et al., 202 4), Lancaster’s correlation (s) (Holzmann and Klar, 2024), and Szekely’s distance \\ncorrelation (Szekely, Rizzo, and Bakirov, 2007) and its variants (such as Sejdinovic et al., 2013, and \\nGao and Li, 2024)   \\n3. NAbC remains “estimator agnostic,” that is, valid regardless of the sample -based estimator used to \\nestimate any of the above -mentioned dependence measures  \\n4. NAbC provides valid confidence intervals and p -values at both the matrix  level and the pairwise cell  \\nlevel, with analytic consistency between these two levels (i .e. the confidence intervals for all the cells \\ndefine that of the entire matrix, and the same is true for the p -values; this effectively facilitates , and in \\nmany cases makes possible,  granular and targeted attribution analyses)  \\n5. NAbC provides valid confidence intervals and p -values not only for  one-sample tests against \\nmatrices of fixed, assumed ‘true’ values, but also for two-sample tests comparing two matrices, so \\nthat we can assess inferentially whether dependence structures truly are different , for example,  \\nacross different sectors or segments of our business es. \\n6. NAbC provides a one -to-one quantile function, translating a matrix of all the cells’ cumulative \\ndistribution function ( cdf) values to a (unique) correlation /dependence measure matrix, and back \\nagain, enabling precision in reverse scenarios and stress testing , as well as informed  and targeted \\n‘what if’ analyses  \\nJD Opdyke, Chief Analytics Officer                  Page 7 of 88                      Beating the Correlation Breakdown  \\n 7. all the above results remain valid even when selected cells in the matrix are ‘frozen’ for a given \\nscenario or stress test  – that is, unaffected by the scenario – thus enabling flexible, granular, and \\ninferentially valid realistic scenarios  \\n8. NAbC remains valid not just asymptotically, i .e. for sample sizes presumed to be infinitely large, but \\nrather, for the specific sample sizes we have in reality  (for full-rank matrices with n>p)4, enabling \\ninferentially reliable application in actual, real-world, non-textbook settings  \\nThe alternative  to a method satisfying the eight objectives above , simultaneously, is to use a piecemeal,  \\nincomplete  patchwork of disparate derivations  of distributions, some asymptotic, some not,  valid under  \\ntypically restrictive and unrealistic  data conditions for only a few of the widely used dependenc e \\nmeasures : in the end, the patchwork limits, and arguably all but prohibits, reliable comparative analyses \\nwhich, if attempted,  in reality become more confound ing than elucidating .  This is exacerbated by the \\nunwieldy, opaque,  highly complex, and difficult -to-implement nature of many of these solutions.  \\nNAbC circumvents  all of these problems  with a single, unified, and straightforward method  that, \\ncompared to its more limited and narrowly defined competitors,  simultaneously  and dramatically \\nincreases i. robustness , ii. scenario flexibility , iii. accuracy in attribution analys es, and iv. targeted \\nprecision in ‘what if’ intervention analys es, all while enabl ing v. ceteris paribus analys es across \\ndependence measures .  What’s more, in satisfying the eight objectives listed above, NAbC also satisfies \\nthe original moti vating factor for its development : it provides the same level of probabilistic rigor in the \\nobjective, quantitative analysis of portfolio dependence structure as has been applied to the other \\nparameters in our investment and r isk portfolio models.  \\nImportantly, n ote that the original statement in this Introduction, “effective modeling of financial \\nportfolios, including their construction, allocation, forecasting, and risk analysis, simply is not possible \\nwithout explicitly modeling the dependence structure of their assets. ” applies to all frameworks for \\nportfolio analysi s, even th ose that may not always make expli cit their estimation of, or their reliance on , \\ndependence structure .  For example, some path dependent approaches generate distributions of \\nportfolio results based in large part, or even primarily, on (usually subjectively defined ) probabilities \\nassociated with various scenarios , without explicitly defining dependence structure .  But such \\napproaches still make many implicit assumptions regarding dependence structur e, such as that it doe s \\nnot change  from one period to the ne xt, or that it does not change under one scenario versus another , or \\nthat, even if (Pearson ’s) correlation s may be controlled  via ‘views’ specified  in the model, other measures \\nof dependence, such as tail dependence , are not unwittingly changed from one period to another  (even if \\nthis is unlikely when ‘views’ on volatilities are changed ).  Whether impl icit, indirectly explicit  via ‘views’ on \\nother parameters , or explicit, all such assumptions  about dependence structure will affect simulated \\nresults, and they always should be made fully explicit in any model  (see Meucci , 2010b, and Vorobets, \\n2025, for examples), even if only for ex post  testing using NAbC  to ensure that  the effects of (possibly \\nchanging ) dependence str ucture are not (unknowingly) confounding results.  \\n \\n4 Recall that this condition is required  for the all-pairwise matri x to be positive definite.  \\nJD Opdyke, Chief Analytics Officer                  Page 8 of 88                      Beating the Correlation Breakdown  \\n But beyond and in addition to simply avoiding confounding, NAbC provides  such models with  statistical \\ncontrol and inferential validity when specifying dependence matrix values based on scenario ‘views’.  As \\nopposed to ad hoc or judgement -based matrix values, the ‘view’ of an extreme correlation /dependence  \\nmatrix should be defined probabilistically , based directly on its finite sample distribution , which NAbC \\nprovides.  For example, t he ‘view’ of a correlation matrix corresponding to an extreme scenario , when \\nused as an input to a path dependen t simulation, should be a percentile  (say, 99%tile ) of the distribution \\nof the all-pairwise matrix, as provided by NAbC ’s quantile function .  All that NAbC needs to define th e \\nvalues of this matrix  is the data generating m echanism and the null hypothesis , i.e. the baseline values  of \\nthe matrix.  Conversely, NAbC also can provide the cdf value  (percentile) of an all-pairwise matrix  whose \\nvalues are specified for a specific scenario , thus ensuring that it is sufficiently ‘extreme,’ or not too \\n‘extreme,’ (e.g. is it the 80%tile?  Or the 99.999%tile?) for the scenario being tested .  NAbC provides both : \\nthe matrix  corresponding to a specified  percentile, and the percentile corresponding to  a specified \\nmatrix.  Only in this way does a ‘view’ on dependence str ucture retain objective  meaning regarding its \\nrelative size,  not to mention its  inferential validity , as opposed to being informed by  qualitative , subjective  \\njudgement s or ad hoc procedures . \\nI conclude this Introduction wit h a request of my valued readers: at various stages in this monograph you \\nwill recognize that other papers I am discussing and presenting will provide solutions, often very good \\nones, to pieces of the problem statement I define above.  However, they all have notable if not mortal \\nshortcomings when all eight of the objectives  I have listed need to be achieved  simultaneously, which is \\nwhat I have found is required for the applied solution to this problem in non-textbook financial settings.  I \\nalways try to explain, as proximate as possible in the text, where and why these limitations restrict the \\nscope of application  of a particular, more narrow solution , so I would gently ask the reader to suspend \\ndisbelief long enough to allow me to present these before deciding I have missed an important, existing \\nsolution in the literature.  However, i f that does turn out to be the case in any specific instance, I stand \\neager to learn of and correct the oversight.  \\n \\nBackground  \\n \\nThe primary objective of this monograph is to develop and implement a new and straightforward method \\n– Nonparametric Angles -based Correlation (“NAbC”) – for defining the finite -sample distributions of a \\nvery wide range of dependence measures for financial portfolio analysis.  As described above, t hese \\ninclude ANY that are positive definite, such as the foundational Pearson’s product moment correlation \\nmatrix (Pearson, 1895), rank -based measures like Kendall’s Tau (Kendall, 1938) and Spearman’s Rho \\n(Spearman , 1904), as well as measures designed to capture highly non -linear and/or cyclical \\ndependence such as the tail dependence matrix (see Embrechts, Hofert, and Wang, 201 6, and \\nShyamalkumar and Tao, 2020 ), Chatterjee’s correlation (Chatterjee, 2021), Lancaster’s correlation \\n(Holzmann and Klar, 2024), and Szekely’s distance correlation (Szekely, Rizzo, and Bakirov, 2007) and \\ntheir many variants (such as Sejdinovic et al., 2013, and Gao and Li, 2024).  \\nJD Opdyke, Chief Analytics Officer                  Page 9 of 88                      Beating the Correlation Breakdown  \\n Without an estimator ’s finite sample distribution, valid inference s simply cannot be made about its \\nestimates from actual data samples .  While much has been accomplished regarding distributional \\nresults and inferential capabilities of estimat ors of these dependence measures  for the bivariate case, \\nthis is different from application to an entire  (multivariate ) portfolio, where estimation of the all-pairwise \\nmatrix imposes additional constraints and complications (e.g. the requirement of positive definiteness ).  \\nThis is where NAbC comes in .  Motivation for NAbC’s development has been enabling such in ferential \\nanalyses in real-world settings, under real world data conditions, for actual  financial portfolios (as \\nopposed to textbook settings , or strictly the bivariate case ).  Fortunately, the development of dependence \\nmeasures like the ones listed above not only is foundational for modern statistics, but also has been a \\nvery active area of research  recently.  S o NAbC has no shortage of ready-made, well tested, useful and \\nusable dependence measures to rely on in its inferential  application.   I first review these dependence \\nmeasures in this chapter  before demonstrating how NAbC enhances and in many cases, enables their \\nrigorous, inferential application to financial portfolios.  \\n \\nTypes of D ependence Measures  \\nMeasures of association , otherwise known as  dependence measures,  are as old as modern statistics \\nitself (see Pearson, 1895) .  They provide a quantitative assessment of how variables move together or in \\nopposite  directions  over time .  I address their relation to  causal mechanisms  in later chapters , and \\nmerely note here that they remain distinct from  what are often called  ‘metrics’ or ‘distance metrics ,’ even \\nthough the two are sometimes confused.5   \\n \\nMonotonic Dependence Measures  \\nThe oldest and most widely used and known dependence measure is Pearson’s product moment \\ncorrelation (see Pearson, 1895) , which is what is usually referenced when “correlation” is mentioned .  \\nTaking two variables,  say, the financial returns of two assets  X and Y, Pearson’s measures how often and \\nto what degree they deviate from their respective sample means in the same or in opposite directions , as \\n \\n5 Even though ‘metrics’ or ‘distance metrics’ often are built directly on dependence measures, they typically do not share many \\nof their characteristics (e.g. their spaces are not  necessarily  positive definite (see  Alpay & Mayats -Alpay, 2023; and  Meckes, \\n2013))  In finance they often are used non -inferentially and mechanistically in hierarchical portfolio construction models (see \\nTumminello et al., 2005; and Dom et al., 2024 ) where they have received  mixed reviews  (see Aznar, 2023 ; Cota, 2019 ; and \\nCiciretti & Pallotta, 2023 ), especially under correlation breakdowns (see Marti et al., 2021).  However,  a recent and ingenious \\npaper by Cotton (2024) provides the first methodological basis for their (partial and conditional ) usage for portfolio \\nconstruction .  Still, as they stand,  they are not designed to answer the inferential questions posed herein.  In fact, I show in \\nlater chapters how NAbC provides a generalized entropy that has many  useful advantages over an entire class of metrics most \\ncommonly used in this setting , called ‘norms.’  \\n \\nJD Opdyke, Chief Analytics Officer                  Page 10 of 88                      Beating the Correlation Breakdown  \\n shown in (1) below.6 \\n(1) \\n()\\n() ()() 1 1 1\\n,22\\n1 1 1 1111\\n,\\n1111n n n\\ni j i j\\ni j j\\nXY\\nn n n n XY\\ni j i j\\ni j i jX X Y Y nnn Cov X Yrss\\nX X n Y Y nnn= = =\\n= = = =\\uf0e6 \\uf0f6\\uf0e6 \\uf0f6− − −\\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0e8 \\uf0f8\\uf0e8 \\uf0f8==\\n\\uf0e6 \\uf0f6 \\uf0e6 \\uf0f6− − − −\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf0e8 \\uf0f8 \\uf0e8 \\uf0f8\\uf0e5 \\uf0e5 \\uf0e5\\n\\uf0e5 \\uf0e5 \\uf0e5 \\uf0e5  \\nThe numerator is the (sample) covariance of X and Y, and the denominator – the product of the (sample) \\nstandard deviations of X and Y – has the effect of scaling the  (sample) covariance to a (maximum)  range \\nof -1 to 1.7  So Pearson’s is just the scaled covariance between X and Y.  \\nAnother of the most commonly used dependence measures is Spearman’s Rho (see Spearman, 1904), \\nwhich is exactly the same formula as Pearson’s but instead of using the values of X and Y, their ranks are \\nused instead:  \\n(2a) \\n()\\n() ()1 1 1\\n,22\\n1 1 1 1111\\n1111i j i j\\ni j i jn n n\\nX X Y Y\\ni j j\\nXY\\nn n n n\\nX X Y Y\\ni j i jR R R R nnnsr\\nR R n R R nnn= = =\\n= = = =\\uf0e6 \\uf0f6\\uf0e6 \\uf0f6− − −\\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0e8 \\uf0f8\\uf0e8 \\uf0f8=\\n\\uf0e6 \\uf0f6 \\uf0e6 \\uf0f6− − − −\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf0e8 \\uf0f8 \\uf0e8 \\uf0f8\\uf0e5 \\uf0e5 \\uf0e5\\n\\uf0e5 \\uf0e5 \\uf0e5 \\uf0e5  \\nIf there are no ties in the data, (2a) can be shortened to  \\n(2b) \\n()2\\n1\\n, 36\\n1iin\\nXY\\ni\\nXYRR\\nsrnn=−\\n=−−\\uf0e5 (see Zar, 1999)  \\nUsing ranks can make Spearman’s less sensitive than Pearson’s to extreme data values  under some data \\nconditions , just like another rank-based dependence measure, Kendall’s Tau . \\nAlso called a measure of concordance, Kendall’s Tau ( see Kendall, 1938) is the sum of all pairwise \\ncomparisons of every data point of X and Y , divided by the total number of pairs.  The pairwise \\ncomparisons are given values of 1, 0, or -1, respectively,  if both from one period to another are in \\nincreasing /decreasing  order, if the values from both periods are tied for either of the assets , or if both  \\nassets are NOT in increasing /decreasing  order; it thus gives the number of pairs in concordance minus \\nthe number in discordance  relative to the total number of pairs , as shown below.  \\n \\n6 Importantly, a ll formulae of estimators herein, unless otherwise noted, refer to th ose based on sample data, as opposed to \\nthose based on an entire population of data . \\n \\n7 Note that this range can be tighter under specific circumstances, such as for equicorrelation matrices where \\n() 1 p 1 1, p dim( ). rr \\uf0e9\\uf0f9− − \\uf0a3 \\uf0a3 =\\uf0eb\\uf0fb\\n \\nJD Opdyke, Chief Analytics Officer                  Page 11 of 88                      Beating the Correlation Breakdown  \\n (3a) \\n()()()()1\\n11#concordant pairs #discordant pairs 2, = sgn sgntotal # pairs 1nn\\ni j i j\\ni j iX Y x x y ynn\\uf074−\\n= = +−= − −−\\uf0e5\\uf0e5  \\n() () () where sgn 1 if  0, sgn 1 if  0, sgn 0 if  0 , for both  and z z z z z z N n= \\uf03e =− \\uf03c = =\\n \\nHowever, ties in the values of either of the pairs, \\n()()  and  or  and i j i jx x y y , will restrict the range from \\nachieving -1 or +1, even under otherwise perfect discordance or concordance,  respectively,  so a \\ncommonly used variant  of Kendall’s Tau  that avoids this drawback when ties exist is: \\n(3b) \\n()\\n()()()()1\\n110 1 0 21, = sgn sgnnn\\nb i j i j\\ni j iX Y x x y y\\nn n n n\\uf074−\\n= = +\\uf0e9\\uf0f9−− \\uf0ea\\uf0fa−− \\uf0eb\\uf0fb\\uf0e5\\uf0e5  \\n() () () grps ties  grps ties\\n0 1 2\\n11where 1 2; 1 2; 1 2;ij\\ni i j j\\nijn n n n t t n u u\\n=== − = − = − \\uf0e5\\uf0e5\\n \\n               \\n# ties in i-th group of ; # ties in j-th  group of ijt x u y==  \\nThe “big 3” dependence measures – Pearson’s, Kendall’s, and Spearman’s – are by far the most widely \\nused in practice .8  Although widely held myths persist regarding Pearson’s as a measure strictly of linear \\nmonotonic relationships  (see van den Heuvel  & Zhan, 2022 ), all three measure 1. monotonic association  \\n(i.e. the direction of the association , positive or negative, does not change  within the covered time period ) \\nthat is 2. symmetric , or non-directional  in the variable  order (i.e. the measured dependence  of X on Y is \\nthe same as th at of Y on X).  It is important to recall here that as measures of monotonic dependence, \\nvalues of zero  generally  do not necessarily imply independence between X and Y,9 but independence \\nbetween X and Y does imply values of zero  for the big 3 .10  Many of the dependence measures treated \\nbelow avoid this conceptual shortcoming.  \\nThe properties of the big 3 have been studied  extensively  in the literature, but real gaps remain .  Our \\ninterest in this monograph lies not just in a single bivariate relationship between X and Y, but rather, in all \\npairwise relationships of all assets in a portfolio , simultaneously: X may be strongly , positively  associated \\nwith Y, which also may be positively  associated with  Z, which also may be negatively associated with A, B, \\nand C, while B and C may be modestly but negatively associated  with X again!  So we have a matrix of \\ndependence measure values with rows and columns identifying the pairwise relationships between all \\n \\n8 Others include Hoeffding’s D (see Hoeffding,1948 ), Blomqvist’s coefficient  (see Blomqvist, 1950 ), and Gini’s gamma (see \\nGini 1914; and Genest et al., 2010) . \\n \\n9 However, an exception occurs when data is distributed as bivariate normal, in which a Pearson’s value of zero does indicate \\nindependence.  \\n10 This is easy to visualize with a non -monotonic relationship like \\n2y=x +\\uf065 , which on average will yield big 3 values close to \\nzero.  But the relationship is  non-linear and  u-shaped, which most certainly is not one of independence.  \\nJD Opdyke, Chief Analytics Officer                  Page 12 of 88                      Beating the Correlation Breakdown  \\n the asset pairs, as shown in (4) for assets 1, 2, 3, and 4.   I refer to this matrix herein as the all -pairwise \\nmatrix.  \\n(4) \\n1,2 1,3 1,4\\n2,1 2,3 2,4\\n3,1 3,2 3,4\\n4,1 4,2 4,31\\n1\\n1\\n1r r r\\nr r rRr r r\\nr r r\\uf0e9\\uf0f9\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa=\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\uf0eb\\uf0fb  \\nSome of the characteristics of this matrix, for all of the big 3 and many of the other measures presented \\nbelow, include:  \\ni.   Symmetry: \\n,,i j j irr=  \\nii.  Unit diagonal entries: \\n1ijr==  \\niii. Bounded non -diagonal entries , with maximum range of : \\n,11ijr− \\uf0a3 \\uf0a3  \\niv. The matrix is positive definite,11 i.e. all eigenvalues  \\n0i\\uf06c\\uf03e  \\nFor completeness and for reference throughout this monograph , I define eigenvalues here:  \\nIf there exists a nonzero vector v such that \\nRv v\\uf06c= then λ is an eigenvalue of R and v is its \\ncorresponding eigenvector.  λ and v can be obtained by solving  \\n() () det 0, then det 0, where I R I R v I\\uf06c\\uf06c− = − =\\nis the identity matrix and det is the determinant.   The \\neigenvalue can be thought of as the magnitude of the (portfolio) variability  in the direction of the \\neigenvector .  With actual, real -world financial data (i.e. values that are not imaginary or complex), this \\nvariability can never be negative,12 so numeric computational issues aside ,13 proper measures of \\ndependence must be positive definite .14   \\n \\n11 See Saboto et al. (2007) for proofs of the (semi)positive definiteness of Pearson’s, Kendall’s, and Spearman’s.   Semi-positive \\ndefiniteness includes the case of eigenvalues exactly equal to zero, which I largely ignore herein as a border case relevant \\nmainly for  textbook examples.  \\n \\n12 This can be seen most easily when the covariance (or equivalently, Pearson’s correlation) is the dependence measure used: \\nthe covariance is  the (expected value of  a) sum of squared real numbers (as no imaginary or complex values are observed in \\nfinancial returns .  Because a squared, real number greater than zero is always greater than zero, their sum can never be \\nnegative.  \\n \\n13 Numerical calculations can sometimes render estimates of specific eigenvalues slightly negative, but t he NAbC method \\nproposed herein in later chapters specifically is designed to be more robust to such numerical errors than the more common, \\nlimited approaches related to eigen decompositions in the extant literature.  \\n \\n14 If any λ = 0 the matrix is said to be positive semi -definite, although herein this is treated as a textbook border case as all \\nrelevant financial returns would have to be exactly zero for the eigenvalue to be zero.  \\nJD Opdyke, Chief Analytics Officer                  Page 13 of 88                      Beating the Correlation Breakdown  \\n Positive definiteness is proven mathematically in later chapters  for all widely used dependence  \\nmeasures  in this setting , and it remains  crucial for a number of important reasons discussed throughout \\nthis monograph .  But the main point here is that we need to understand the characteristics of the \\nestimators we use to estimate the values of the all -pairwise matrix, based on our sample of financial \\nreturns data.  Why?  Because we want to be able to make inferences  from these sample-based values to \\nanswer critical questions about the behavior of the portfolio , and specifically its dependence matrix.  F or \\nexample, Q: has the dependence matrix changed from our established baseline, or from period 1 to \\nperiod 2? ( A: one-sample or two -sample matrix-level p-value, respectively )  If so, Q: which pairwise \\nrelationships primarily are driving this conclusion? ( A: attribution analyses using cell -level p-values)  Q: \\nHow likely is scenario “ E” given our current  baseline dependence matrix of “ F”? (A: one-sample matrix-\\nlevel p-value)  Q: What are the two matrices that correspond to the 95% confidence interval for our \\ncurrent matrix “C”? ( A: matrix level confidence intervals)  Q: Do the matrices associated with our  \\nscenarios and stress tests fall outside these two matric es?  If not, do we need to revisit their \\nconstruction?   Q: What is the probability of observing the matrix associated with  reverse scenario “X”?  (A: \\nmatrix level quantile and p -value)  Q: If pairwise cells A, B, and C remain unaffected by Scenario Q, how \\nmuch do cells X, Y, and Z have to change for dependence measure #3 to be able to detect the change? ( A: \\nscenario -restricted cell -level and matrix -level p-values)  What is the answer to this question for \\ndependence measure #4?   What are the implications of t his comparison  for detecting correlation \\nbreakdowns?   Are the power and the robustness and the average run length of dependence measure #3, \\nwhen used in the appropriate statistical process control chart, better than #4?  Or are there tradeoffs \\nbetween the two dependence measures?  Do answers to these questi ons depend on the (sample-based) \\nestimators used for each, #3 and #4?  (A: matrix and cell -level p-values and confidence intervals, \\nrespectively) . \\nThe list of relevant, financially material inferential analyses here is endless, but one thing is necessary to \\nbe able to answer any of these questions with probabilistic  rigor: we must have the finite -sample \\ndistribution of whatever estimator we are employing to estimate the values of the all-pairwise matrix.  \\nConceptually this is the same as using a  “bell” curve (a.k.a. a normal or Gaussian distribution)  to answer \\nhow likely it would be to observe , for a single variable,  a sample mean  at least as large as what we \\nactually observe in practice :15 in our case, with the all -pairwise dependence matrix, the question could \\nbe identical except that the distribution simply is multivariate  (i.e. many variables)  as opposed to \\nunivariate  (one variable) , and in almost all cases the distribution would not be Gaussian . \\nHowever, the extant literature provides relatively little to define the  multivariate,  finite sample \\ndistributions of these dependence measures  under real -world data conditions, let alone the use of a \\ncommon method across all relevant dependence measures .  Either the distributions derived are \\nasymptotic (i.e. assume infinitely large sample sizes) ; or they require very restrictive and/or unrealistic \\n \\n15Relying on the central limit theorem (CLT) in this way requires additional assumptions, such as the finiteness of the mean \\nand the variance of the distribution of the data , and under some conditions that the data is independent and identically \\ndistributed (“iid”).  Also, the size of the sample required for the convergence of the sample mean’s distribution to the Gaussian  \\ndistribution  can rely heavily on the distribution of the underlying data . \\nJD Opdyke, Chief Analytics Officer                  Page 14 of 88                      Beating the Correlation Breakdown  \\n assumptions about the distribution s of the returns data (e.g. that they are multivariate Gaussian,  or \\nelliptical; or even that they are  independent and identically distributed (“ iid”), or all symmetric, or all \\nstationary , etc.); or they require very restrictive and/or unrealistic assumptions about the values of the \\ndependence measures themselves  (e.g. the cells are all zeros, or all equal  to each other , or follow very \\ndiscrete and limited block structures ); or they estimate the all -pairwise matrix in ways that do not \\nguarantee  its positive definiteness ; or, most typically, they require multiple  of these unrealistic \\nassumptions and restrictions combined.  \\nBut in real -world practice, w hat we need  for useful and useable application  to answer the kinds of \\ninferential questions  listed above is the finite-sample distribution  of ALL of the relevant dependence \\nmeasures  (for ceteris paribus comparisons)  under real -world data conditions , that is, non -iid \\nmultivariate financial returns data with marginal distributions that have different and varying degrees of \\nasymmetry, serial correlation, (non)stationarity, and heavy-tailedness.  The finite sample distribution  of \\nthe matrix also must be valid f or all potential values of the dependence matrix , regardless of the \\nestimator being used .  The inferential results that depend on this distribution, i.e. the p-values and \\nconfidence intervals, must be provided at both the matrix level and the individual pairwise cell levels , and \\nthese results must be  analytically consistent  across the levels .  And the method generating the \\ndistribution that  provides all  of these results must remain valid  even when the matrix is restricted by the \\ndictates of a particular Scenario, for which many cells remain ‘frozen’ (i.e. unaffected by the scenario ); for \\nexample, many pairwise associations strongly affected in a Covid -type scenario will remain unaffected \\nunder a housing bubble scenario.  \\nThe method derived in the coming chapters – Nonparametric Angles -based Correlation, or “NAbC” – \\nchecks all of these boxes, just as described in the Introduction .  This allows for valid statistical inference , \\nunder real -world conditions, without unrealistic and restrictive assumptions  on the data or the values of \\nthe all-pairwise matrix.  NAbC not only provides answers to any of the inferential questions posed above , \\nand then some, but also does so using the one single method for defining the finite -sample d istribution of \\nthe dependence matrix.  This allows for ceteris paribus comparisons of different dependence measures, \\nand different estimators of those dependence measures, under controlled conditions so that we can \\nchoose the best among them (i.e. which is mo re robust, powerful/precise, accurate, with shortest \\naverage run -length, etc., all else equal)  depending on the data conditions and/or the hypotheses being \\ninvestigated .  The alternative is to use an incomplete  patchwork of disparate derivations  of distributions, \\nsome asymptotic, some not,  valid under questionably similar but usually very restrictive and typically \\nunrealistic conditions that, in the end, prohibit reliable comparative analyses and are more confound ing \\nthan elucidating . \\nBefore deriving and implementing  NAbC, and demonstrating its broad scope and flexibility under  real-\\nworld data conditions, I first complete this section with additional, important dependence measures that  \\nall fall within  NAbC’s range of  applicatio n, and for the newer ones, are increasingly used in practice . \\n \\n \\nJD Opdyke, Chief Analytics Officer                  Page 15 of 88                      Beating the Correlation Breakdown  \\n Tail Dependence  \\nAnother important and time -tested dependence measure , especially for risk analyses,  is the tail \\ndependence matrix  (TDM).  Conceptually, TDM measures the probability of a variable (return) value \\nresiding in the tail of one variable’s distribution given that the value of the other asset resides in the tail of \\nit’s distribution.  More precisely, TDM provides the probability of a variable exceeding a quantile  of its \\ndistribution  conditional on the other variable in the pair exceeding the same quantile of its distributi on.  \\nHence, the tail dependence matrix consists of conditional probabilities of quantile exceedance, so each \\nvalue can range from zero to one , rather than -1 to 1 like the “big 3.”  B ut otherwise the matrix conditions \\nlisted in (4) above all hold (its positive definiteness is proven later herein, and was proven by Embrechts \\net al., 2016).  The upper tail dependence matrix only is equal to the lower tail dependence matrix if data \\ndistributions are perfectly symmetric: otherwise, the  two metrics  have distinct values, as shown below in \\n(5) and (6):  \\n(5) \\n()() ( )11\\n,1lim |X Y Y XqTDMU P Y F q X F q\\n−−−\\n→= \\uf03e \\uf03e   \\n(6) \\n()() ( )11\\n,0lim |X Y Y XqTDML P Y F q X F q\\n+−−\\n→= \\uf0a3 \\uf0a3   \\n() () \\uf07b \\uf07d1where quantile function = inverse cdf = inf : F q x F x q−= \\uf0ce \\uf0b3\\n \\nOther measures of tail dependence exist  (see AghaKouchak et al., 2013 , Babić et al., 2023, Manistre, \\n2008, Li and Joe, 2024, Krupskii and Joe, 2014, Lauria et al., 2021, and intriguingly,  Siburg et al., 2024), but \\n(5) and (6) are the oldest , most widely used , and best understood .  Tail dependence is especially \\nimportant in  the risk analy tics of financial portfolios because “tail events” often represent  the most \\nmaterial financial impacts, are typically associated with non -linear effects and associations, and are \\nclosely tied to correlation breakdowns: as is commonly and rightly stated, “when things go bad they go \\nbad together. ”  The eponymous phenomenon of correlation breakdowns is treated in more detail later in \\nthis monograph , but note that  the tail dependence matrix has been one of the principal tools used  in both \\nthe literature and by prac titioners to quantitatively estimate, model, and mitigate  it. \\n \\nMore Recent Dependence Measures  \\nThe design of a  more recent and flexible dependence measure , Szekely’s  distance correlation (Szekely et \\nal., 2007),  seeks to better handle dependence that is both non-linear and non -monotonic.  It uses two \\nmatrices: the matrix of pairwise distances between all values in a sample from X, and the same matrix \\ncalculated from a sample from Y.  To the extent that these matrices vary together, Szekely’s distance \\ncorrelation will approach a value of 1, and to the extent they do not, it will approach a value of zer o.  So its \\nrange is zero to one and a value of zero, unlike the “big 3,” DOES indicate independence between X and Y.   \\nAlso unlike the “big 3” its value does not indicate with a positive or negative sign whether dependence \\nbetween X and Y is positive or negative.   Notably, the distance correlation can be calculated in arbitrary – \\nJD Opdyke, Chief Analytics Officer                  Page 16 of 88                      Beating the Correlation Breakdown  \\n and different – dimensions, so the sample from X can be drawn , for example,  from a three dimensional \\ndistribution , and the sample from Y can be drawn from a six dimensional distribution.   \\n(7) first, create n x n distance matrices a and b  by letting  \\n2 2 2\\n, , 1 2  and ,  , 1,2,3,...,  where vector i j i j i j i j n na x x b y y i j n z z z z= − = − = = + + +\\n \\nNext, subtract from a and b their row and column means, and add their respective  matrix mean s, as \\nshown below : \\n, , *, ,* *,* , , *, ,* *,*  and i j i j j i i j i j j iA a a a a B b b b b= − − + = − − +\\n \\nThen Szekly’s distance correlation = \\n22\\n, , , , 2 2 2\\n1 1 1 1 1 11 1 1n n n n n n\\ni j i j i j i j\\ni j i j i jdcorr A B A Bn n n= = = = = ==\\uf0d7\\uf0e5\\uf0e5 \\uf0e5\\uf0e5 \\uf0e5\\uf0e5  \\nAnother recent dependence measure – Lancaster’s  correlation (see Holzmann and Klar, 2024 ) – shares \\nseveral characteristics  with Szekely’s : its values range from zero to one, a value of zero indicates \\nindependence, and it does not indicate with a positive or negative sign whether the dependence between \\nX and Y is positive or negative.  Lancaster’s correlation was designed to not only handle non -linear and \\nnon-monotonic dependence, but also to improve upon, via increased robustness and generalizability and \\nease of computation,  another dependence measure, the maximal correlation (see Hirschfeld (1935) and \\nGebelein (1941)). \\n(8) \\n()() ( ) ()() ()()2 2 1 1max , , ,  where  and XY lan r X Y r X Y X F X Y F Y−−= =\\uf046 =\\uf046\\n , where r is Pearson’s \\ncorrelation, \\n is the absolute value function, \\n1−\\uf046 is the quantile (inverse cdf) function of the standard \\nnormal distribution , and F is the (empirical) cdf of each variable.  \\nA second version is called linear Lancaster’s correlation:  \\n(9) \\n()() ( ) ()()()()()()2222\\n11max , , ,  where 1  and 1nn\\nii\\niilanL r X Y r X Y X X X X X n Y Y Y Y Y n\\n=== = − − − = − − − \\uf0e5\\uf0e5\\n  \\n            \\n11and  and nn\\nii\\niiX X Y Y\\n====\\uf0e5\\uf0e5  \\nHolzmann and Klar (2024) conduct empirical analyses comparing Szekely’s distance correlation and \\nboth Lancaster’s correlations under a wide range of data conditions.  They also compare these to another \\nnew, but asymmetric dependence measure, called Chatterjee’s correlation coefficient.  \\n \\n \\n \\nJD Opdyke, Chief Analytics Officer                  Page 17 of 88                      Beating the Correlation Breakdown  \\n Asymmetric, Directional Dependence Measures  \\nChatterjee ’s correlation coefficient garnered much attention  upon its publication in 2021.  This is largely \\ndue to its simplicity and ease of implementation  as a measure of non-linear, non -monotonic, regression -\\nbased, and cyclical dependence.  If X and Y pairs are ranked according to X values, with no ties on the X \\nvalues, so that \\n()()()()()() ()()() ( ) 1 1 2 2, , , , , ,nnX Y X Y X Y\\n  then:  \\n(10) \\n()1\\n1\\n1\\n23\\n, : 1  where  = rank of 1n\\nii\\ni\\nn i irr\\nchcorr X Y r Yn\\uf078−\\n+\\n=−\\n= = −−\\uf0e5  \\nUnder ties for some of the X values,  break ties uniformly at random, and  \\n(11) \\n()\\n()()()1\\n1\\n1\\n1, : 1  where  = #  such that \\n2n\\nii\\ni\\nni n ji\\nii\\nin r r\\nchcorr X Y l j Y Y\\nl n l\\uf078−\\n+\\n=\\n=−\\n= = − \\uf0b3\\n−\\uf0e5\\n\\uf0e5  \\nUnlike the big 3, Chatterjee’s new correlation coefficient ranges from zero to one asymptotically (it can \\nexceed these bounds slightly under finite samples), and a value of  zero does indicate independence.  \\nAlso, no positive or negative dependence is indicated by a positive or negative sign on the measure value.  \\nMost notably, this is an asymmetric dependence measure, that is, the order of X and Y matters: \\n(),nXY\\uf078\\ndoes not necessarily equal \\n(),nYX\\uf078 .  In other words, the dependence of Y on X is not \\nassumed to be identical to the dependence of X on Y, respectively.16  However, note that Chatterjee’s  can \\nbe made to be symmetric by simply taking the maximum of two measures, one in each direction as in \\n(12):  \\n(12) \\n()() _ max , , ,nn chcorr sym X Y Y X \\uf078\\uf078=\\uf0e9\\uf0f9\\uf0eb\\uf0fb  \\nChatterjee’s breakthrough has spawned many variants (see Lin & Han, 2023, Pascual-Marqui et al., 2024, \\nand especially Gao and Li, 2024 ).  One of these is the “improved Chatterjee’s correlation” derived by Xia \\net a. (2024), the motivation of which is to increase power by using inverse distance weightings of all \\nneighboring data values as opposed to just one.  \\n \\n16 It is important to note that herein, when using dependence measures that are asymmetric/directional, the corresponding all -\\npairwise matrix remains symmetric.  So when using, say, Chatterjee (2021), on the returns of two particular assets in the \\nportfolio,  say, X3 and X4, the value in cell row 3, column 4 of the matrix is \\n() 3, 4nXX\\uf078 , and the value in cell row 4, column 3 \\nof the matrix is identical, that is, \\n() 3, 4nXX\\uf078 ; it is NOT \\n() 4, 3nXX\\uf078 . \\n \\nJD Opdyke, Chief Analytics Officer                  Page 18 of 88                      Beating the Correlation Breakdown  \\n (13) \\n(),11\\n3n\\nij\\nij IM\\nn n\\nijr r i j\\nichcorr X Ynij\\uf078\\uf0b9\\n\\uf0b9−−\\n= = −+−\\uf0e5\\n\\uf0e5  \\nXia et a. (2024) test the power and level of “improved Chatterjee” against both Chatterjee and Szekely’s \\ncorrelations in an empirical study under widely varying data conditions.  Both Chatterjee’s and improved \\nChatterjee’s coefficients exhibit power under non-monotonic, non -linear, and cyclical dependence, with \\nthe latter usually winning .17 \\nInterestingly, Zhang (2023) has proposed combining Chatterjee’s and Spearman’s in an effort to obtain \\nthe best of both worlds: a dependence measure that has reasonable power under cases of  non-\\nmonotonic, non -linear, and/or cyclical dependence  (where Spearman’s has little to no power, especially \\ncompared to Chatterjee’s) as well as reasonable power under monotonic dependence (where \\nChatterjee’s has less power than Spearman’s).  \\n(14) \\n() () \\uf07b \\uf07d , , max , 5 2 ,n X Y n zcorr I X Y sr X Y \\uf078 ==  \\nZhang’s (2023) combined correlation r anges from 0 to 1 , where zero indicates independence .  This \\ndependence measure also is asymmetric due to its inclusion of Chatterjee ’s coefficient.  \\nFinally, asymmetric, directional dependence measures also can be applied only to the tails of X and Y , \\nand it is important to note that correlation breakdowns often are associated specifically with \\n(asymmetric) tail dependence: “Extensive evidence has been gathered showcasing the prevalence of \\nheavy-tailed distributions and asymmetric tail interdependence  within equity and foreign exchange \\nmarkets, particularly during times of crisis.  …This phenomenon causes markets that typically exhibit \\nminimal or no cor relation to behave similarly, often in opposition to fundamental principles.” (Pramanik, \\n2024).  One straightforward example of an  asymmetric tail dependence measure is that of Deidda et al, \\n(2023) which is essentially Kendall’s Tau applied conditionally , only when the percentile, q,  of X (or Y) is \\nexceeded:  \\n \\n17 Note that both Chatterjee (2021) and Xia et a. (2024) test against two dependence measures not explored further herein: the \\nHSIC measure of Gretton et al. (2007), and the HHG measure of Heller et al. (2013).  Both appear to have excellent power \\nunder circ ular and heteroskedastic data, and the former maintains reasonably large power under other conditions where both \\nChatterjee statistics outperform it.  While both HSIC and HHG are much more computationally intensive than either \\nChatterjee dependence measure , Sejdinovic et al. (2013) intriguingly prove that “reproducing kernel Hilbert space (RKHS) -\\nbased dependence measures are precisely the formal extensions of the [Szekely et al. (2007)] distance covariance.”  So HSIC \\nis a generalized version of Szekely  et al. (2007) that circumvents “the problem of nonintegrability of weight functions by using \\ntranslation -invariant kernels called distance -induced kernels. ”  RKHS-based dependence measures remain an active and \\nintriguing area of continuing research (for examples, see Ke, 2019; Mitchell et al., 2022; Tripathi et al., 2022;  Wahba, 2017; and  \\nZhang & Songshan, 2023 ). \\n \\nJD Opdyke, Chief Analytics Officer                  Page 19 of 88                      Beating the Correlation Breakdown  \\n (15) \\n() ()()() ( ) ,\\n11ˆ sgn sgn ,\\n2X Y i j i j i j nk\\ni j nq X X Y Y I X X Xk\\uf074−\\n\\uf0a3 \\uf0a3 \\uf0a3= − − \\uf03e\\uf0e6\\uf0f6\\n\\uf0e7\\uf0f7\\uf0e8\\uf0f8\\uf0e5  \\nwhere 1 , and q k n k n= − \\uf0a3\\n is the number of exceedences used in the tail, and \\n()I is the indicator \\nfunction ( one when true, zero otherwise ) ensuring that only the k largest observations of X are used.  Note \\nagain that generally, \\n()(),,ˆˆX Y Y Xqq\\uf074\\uf074\\uf0b9 , that is, this tail dependence measure is directional, and the affect \\nof X’s tail on Y’s tail is not assumed to be the same as that of Y’s tail on X’s tail.  \\nOther directional, asymmetric dependence measures include the QAD measure of Junker et al.  (2021), \\nand others  described in Jondeau (2016).18 \\nIt remains notable that NAbC’s broad scope allows for its application to  these asymmetric , directional \\ndependence measures as readily as it is applied to the big 3.  As seen in a later chapter, t his gives NAbC \\ngreat utility in some surprising settings .  Even as it is designed fundamentally  as a method for  robust \\nstatistical inference, when using these directional dependence measures it appears NAbC can be \\napplied to increase the power of causal models to accurately recover directed acyclic graphs (DAGs) .  \\nThis is an area of continuing research, but serves as a n example of how NAbC’s breadth of application \\ncan be useful even beyond ceteris paribus comparisons of the inferential power of competing \\ndependence measures .  Yet this remains invaluable as it stands, as such comparisons often would not \\nbe possible without NAbC .  All dependence measures have strengths and weaknesses  not only under \\ndata different conditions , but also depending on the specific questions applied researchers and \\npractitioners need to answe r, so we need to be able to test them, using the same unifying method under \\ncontrolled conditions , to determine which is most appropriate  for a given situation .   \\nHowever, t he main characteristic that unifies them for purposes of this monograph is that the all-pairwise \\nmatrix of all  of them is positive definite .  This is proven mathematically below, and covers essentially all \\ndependence measures in widespread usage , both in finance and other applied, empirical settings.  So  \\nrequiring positive definiteness really is no restriction at all, and allows for NAbC’s very widespread \\napplication  in practice .  It’s ability to define all these dependence measures’  finite-sample distributions  \\nallows it to be the unifying method of choice, if not the only method in many cases, for answering the \\ntypes of inferential questions posed above , especially for comparative , ceteris paribus analyses .   \\nTo conclude this chapter, i t is important to reemphasize here that several of the dependence measures \\nlisted above (e.g. Szekely’s, as well as some variants of Chatterjee’s (see Pascual -Marqui et al., 2024)), \\ncan be applied on a multivariate basis, in arbitrary dimensions  (see Chatterjee (2024)  and Han ( 2021) for \\nuseful survey s of these, in addition to Grothe et al. (2014), Latif and Morettin (2014), Reddi et al. (2015), Li \\n \\n18 Note that under certain conditions, such as when categorical and ordinal data are being analyzed and the number of \\ncategories between the two variables differs dramatically, even Pearson’s correlation can be unambiguously directional (see \\nMetsämuuronen, 2022, for details).  \\n \\nJD Opdyke, Chief Analytics Officer                  Page 20 of 88                      Beating the Correlation Breakdown  \\n and Joe (2024), Yu et al. (2021) and Puccetti (2022) for some approaches not covered herein19).  But \\n“positive definite” is not applied in this multivariate sense (see Cardin, 2009) ; herein it is applied strictly \\nto the all-pairwise matrix , which contains only bivariate relationships .  I explain further below in the \\nScenarios chapter some of the reasons for using the all -pairwise framework here, which arguably is most \\ncommonly used in real -world financial settings, as it remains  highly flexible, typically provides more \\nstatistical power  compared to multivariate approaches,  and simultaneously allow s for more precise \\nattribution and intervention  ‘what if’ analyses.  \\nWith this brief but important review of relevant and widely used dependence measures aside, I address \\ntheir estimation in the next chapter, before turning to  the derivation of NAbC in subsequent chapters.  \\n \\nEstimation  \\n \\nRegarding estimation of the all -pairwise matrix, the literature focuses almost exclusively on estimators \\nfor the covariance matrix and the Pearson’s correlation matrix.  This is not terribly surprising given the \\nrelatively long history and widespread usage  of Markowitz’ s portfolio framework  (see Markowitz, 1952) \\nand related models. \\n“Accurate covariance matrix prediction is crucial for portfolio optimization and risk management \\nbecause it captures the relationships and co -movements between asset returns.” (Lee et al., 2024)  \\nBut fortunately, some see the bigger picture, that these analyses can and should be broadened to ALL \\npositive definite dependence measures  \\n“Modeling covariance matrices – or more broadly, positive definite (PD) matrices – is one of the most \\nfundamental problems in statistics” (Lan et al., 2020).  \\nSo I first focus below on estimation of Pearson’s matrix, and later on the other dependence measures \\ndiscussed in the previous chapter.  \\nThe first of the two major challenges of estimating the all -pairwise matrix of any dependence measure is \\nsample size, because we are not just estimating a single parameter, say, a volatility or a beta of a single \\nasset, but rather , \\n()1\\n2 2p pp− \\uf0e6\\uf0f6=\\uf0e7\\uf0f7\\uf0e8\\uf0f8  pairwise associations.  To do this accurately and with reasonable \\n \\n19 Note that the primary focus of the development of many  multivariate dependence measures is on testing the null hypothesis \\nof (multivariate) independence , and thus, on the level  and power  of this specific test for these measures .  While this objective \\nis foundational, th at of this monograph is not only on this binary question , but also on the practical and accurate \\nmeasurement of the magnitude of such dependence (measured using bivariate associations) when it exists.  Consequently , I \\nfocus on dependence measures in the literature with strong results related to their statistical  power, level control, ease of \\nimplementation, low computational complexity, and attainment of the full range of values they are meant to attain under the \\nrelevant sample spaces.   More importantly, relying on the bivariate, as opposed to multivariate, relationships measured in the \\nall-pairwise matrix is critical to the scenario flexibility provided by NAbC, as explained in later chapters . \\nJD Opdyke, Chief Analytics Officer                  Page 21 of 88                      Beating the Correlation Breakdown  \\n precision, we need more data  than is needed for a single estimate .  Regarding accuracy, the sample \\ncovariance matrix, and thus, the sample Pearson’s matrix are consistent estimators, that is, they are \\nasymptotically unbiased.  But regarding precision, their estimates will be way too variable to be useable \\nor useful, not  to mention biased  in the finite -sample case , in the absence of large(r ) data samples.  A \\nwidely recognized rule of thumb is that the sample size needs to be at least ten times the dimension  of \\nthe matrix (N ≥10p; see Bongiorno  et al., 2023), but this arguably depends on the method used to \\nestimate it.  For example, Bun et al. (2016) devise a rotationally invaria nt estimator that “cleans” or “de -\\nnoises” the estimate of  Pearson’s  matrix using functions of its eigen values, a method for which they \\nargue that N ≥2p is sufficient.  Note that the estimators of all  the dependence measures presented herein \\nare at least asymptotically  unbiased, and that some researchers believe the sample size issue has  been \\naddressed as well as it can be, especially if the best methods are being used ( see Bouchaud, 2021: “Now \\nthe data problem is solved as best as possible…” referring to Bun et al., 2023, among others).  With this in \\nmind, currently it would appear that Bun et al. (2016 ) is the state-of-the-art estimator for the \\nunconditional  estimate of Pearson’s matrix  (see du Plessis & van Rensburg (2020) for a comparison \\nstudy).  But unconditional estimate s assume the values of the matrix do not change over time, that  is, \\nthat the financial time series data is stationary.  And this brings us to the second major challenge when \\nestimating any all -pairwise dependence matrix: non -stationarity.   \\nAs Bouchard  (2021) rightly points out,  portfolio frameworks like that of Markowitz (1952), and really any in \\napplied usage, require  knowledge of the dependence measure (here, Pearson’s ) to be representative of \\nthe future realized correlations , because financial data is not stationary (i.e. its conditional distribution \\nchanges over time).  Therefore , we need a forecast, into the near -term future, of the conditional  \\nPearson’s matrix.  And a very compelling one, namely, “Average Oracle ” (AO), is exactly what is provided \\nby Bongiorno  et al. (2023)  (see also Bongiorno  & Challet, 2023a, for an extensive empirical study against \\ncompetitors ).  Conceptually AO is very straightforward: based on the  eigen decomposition of Pearson’s \\nmatrix, it uses the (oracle) covariance of the next-period ‘future’ with the eigenvectors of the adjacent \\npast period to obtain eigenvalues that, when averaged over many samples, embed the desired, dynamic  \\ntime effects for a robust forecast of Pearson’s matrix.   Somewhat surprisingly, this intuitive method \\noutperforms all flavors of advanced “shrinkage,” both non -linear (see Ledoit & Wolf, 2017) and quadratic \\n(see Ledoit & Wolf, 20 22a, 2022b) as well as DCC and NLS combinat ions (see Engle et al., 2019).  It is \\nfast, straightforward to understand and implement, and importantly, fully nonparametric.  AO’s \\noutperformance of the widely used NLS approach perhaps should not be so surprising given that \\nBongiorno & Challet (2023 b) recently proved that NLS is not optimal for portfolio optimization, as was \\nwidely believed , because it does not optimize under non -stationarity .20  So I recommend  AO as the \\ncurrent state -of-the-art conditional estimator of Pearson’s matrix .  However, this literature is vast, \\ncomprising easily many hundreds of papers if covariance estimation is included , and given the current \\nrapid pace of research in this area, it is certainly possible  that new, worthy competitors exist, especially \\n \\n20 For those that view shrinkage favorably in general, an improved shrinkage competitor with arguably better properties than \\nNLS is that of Kelly et al. (2024)).  \\n \\nJD Opdyke, Chief Analytics Officer                  Page 22 of 88                      Beating the Correlation Breakdown  \\n under specific data conditions (see include Zhang et al. (2022) , Zhang et al. (2023), Vanni et al. (2024)  and \\nZhangshuang et al. (2025)  for such recent possible examples).  \\nFor estimation , conditional or unconditional,  of the all-pairwise matrices based on the other dependence \\nmeasures listed in the previous section,  the literature has little to offer  beyond the fact that all of the \\nsample estimators presented  in the previous chapter  are at least asymptotically unbiased  (see Zhao et \\nal. (2014) for an exception) .  So as long as sample sizes are sufficiently large these estimators will retain \\ngood statistical properties.  However, I offer two additional suggestions below for pos sible \\nimprovements, the latter of which is an active research paper I am pursuing.  The first is simply the \\ninverse of a common robustification technique using a well established relationship between Pearson’s \\nand the rank -based measures, Kendall’s and Spe arman’s.  Estimates of (bivariate) Kendall’s Tau or \\nSpearman’s Rho often are used to robustify those of Pearson’s using the widely known relationships of \\n() sin 2r\\uf074\\uf070=\\nand \\n()() 2sin 6r sr \\uf070 = , respectively, which are valid under iid elliptical data distributions  \\n(see Sheppard, 1899; Greiner, R. , 1909; Lindskog et al., 200 3; Heinen & Valdesogo , 2022; McNiel et al., \\n2005;and Hansen & Luo, 2024 ; and for advanced methods on this, see Barber & Kolar, 2018, and Niu et \\nal., 2020).  Yet under specific, known data conditions that are non -elliptical, it may be demonstrable that \\nthese transformations remain reasonably accurate  (see Hamed (2011) and  Hansen & Luo (2024) for \\nexamples).  In this case, given a strong estimator of Pearson’s from an improved estimation method  like \\nthose described  above (e.g. Average Oracle  of Bongiorno  et al. (2023) ), the inverses  of these functions  \\ncould be used to obtain estimates for the all-pairwise matrices of Kendall’s and Spearman’s that likely \\nwould share some of the benefits of an improved estimate for Pearson’s matrix, especially when it is \\nconditional.  Note, however, that these transformations  would require verifying, and sometimes \\nenforcing , positive definiteness ex post  (see McNeil et al.  (2015) and Higham (2002) ).  Also, they are \\nlimited to Kendall’s and Spearman’s.  \\nHowever, a much broader and  more promising  (if still unproven ) approach , would be to use Average \\nOracle on ANY of the above-mentioned measures directly , simply replacing Pearson’s matrix but keeping \\nthe methodology otherwise identical .  Again, because their all-pairwise matrices  all are positive definite, \\nthey will have valid  eigen decompositions wherever the covariance matrix will  (and even in some cases \\nwhere the covariance matrix is singular) , and the ‘training’ eigenvectors of the adjacent past can be u sed \\nin exactly the same manner with next-period ‘future’ all -pairwise dependence matri ces to obtain \\n(averaged) eigenvalues that embed the measured , average , empirical time dependency .  This is the \\nsubject of my continuing research, but  like the original method of Bongiorno  et al. (2023), preliminary \\nresults appear very promising.  \\nPromising additional approaches to estimation aside, a ll the above reemphasize s the fact  that NAbC is \\nnot an estimator  of any of these dependence measures : rather, it provides the finite-sample distribution \\nof the estimate, for any estimator of any dependence measure (that is positive definite ) under any real-\\nworld data conditions .  This allows us to make actionable inferences about dependence structure in a \\nunified way, allowing for comparative, ceteris paribus analyses .  The literature to date  provides such \\ndistributions  in a highly piecemeal fashion for some of the dependence measures under some (often very \\nJD Opdyke, Chief Analytics Officer                  Page 23 of 88                      Beating the Correlation Breakdown  \\n limited and/or unrealistic) data conditions for some (often very limited and/or unrealistic) ranges of \\nvalues.  The derivations often are extremely complex and unwieldy and unusable for many practitioners .  \\nNAbC sidesteps all of those problems  with a single, unified, and straightforward method.  Estimation of \\nthe all-pairwise matrix is the only thing out -of-scope for NAbC, but in a sense this is a strength of the \\napproach  since it permits NAbC to remain “estimator agnostic, ” allowing for its application on any \\nreasonable and relevant estimator of the all -pairwise matrix  and providing the flexibility  to use those that \\nare most robust and/or most precise and/or most accurate – or any combination thereof – under different \\nconditions.   So I do not need to reinvent the already well -made wheel of estimation here21: we get \\ntremendous benefit, not previously attainable, b y applying NAbC t o obtain the finite sample d istribution  \\nof the estimate, and thus, make statistically valid and actionable inferences about our portfolio’s \\ndependence structure.  Derivation and application of NAbC follows below in the next chapter.  \\n \\nNAbC: (Robust) Statistical Inference  \\n \\nBrief Literature review of Pearson’s Matrix: Distributional Results and Sampling Algorithms  \\nI begin with Pearson’s product moment correlation matrix, the oldest and arguably most widely used \\nmeasure of dependence.  Although its limitations often are mischaracterized or misunderstood, \\nespecially as they relate to widely held views classifying it strictly as a measure of linear association (see \\nvan den Heuvel & Zhan, 2022), in many setting s it remains either optimal or centrally relevant for wide -\\nranging purposes .22  These include  robust asset allocation (Welsch and Zhou, 2007), Black -Litterman \\nvariants (Meucci, 2010a, Qian and Gorman, 2001), entropy pooling with fully flexible views (Meucci, \\n2010b), portfolio optimizations combined with random matrix theory (Pafka and Kondor, 2004), stress \\ntesting (Bank for International Settlements, Basel Committee on Banking Supervision, 2011 a), and even \\nnon-linear, tail -risk-aware trading algorithms (Li et al., 2022, and Thakkar et al., 2021) to name a few.  \\nConsequently, Pearson’s is the foundational dependence measure we start with  (see also Rodgers & \\nNicewander (1988)  for a broad , useful, and applied  introduction to Pearson’s).  \\nWhen it comes to statistical inference and simulation -based decision-making, the extant literature on \\nPearson’s matrix can be placed roughly into two categories: 1.  distributional derivations that preserve \\ninferential capabilities , but usually under narrow , limiting,  and often unrealistic constraints;  and 2. \\nsample-generating algorithms that  either share the same limitations, or  attempt to generate stylized, \\nreal-world distributions  but fail to preserve inferential  (probabilistic)  validity.  The latter often are used \\nindirectly for such purposes as scenario analytics and stress testing.  Of course, we want both  worlds: \\n \\n21  With the possible exception, which I currently am researching, of applying AO to dependence measures beyond Pearson’s, \\nwhich could be a notable improvement , as it is for Pearson’s,  over any other method for forecasting their conditional values.  \\n22 In addition to the linearity ‘myth’ effectively addressed in van den Heuvel & Zhan  (2022), note also that while Pearson’s, \\nunder dependence, does not retain invariance under marginal transforms generally, the set of cases where it does retain \\ninvariance is broader than previously thought (see Koike et al., 2024).  \\n \\nJD Opdyke, Chief Analytics Officer                  Page 24 of 88                      Beating the Correlation Breakdown  \\n robust, fast, straightforward algorithms to generate samples when needed (i.e. in the absence of fully \\nanalytic solutions), that also preserve inferential capabilities, so that  we can base consequential \\ndecisions on rigorously defined probabilities.  \\nI begin with a brief and admittedly non-comprehensive , but well-targeted literature review of 1. and then \\n2. under typically more restricted cases, and then treat both under more general conditions .  \\nSubsequently I develop NAbC under both a narrowly defined  but foundational case,  and then fully \\ngeneral conditions  that satisfy the eight original objectives .  Defining NAbC under a narrow case  provides \\na fully analytic version of it that very transparently  shows how NAbC accomplishes both purposes  above \\n– useful sample generation and valid statistical inference, simultaneously  – while also serving as a \\nhelpful referential baseline for NAbC’s generalization to all dependence measures , under all data \\nconditions.  \\nDistributional Results  \\nDerivations of the distribution of Pearson’s matrix go all the way back to the father of modern statistics, \\nSir Ronald A. Fisher (see Fisher, 1915, 1928).  Intriguingly, Fisher (1928) recognizes the relationship \\nbetween the Pearson’s correlation formulae and the cosine between the angles of the two data vectors  in \\nthe bi-variate case , i.e. what is now widely referred to as “cosine similarity”  (described in more detail \\nlater).23  He builds on this in  his derivations ( as does NAbC below ), and although without closed forms \\nsome of the  mathematical results  prove unwieldy, they are foundation al for those (re)derived below.  \\nJoarder and Ali (1992) replicate some of Fisher’s earlier results (see Fisher, 1915), and more generally \\nderive the distribution of Pearson’s for any dimension when the underlying data is elliptically distributed  \\n(which includes the case of Gaussian data) .  Their density, however, requires iterated integration on the \\norder of the dimension of the matrix, so l ike many of Fisher’s results, while mathematically  correct, it \\nremains unscalable and less readily implemented . \\nFor more recent results, below I s tart with narrow ly defined  cases and then expand.  Restrictions  on the \\nnarrow cases  include i. on the underlying data (e.g. only Gaussian); ii. on the dimension of the matrix  (e.g. \\nonly the bivariate case of p=2); iii. on  the values of the matrix (e.g. only the identity matrix , where all \\ncorrelations equal zero ); and iv. with a priori known, rather than estimated , parameter values (e.g. known \\nvariances).  \\n• Gaussian data, any matrix,  p=2  \\n \\n23 Briefly, this widely used mathematical relationship recognizes that the cosine of the angle between two mean -centered data \\nvectors equals Pearson’s (bivariate) correlation coefficient, as below:  \\n(16) \\n()1 1 1 ,\\n,22\\n1 1 1 1inner product ˆˆcos , 0product of normsN N N\\ni j i j\\ni j j XY\\nXY\\nN N N N XY\\ni j i j\\ni j i jX X Y Y\\nCovrss\\nX X X Y\\uf071 \\uf071 \\uf070= = =\\n= = = =\\uf0e6 \\uf0f6\\uf0e6 \\uf0f6−−\\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0e8 \\uf0f8\\uf0e8 \\uf0f8= = = = = \\uf0a3 \\uf0a3\\n\\uf0e6 \\uf0f6 \\uf0e6 \\uf0f6−−\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf0e8 \\uf0f8 \\uf0e8 \\uf0f8\\uf0e5 \\uf0e5 \\uf0e5\\n\\uf0e5 \\uf0e5 \\uf0e5 \\uf0e5X,Y\\nXY  \\nJD Opdyke, Chief Analytics Officer                  Page 25 of 88                      Beating the Correlation Breakdown  \\n For Gaussian data with matrix dimension p=2, i.e. the bi -variate case,  Taraldsen (2021) derived the exact \\nconfidence distribution of Pearson’s correlation:  \\n(17)\\n()()()()()()()\\n()1 2 2 2 1 2 222\\n211 1 1 3 1 1 1| = , ; ;   where 2 2 2 2 2 1 2,1 2vv vrr rr F v\\nBv\\uf072\\uf072 \\uf072\\uf070\\uf072−− −− \\uf0d7 − \\uf0d7 − + \\uf0e6\\uf0f6\\uf0d7 − +\\uf0e7\\uf0f7+ \\uf0e8\\uf0f8  \\n()()()() ,  the Beta function, 1 1,and  is the Gaus sian hypergeometric function where B X Y X Y X Y v n F= \\uf047 \\uf047 \\uf047 + = − \\uf03e\\uf0e9\\uf0f9\\uf0eb\\uf0fb\\n \\n\\uf05b\\uf05d()()\\n()()()()()()21 0, ; ;   where 1 2 1 ,  1,  1!n\\nnn\\nn\\nnnab zF a b c z h h h h h n n hcn\\uf0a5\\n= \\uf0d7 = + + + − \\uf0b3 =\\uf0e5\\n24 \\nIt is important to note here that Taraldsen (2021) shows that the Fisher’s Z -transformation  (Fisher, 1921) , \\na widely used approximation of th is distribution, loses accuracy as  correlation  values approach one  or \\nnegative one , especially for smaller samples.  \\n• Gaussian  data, identity matrix  only, p≥2 \\nFor the Gaussian identity matrix  (all correlations of zero)  with p≥2, Gupta & Nagar (2000) derive the \\ndensity  \\n(18)\\n()()()\\n()()()()22\\n14\\n, , ,\\n112\\n, 1 1, 1, 1 ,  and 1 2 2\\n12pnp\\np\\npp\\ni j j i i i p\\nipnR\\nf R r r r i j p n n i\\nn\\uf070−−\\n\\uf0e9\\uf0f9−\\uf0eb\\uf0fb\\n=\\uf0e9\\uf0f9\\uf047−\\uf0e9\\uf0f9\\uf0eb\\uf0fb\\uf0eb\\uf0fb= − \\uf0a3 = \\uf0a3 = \\uf0a3 \\uf0a3 \\uf047 − = \\uf047 − \\uf0e9 \\uf0f9 \\uf0e9 \\uf0f9\\uf0eb \\uf0fb \\uf0eb \\uf0fb\\uf047−\\uf0e9\\uf0f9\\uf0eb\\uf0fb\\uf0d5  \\nAlthough Pham-Gia & Choulakian (2014) claim this is a new result, it actually is identical to that of \\nJoarder and Ali (1992)  under these conditions (see (4.2)) , and after some manipulation, that of Fisher \\n(1915) for the bivariate case  (see (4.2), (4.3), and (3.1) in Joarder & Ali, 1992). \\n• Gaussian data, any matrix, with p ≥2: \\nUnder Gaussian data, with p ≥2, Pham-Gia & Choulakian (2014) provide  the distribution of the sample \\nPearson’s matrix under any values, not just the identity matrix:  \\n(19) \\n()()\\n()()()2,,\\n,, 22\\n1\\n,\\n11 2 exp\\n12i j i j\\nij i i j j np\\nnpp\\np i i\\nisn\\nf R R\\nn\\uf06c\\n\\uf073\\uf073\\n\\uf06c\\uf03c−−\\n−\\uf0e9\\uf0f9\\uf0eb\\uf0fb\\n=\\uf0ec\\uf0fc\\uf0ef\\uf0ef\\uf0e9\\uf0f9\\uf047 − −\\uf0e9\\uf0f9 \\uf0ed\\uf0fd \\uf0eb\\uf0fb\\uf0eb\\uf0fb\\uf0ef\\uf0ef\\uf0ee\\uf0fe=\\n\\uf0e9\\uf0f9\\uf047 − \\uf04c\\uf0e9\\uf0f9 \\uf0ea\\uf0fa \\uf0eb\\uf0fb\\uf0eb\\uf0fb\\uf0e5\\n\\uf0d5  \\n \\n24 Interestingly, the Gaussian hypergeometric function makes many appearances in this and related settings: i.  in derivations of \\nthe distribution of individual (bivariate) correlations (besides Taraldsen, 2021 , see also Muirhead, 1982) ; ii. in moments of the \\nspectral distribution under some conditions (see Adams et al. 2018, and \\nhttps://reference.wolfram.com/language/ref/MarchenkoPasturDistribution.html ); iii. in the cumulative distribution function of \\nPearson’s under the Gaussian identity matrix of any dimension (see Opdyke (2022, 2023, and 2024) ; and iv. in the definition of \\npositive definite functions ( see Franca & Menegatto, 2022).  \\nJD Opdyke, Chief Analytics Officer                  Page 26 of 88                      Beating the Correlation Breakdown  \\n \\n\\uf07b\\uf07d ()()()14\\n,\\n1with sample covariance , 1 , 1 2 2 ,p\\npp\\ni j p\\nis i j p n n i \\uf070\\uf0e9\\uf0f9−\\uf0eb\\uf0fb\\n=\\uf0a3 \\uf03c \\uf0a3 \\uf047 − = \\uf047 − \\uf0e9 \\uf0f9 \\uf0e9 \\uf0f9\\uf0eb \\uf0fb \\uf0eb \\uf0fb \\uf0d5 \\n\\uf07b\\uf07d1\\n,, known variance ,  is the determinant fun ction,  is the true correlation matrix, and  the diagonals of i i i i\\uf073\\uf06c−\\uf04c\\uf04c\\n \\nThe limitations  of Pham-Gia & Choulakian (2014) include the requirement of a priori knowledge of true \\n(not estimated) variances , and of course, its validity only for normally distributed data .  It also arguably is \\nquite cumbersome to implement.   \\nSampling Algorithms  \\nMoving now to  sample generation under various ‘non-generalized’ conditions , i.e. conditions that are not \\ngeneralized to those common in financial portfolios , the literature provides  a number of  methods , many \\nof which are quite involved.   Note that I have focused on more recent ones, as these usually explicitly \\nsubsume previously published algorithm s, and many of the below are even compared against each other  \\nin their own empirical simulation studies .  Note that none of these  are tailored to generate  the stylized \\nempirical characteristics observed  in financial portfolios , and hence are labelled here as ‘non-\\ngeneralized’ .  \\ni. The onion  and c-vine methods , the former of which can generate random correlation matrices \\nwith the joint density of the correlations being proportional to a power  of the determinant of the \\ncorrelation matrix , and the latter of which is based on partial correlations specified in a vine \\ncopula (specifically, a c -vine copula) . (Lewandowski et al., 2009)  \\nii. the chordal sparsity method of Kurowicka (2014) , which generalizes Lewandowski et al.  (2009), \\nalthough “i t is not clear whether it is possible  to extend them to other patterns of unspecified  \\ncorrelations ” beyond those with chordal sparsity patterns . \\niii. The restricted Wishart distribution approach of Wang et al. (2018) , which is equivalent to \\nLewandowski et al.  (2009) but somewhat more efficient . \\niv. The hyperspherical coordinate approach of Pourahmadi et al. (2015)  \\nv. The Cholesky -Metropolis method of Cordoba et al. (2018) , which claims to be faster than the \\npreviously listed method s. \\nvi. The direct formulation method of Madar (2015)  \\nvii. The flexible bijection method of Veleva (2017)  \\nviii. The rejection  algorithm  of Makalic and Schmidt (2018) , which is based on the polar  \\n(hyperspherical)  angles representation of  Pearson’s matrix25 \\n \\n25  See Joarder & Ali (1992), Pinheiro & Bates (1996), Rebonato & Jaeckel (2000), Rapisarda et al. (2007) , and Pourahmadi & \\nWang (2015) .  The use of spherical angles for analysis of Pearson’s matrix goes back at least to Fisher (1915), but Joarder & Ali \\n(1992) and Rapisarda et al. (2007)  provide a geometrically motivated, thorough, and clear description s of its derivations, and \\nRebonato & Jaeckel (2000)  appears to have been the first to propose its application in financial settings.  \\nJD Opdyke, Chief Analytics Officer                  Page 27 of 88                      Beating the Correlation Breakdown  \\n Makalic and Schmidt (2018)  is treated in more detail below.  Implementation of a ll but vi., vii., and viii. \\nabove arguably remain quite involved, but one of the self-described  focuses of most of these is \\ncomputational efficiency  (which is not surprising as they are sampling algorithms).  \\n“This new method is faster than the original onion method for generating random matrices, especially in \\nthe low dimension (T < 120) situation. ” Wang et al., 2018  \\n“…comparing the HP and C -Vine algorithms, it is evident that the HP algorithm is faster for larger n and is \\nslower for smaller n.” Pourahmadi et al., 2015  \\n“We have also executed a comparative performance study, where our approach  has yielded faster results \\nthan all of the related approaches in the literature. ” (Cordoba et al., 2018) , comparing against  the c-vine \\nand onion methods  of Lewandowski et al.  (2009) and the hyperspherical approach of Pourahmadi et al. \\n(2015), but not Makalic and Schmidt, 2018  \\n“…emphasizes the efficiency of the proposed algorithm.” (Makalic and Schmidt, 2018)  \\nFrom a close read of the runtime results of the successively published and compared algorithms  above, \\nit appears that Makalic and Schmidt (2018) is the fastest among them  (excepting those of Madar (2015)  \\nand Veleva (2017) , which have not been compared to the others) .  However, as discussed in more detail \\nbelow, Roman (2023) shows that for the case of the Gaussian identity matrix, when NAbC is used as a \\nsampler, it is over 30% faster than Makalic and Schmidt (2018); when NAbC is used analytically, its \\nresults are, for all intents and purposes , instantaneous , as can be seen in the excel workbook at the \\nfollowing link  (see http://www.datamineit.com/JD%20Opdyke --The%20Correlation%20Matrix -\\nAnalytically%20Derived%20Inference%20Under%20the%20Gaussian%20Identity%20Matrix --02-18-\\n24.xlsx).  Runtimes of NAbC under the fully general case, i.e. that of fully realistic financial data \\nconditions (which none of the algorithms above claim to cover),  as well as any  valid matrix values,  are \\ndiscussed  in later chapters below.  \\nBut beyond speed, the more important issue regarding these sampling algorithms is that none preserve \\ninferential validity, on a sample -by-sample basis, by providing a readily calculated cumulative \\ndistribution function  (cdf) value (probability density function (pdf) values will be more cumbersome and \\nless useable here) .  In other words, to make these simulation results truly useful for precise, powerful \\nhypothesis testing and other inferential purposes , we need to know where in the distribution of \\ncorrelation matrix samples a particular correlation matrix sits: what probability is it associated with?  \\nCalculating this  (cdf value)  is tortured, if not  impossible for these methods, although this arguably should \\nbe the primary focus of such algorithms.   \\nThe counter argument is that the group of samples that these algorithms generate, taken as a whole, is a \\nvalid representation of the data generating mechanism behind the specified  correlation matrix.  This \\ngroup of samples can then be used as inputs, one -by-one, to broader and arguably real -world portfolio \\nsimulations.  While this is certainly true , at best the group of sample correlation matrices, then, only \\nprovide indirect inferential value , with what is arguably a notable lack of control.  For example, the group \\nof samples cannot be used to specify, for controlled portfolio simulations, the two matrices representing \\nJD Opdyke, Chief Analytics Officer                  Page 28 of 88                      Beating the Correlation Breakdown  \\n the 95% confidence interval under a given null hypothesis for the dependence structure, which is the \\nkind of targeted, controlled capability needed for precise, powerful testing and consequent, targeted \\ndecision-making.  Some may argue that the group of sample  matrices can be used with ad hoc measures \\nof ‘distance’ from a hypothesized matrix of ‘true’ values (e.g. a Euclidean ‘norm’ distance from, say, the \\nidentity matrix) , yet such multivariate distance s can be measured in many different and equally valid \\nways under various conditions  (this is addressed in more detail in the Generalized Entropy chapter \\nbelow).  The same ‘distances’ also can have different interpretations under different conditions , and even \\nwidely used ones can be ‘wrong’ when applied to very commonly used dependence measures in this \\nsetting (see Zhang et al., 2024, for a compelling example) .  So making inferences based on them remains \\narguably as ad hoc , at best, as the arbitrary choice of how to measure distance between a sample matrix \\nand its null hypothesis .  Neither can such distances be used to rank order the sampled matrices to \\nobtain, say, an empirical cdf, because different distances will yield different rank orderings.  The only \\n‘distance’ that avoids these issues is probability itself , most conveniently represented as a cdf value . \\nIn the end, for real inferential capability and consequential decision -making ability, w hat is necessary \\nhere is an analytically rigorous connection between a specific  sampled correlation matrix and its \\nassociated , properly defined cdf value , and none of these sampling methods prov ide this.  Fortunately, \\nNAbC does, as is discussed further below.   But first, I finish reviewing results from the extant literature \\nthat cover 1. distributional derivations of Pearson’s matrix , but under the more general case of realistic, \\nfinancial returns data, as well as 2. sample generation algorithms of Pearson’ s matrix under these same \\nconditions  (which should correspond to their stylized, empirical characteristics) .  For 1., I cover three \\nrecent and intriguing methods, labelled A.-C. below.  \\nDistributional Results, More General Conditions  \\nA. Archakov & Hansen (2021) introduce a n original parameterization of Pearson’s  matrix that maps \\nuniquely, one -to-one, to the positive definite space, thus providing a density for inference.  It is valid \\nunder general conditions, based on the Fisher Z transformation, remains invariant to reorderings of the \\nvariables /assets, i.e. the rows and columns of the matrix , and is accompanied by an algorithm that \\nprovides the inverse mapping from the parameterization to the correlation matrix  (i.e. a matrix level \\nquantile function) .   \\nThis approach is unique , but the method still has limitations.  As th e authors state, “This makes the \\ntransformation potentially useful for … inference.  These attributes tend to deteriorate as C approaches \\nsingularity.  This is not unexpected, because it is also true for the Fisher transformation when the \\ncorrelation is cl ose to ±1.”   As previously noted, Taraldsen (2021) similarly shows that the approximate \\ndensity of the pairwise correlation using Fisher’s Z -transformation loses accuracy as  correlation values \\napproach  ±1, especially for smaller samples.  This is consistent with the authors’ comments here, but \\nthey state this may only be material under extreme conditions.  All else equal, having a method that \\navoided this non -robustness issue altogether would be preferrabl e, especially because such correlation \\nbreakdowns are exactly when we most need robust, accurate inferences.  \\nJD Opdyke, Chief Analytics Officer                  Page 29 of 88                      Beating the Correlation Breakdown  \\n Also, the method only provides the distribution  of the entire correlation matrix: it does not appear to be \\nable to modify correlation matrices, cell -by-cell, probabilistically, based on their individual distribution , \\nfor things like  scenarios and  ‘what if’ analyses .  While this may not be a  stated objective of the method, all \\nelse equal  it would be an incredibly useful feature for stress testing and scenario analysis , as well as \\nattribution analyses .  The same holds true for larger submatrices of the matrix , i.e. submatrices large r \\nthan one cell , representing one pairwise association.   Note that NAbC shares none of these limitations, \\nbut shares all of the method ’s advantages  listed above , in addition to many others.  \\nB. Lan et al. (2020) take a fully Bayesian approach to this problem for both covariance and Pearson’s \\ncorrelation matrices.  Similar to NAbC, they use the Cholesky factorization to automatically enforce  \\npositive definiteness, and by defining distributions on spheres as NAbC does , utilize a large class of \\nflexible prior distributions.  This method includes estimation  of the correlation /covariance  matrix, which \\nNAbC does not  as described above , but it also lacks very important features that NAbC provides.  As  the \\nauthors state , “The priors for correlation matrix specified through the sphere -product representation are \\nin general dependent among component variables.  For example, the method we use to induce \\nuncorrelated prior between  \\n()  and  by setting 0 for i j jky y i j l k i \\uf03c \\uf0bb \\uf0a3 has a direct consequence that  \\n() Cor , 0 for .ijy y i i\\uf0a2\\uf0a2\\uf0bb\\uf0a3\\n  In another word, more informative priors (part of the components are correlated) \\nmay require careful ordering in  \\n\\uf07b\\uf07d.iy  To avoid this issue, one might consider the inverse of covariance \\n(precision) matrices instead.  This leads to modeling the conditional dependence, or Markov network  …  \\nOur proposed methodology applies directly to (dynamic) precision matrices/processes, which will be our \\nfuture direction.”   \\nFortunately, NAbC does not suffer from this order -dependence problem.  Like Archakov & Hansen (2021)  \\nits results are invariant to the ordering of the rows and columns of the matrix, but unlike Archakov & \\nHansen (2021)  or Lan et al. (2020) it can ‘freeze’ any submatrix of the correlation matrix , even if it is non -\\ncontiguous , as dictated by any particular scenario or stress test, and still obtain a valid, finite -sample \\ndistribution for the matrix.  There are no unintended ‘dependencies’ between cells that con found these \\nresults, whether the matrix is restricted or entirely unrestricted.  As discussed further below, the \\n‘unintended dependencies’ problem  is one that other researchers have struggled with, but which NAbC \\navoids altogether.  \\nC. Like Lan et al. (2020) , Ghosh et al. (202 1) also take a fully Bayesian approach to this problem, and just \\nlike NAbC, they reparameterize Cholesky factors in terms of hyperspherical coordinates where the angles \\nvary freely in the range [0, π). Their focus is on estimation, although as a Bayesian approach it is \\ncomprehensive  and provides credible regions.  Among its arguab le limitations, however, is that its use is \\nrestricted to parametric priors, which given the non -small dimensions of most financial portfolios (e.g. \\nBongiorno  & Challet (2023a) call p=100 , which has p(p -1)/2=4,950 pairwise cells, ‘mid-sized’) it is hard to \\nsee how this would not limit its implementation under complex , real world financial data conditions ( i.e. \\nmultivariate distributions of dimension p=100 + with margins with different and varying degrees of serial \\ncorrelation, asymmetry, (non)stationarity, and heavy -tailedness).  In other words, it is hard to imagine a \\nJD Opdyke, Chief Analytics Officer                  Page 30 of 88                      Beating the Correlation Breakdown  \\n fully parametric multivariate distribution  of dimension p=100 or greater  that was analytically tractable but \\nsimultaneously able to  adequately incorporate all of the characteristics listed above, regardless of the \\nshrinkage or selection priors used .  In contrast, NAbC makes use of flexible nonparametric kernels that fit \\nANY angles distribution resulting from ANY data generating mechanism (with finite mean and variance  for \\nPearson’s matrix ).  Also, like A. and B. above , the approach of Ghosh et al. (2020) ap proach does not \\nappear to have the capability of modeling submatrices while leaving select cells of the correlation matrix \\n‘untouched .’  This is absolutely essential for flexible and realistic scenario modeling and (reverse) stress \\ntesting, and one of the many advantages NAbC provides.   Now I treat some of the more recent, general-\\ncase sample generation algo rithms, listed as D. -F. below.   \\nSampling Algorithms, More General Conditions  \\nD. Marti (2019) proposes using generative adversarial networks (GANs) to incorporat e the stylized \\nempirical characteristics  of financial portfolios’ correlation matrices  into an algorithm that directly \\ngenerates samples of the all-pairwise matrix (CorrGAN) , as opposed to samples of returns .  This appears \\nto be the first method to attempt this approach .  The stylized characteristics include positive -shifted \\ncorrelations, Marchenko -Pastur distributed correlations excepting a few large eigenvalues, Perron -\\nFrobenius property (positive entries of the first eigenvector), hierarchical correlation structure, and scale -\\nfree property of the corresponding minimum spanning tree.  Marti (2019) does not address how \\ncomputationally intensive  is the method, but apparently it is not prohibitively so as he implements it on \\n100x100 matrices in a follow -on blog post (see Marti, 2020 ).   \\nThere are three main l imitations  to this approach.  First, as the author notes, while it appears to capture \\nmost of the identified distributional stylized facts, it does not capture the tails well.  This arguably is the \\nmost important part of the distribution, as it is critically related to portfolio risk analytics, and many if not \\nmost scenarios, especially those that incorporate events like correlation breakdowns.  In addition to \\ntrading algorithms, these are the stated purposes of the method, so difficul ty estimating the tails of the \\ndistribution is not a minor limitation.  Secondly, the method generates samples that “are not exactly \\ncorrelation matrices” with non -unit diagonals and negative eigenvalues.  Marti (2019) states that positive \\ndefiniteness  is enforced ex post using Higham (2002).  I have used Higham (2002) extensively in my \\nresearch in this setting, closely examining its effect s on both the spectral distribution and the distribution \\nof the correlation matrix itself, and have found that both can be dramatically distorted when Higham \\n(2002) is used.26  However, simply dis carding non -positive definite samples is not the answer as this, too, \\nwill distort its true distribution.  The only way to simulate  the matrix and retain inferential validity , at least \\nas it is affected by its eigen decomposition, is to use a method that automatically enforces positive \\ndefiniteness, ex ante.  Finally, the last limitation is that the samples generated by the method do not \\n \\n26 To be clear, this is not a critique of Higham (2002), which is seminal and extremely useful in wide -ranging, applied settings.  \\nRather, it is only to say that ‘fixing’ non -positive definite matrices that are generated by non -trivially complex algorithms o ften, \\nif not usually, strongly distorts the distribution  of the sample matrices, as well as the associated spectral distribution.   This is \\nreadily verified empirically.  \\nJD Opdyke, Chief Analytics Officer                  Page 31 of 88                      Beating the Correlation Breakdown  \\n retain inferential validity  generally , that is, they are not associated with a probability of occurrence  or a \\ncdf value , as described above . \\nE. Papenbrock et al. (2021) develop a novel  and intriguing  approach to simulating correlation matrices for \\nfinancial markets using evolutionary algorithms.  These allow for the flexible yet robust incorporation of \\nmany observed features of real -world financial correlation matrices  (the list is similar to that of Marti \\n(2019), with some enrichments ).  The algorithm scales well and can be used for backtesting, pricing, and \\nhedging correlation -dependent investment strategies and financial products.  However, it has several \\nlimitations: the first relates to how upper and lower barriers are established for the sampled correlation \\nmatrices, which the authors describe as “This neighborhood could be defined in a static way or by expert \\nknowledge.”  Regarding the latter option, m aking this criteri on (strictly) subjective arguably defeats the \\npurpose of objective, quantitative analys is in this setting .  Regarding the former option, Papenbrock et al. \\n(2021) suggest using the most extreme values  of the matrices , although none of the implementations \\nlisted (e.g. random matrix de noising, shrinkage, or exponential weighting) are inferentially valid in \\nthemselves, that is, they do not allow for probabilistic inference.  Arguably, if the range of the matrices \\nsampled  needs to be restricted at all, it should be restricted based on rigorously defined probabilistic \\nbounds, say, 99% confidence intervals.  Fortunately, this is a capability that  NAbC provides.   \\nThe second  limitation  of Papenbrock et al. (2021) is shared with Marti (2019) in that the algorithm does \\nnot enforce positive definiteness ex ante.  The authors do acknowledge the importance of positive \\ndefiniteness in this setting, but do not explain how their algorithm handles non -positive definite samples.  \\nAgain, both ignoring/eliminating them from consideration, and /or ‘fixing’ them with algorithms like that of \\nHigham (200 2), distort the distribution of the correlation matrix in non -trivial ways , and thus invalidate \\ninferences based on it .  Finally, the sample correlation matrices generated by the evolutionary algorithms \\nare not inferentially valid in themselves, i.e. each is not associated with a cdf probability.  Again, none of \\nthese limitations – subjective or ad hoc restriction of the sample space, or ex post enforcement of \\npositive definiteness, or lack of inferential validity – apply to NAbC.  \\nF. A sophisticated and more recent attempt  at directly generating sample correlation matrices with \\nstylized, real-world characteristics  is that of Kubiak et al. (2024) .  They develop denoising diffusion \\nprobabilistic models  (DDPMs)  that, across multiple asset classes and market regimes, compare \\nfavorably against a number of alternate modern approaches , including CorrGAN  of Marti (2019) and other \\nGANs approaches,27 variational autoencoders, and more traditionally, block bootstraps .  Limitations of \\nthe approach are similar to those of the other ‘direct sampling’ algorithms : i. the matrices generate d are \\nnot true correlation matrices, lacking unit diagonals and true asymmetr y; ii. the matrices are not \\nguaranteed to be positive definite, and when they are not, positive definiteness is enforced ex post using \\nHigham (1988); and iii. the sampled matrices do not retain inferential validity, that is, they are not \\nassociated with a probability of occurrence or a cdf value, as described above.  \\n \\n27 Kubiak et al. (2024) believe this is due to “the standard instability issues commonly associated with GAN training. (p.5)”  \\nJD Opdyke, Chief Analytics Officer                  Page 32 of 88                      Beating the Correlation Breakdown  \\n This is an interesting  area of research and the work of these approaches, real limitations notwithstanding, \\nis quite encouraging.  This is especially true because NAbC, which shares none of these limitations, can \\nbe applied to the realistic groups of samples that they generate  to give them inferential validity  (as long as \\nthe samples are not distorted, i.e. are representative of the true distribution) .  As long as researchers can \\nfind a way for these algorithms to generate true correlation matrices and automatically enforce positive \\ndefiniteness ex ante, or convincingly prove that deviations from either are truly numerically de minimis \\nalong any dimension of analysis (which certainly has not been done to date), NAbC can be applied to \\ntheir samples  to provide inferential validity , that is, to associate a cdf value with each and every sample \\nmatrix generated so that the distribution can be used inferentially .  Again, the starting point for NAbC’s \\napplication is the known or well -estimated dependence matrix, and the known or well -estimated data \\ngenerating mech anism (in these cases, the matrix generating mechanism ), and these methods provide \\nboth (again, as long as the samples are representative of the true distribution) .  But until these two ‘fixes’ \\ncan be applied, with proof that this truly has been achieved numerically, if not analytically, real inferential \\nchallenges will remain for this path of ‘direct matrix simulation’ research.   \\nIt is notable that none of this work has demonstrated that generating samples of correlation/dependence \\nmatrices by first generating synthetic returns data that have all of the empirical, stylized characteristics of \\nactual returns data, is not sufficient, if carefully done, to generate the desired sample  matrices , even if \\nthese are not -historically -realized but  rather, for  plausible future scenarios .  This connection needs to be \\nestablished , mathematically and explicitly, because in reality the sample matrices are only and \\nexclusively based on the  sample returns, and without mathematically  defining the path from the returns \\ndata to the stylized sample matrices, something is  missing, if not wrongly specified , on one end or the \\nother.  It is not that we cannot or should not jump right to directly sampling the correlation/dependence \\nmatrix per se; only that the connection between all of the stylized characteristics of the \\ncorrelation/dependence matrix, and those of the returns data on which it is based, need s to be rigorously \\nestablished if we are to have real insight into the mechanics, provably appropriate simulations, and \\ndistribution -based inferences (if not causal drivers) of the former.  \\nIn the absence of an explanation as to why it is not preferable to  start with the returns data itself  (aside \\nfrom computational considerations) , I hypothesize that part of the motivation of taking the ‘direct \\nsimulation’ route, even if not explicitly stated, is the right -minded desire to separate  and isolate  the \\ndistribution of the correlation/dependence matrix from other characteristics of the distribution of the \\nreturns data.  There obviously have been many approaches to accomplish this, and it can remain  very \\nchallenging in certain circumstances, but this is exactly what NAbC does, fully inferentially, transparently, \\nand in some cases fully analytically, as I show below.   I start with a narrow but foundational case under \\nrestricted conditions, and then expand to fully general conditions, because the first provides a useful \\nreferential baseline for understanding the latter. \\n \\n \\n \\nJD Opdyke, Chief Analytics Officer                  Page 33 of 88                      Beating the Correlation Breakdown  \\n NAbC: Pearson’s Correlation, the Gaussian Identity Matrix  \\nCorrelations to Angles, Angles to Correlations  \\nI continue with Pearson’s for the first derivation and implementation of NAbC, and the data and \\ncorrelation structure I initially presume is Gaussian data under no correlation: that is, Pearson correlation \\nvalues of zero off the diagonal of the matrix as below. \\nidentity matrix =  for p = 4 assets  \\nThe key to the NAbC approach rests in its use of the spherical ANGLE, 𝛉 , between the two mean -\\ncentered data vectors of X and Y, as opposed to directly and only using of the values of the correlations \\nthemselves.  As mentioned above, using angles to understand the distribution of Pearson’s goes back at \\nleast to Fisher (1915), and it turns out to be a much more general framework applicable to any \\ndependence measure whose all -pairwise matrix is positive definite,  not just Pearson’s .  But to start with  \\nPearson’s, for a single pair of variables, providing a single bivariate correla tion value, the relationship \\nbetween angle value and correlation value is most readily seen in the widely known “cosine similarity,” \\nwhere the cosine of the angle equals the inner product divided by the product of the two vectors’ \\n(Euclidean) norms as in ( 16), which I show again below  for the reader’s convenience . \\n(16) \\n()1 1 1 ,\\n,22\\n1 1 1 1inner product ˆˆcos , 0product of normsN N N\\ni j i j\\ni j j XY\\nXY\\nN N N N XY\\ni j i j\\ni j i jX X Y Y\\nCovrss\\nX X X Y\\uf071 \\uf071 \\uf070= = =\\n= = = =\\uf0e6 \\uf0f6\\uf0e6 \\uf0f6−−\\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0e7 \\uf0f7\\uf0e8 \\uf0f8\\uf0e8 \\uf0f8= = = = = \\uf0a3 \\uf0a3\\n\\uf0e6 \\uf0f6 \\uf0e6 \\uf0f6−−\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf0e8 \\uf0f8 \\uf0e8 \\uf0f8\\uf0e5 \\uf0e5 \\uf0e5\\n\\uf0e5 \\uf0e5 \\uf0e5 \\uf0e5X,Y\\nXY  \\nIf a portfolio has p assets, the number of its pairwise relationships is npr=p(p -1)/2.  For all these npr \\nrelationships, the matrix analogue to (16), as long as the matrix is symmetric positive definite,28 is well \\nestablished in the literature ( Joarder and Ali, 1992, Pinheiro and Bates, 1996, Rebonato and Jackel, 2000, \\nRapisarda et al., 2007, Pouramadi and Wang, 2015, and Cordoba et al., 2018) and shown below, \\nformulaically in ( 20)-(22) and in computer code (SAS/IML) in Table A.  The steps for translating between \\ncorrelations and angles, in both directions, are shown in A. -C. below.  \\nA. estimate the correlation matrix  from sample data  \\nB. obtain the Cholesky factorization of the correlation matrix  \\nC. use inverse trigonometric  and trigonometric  functions on B. to obtain corresponding spherical angles  \\nand in reverse:  \\n \\n28 Note again that this is true not only for Pearson’s, but also for all relevant dependence measures in this setting, as will b e \\ndiscussed below.  \\n \\n1000\\n0100\\n0010\\n0001\\nJD Opdyke, Chief Analytics Officer                  Page 34 of 88                      Beating the Correlation Breakdown  \\n C. start with a matrix of spherical angles  \\nB. apply trigonometric functions to obtain the Cholesky factorization  \\nA. multiply B. by its transpose to obtain the corresponding correlation matrix  \\n \\n(see Rebonato & Jaeckel , 2000, Rapisarda et al. , 2007, and Pourahmadi & Wang, 2015, but note a typo in \\nthe formula in Pourahmadi & Wang, 2015, for the first 3 steps)  \\nCentral to this correlation -angle translation mechanism is obtaining the Cholesky factor of the \\ncorrelation/dependence matrix, which is usually a hard -coded function in most statistical and \\nmathematical software.  The relevant formulae are included below f or completeness.   \\n(20) A correlation matrix R will be real, symmetric positive -definite,29 so the unique matrix B that satisfies  \\nTR BB=\\nwhere B is a lower triangular matrix (with real and positive diagonal entries), and\\nTB is its \\ntranspose, is the Cholesky factorization of R.  Formulaically, B’s entries are as follows:  \\n()1\\n2\\n, , ,\\n1j\\nj j j j j k\\nkB R B−\\n== \\uf0b1 −\\uf0e5\\n,     \\n1\\n, , , ,\\n1 ,1 for j\\ni j i j i k j k\\nk jjB R B B i jB−\\n=\\uf0e6\\uf0f6= − \\uf03e\\uf0e7\\uf0f7\\n\\uf0e8\\uf0f8\\uf0e5  \\nThe Cholesky factor can be viewed as a matrix analog to the square root of a scalar, because like a square \\nroot the product of it and its transpose yields the original matrix.  Importantly, the Cholesky factor places \\nus on the UNIT hyper -(hemi)sphere (wher e scale does not matter) because the sum of the squares of its \\nrows always equals one.  Next, we recursively apply inverse trigonometric and trigonometric functions to \\neach cell of the Cholesky factor to obtain each cell’s angle value; or in reverse, we ob tain a \\ncorrelation/dependence value from each cell’s angle value (see both Joarder & Ali, 1992, and Rapisarda \\net al., 2007, for meticulous derivations of these formulas) .  Note that this relationship is one -to-one, with \\na unique correlation/dependence matrix yielding a unique angles matrix, and vice versa.   \\n(21) For R, a p x p correlation matrix,  \\n1,2 1,3 1,\\n2,1 2,3 2,\\n3,1 3,2 3,\\n4,1 4,2 4,3 4,\\n,1 ,2 ,31\\n1\\n1\\n1p\\np\\np\\np\\np p pr r r\\nr r r\\nr r r\\nRr r r r\\nr r r\\uf0e9\\uf0f9\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n=\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\uf0eb\\uf0fb\\n , \\n \\n \\n29 Semi-positive definiteness includes the case of eigenvalues exactly equal to zero, which I largely ignore herein as a border \\ncase relevant mainly for textbook examples.  \\nJD Opdyke, Chief Analytics Officer                  Page 35 of 88                      Beating the Correlation Breakdown  \\n \\n where  is the Cholesky factor of  andtR BB B R= \\n()()\\n()()()()()\\n()()()()()()\\n()()()()()() ()2,1 2,1\\n3,1 3,2 3,1 3,2 3,1\\n4,1 4,2 4,1 4,3 4,2 4,1\\n1\\n,1 ,2 ,1 ,3 ,2 ,1 ,\\n11 0 0 0\\ncos sin 0 0\\ncos cos sin sin sin 0\\ncos cos sin cos sin sin 0\\ncos cos sin cos sin sin sinn\\np p p p p p p k\\nkB\\uf071\\uf071\\n\\uf071 \\uf071 \\uf071 \\uf071 \\uf071\\n\\uf071 \\uf071 \\uf071 \\uf071 \\uf071 \\uf071\\n\\uf071 \\uf071 \\uf071 \\uf071 \\uf071 \\uf071 \\uf071−\\n=\\uf0e9\\uf0f9\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa=\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\uf0eb\\uf0fb\\uf0d5\\n \\n() , for  angles 0, .ij ij \\uf071\\uf070 \\uf03e\\uf0ce\\n \\nTo obtain an individual angle \\n,ij\\uf071 , we have:30 \\n() ()1\\n,1 ,1 , , ,\\n1For 1:     arcos  for =1;  and  arcos sin  f or 1j\\ni i i j i j i k\\nki b j b j\\uf071 \\uf071 \\uf071−\\n=\\uf0e6\\uf0f6\\uf03e = = \\uf03e \\uf0e7\\uf0f7\\n\\uf0e8\\uf0f8\\uf0d5\\n \\n(22) To obtain an individual correlation, \\n,ijr , we have, simply from\\nTR BB= : \\n()()()()()()()()()1 1 1\\n, ,1 ,1 , , , , , , ,\\n2 1 1cos cos cos cos sin sin cos sin sin   for 1i k i\\ni j i j i k j k i l j l j i i l j l\\nk l lr i j n\\uf071 \\uf071 \\uf071 \\uf071 \\uf071 \\uf071 \\uf071 \\uf071 \\uf071− − −\\n= = == + + \\uf0a3 \\uf03c \\uf0a3 \\uf0d5 \\uf0d5 \\uf0d5\\n \\nSAS/IML code translating correlations to angles and angles to correlations is shown in Table A below:  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n30 Note that a similar recursive relationship exists between partial correlations, although its sample -generating algorithm  (see \\nMadar, 2015 ) does not generalize  beyond Pearson’s correlations, i .e. to all positive definite measures of dependence , as does \\nthat of NAbC, as shown below . \\nJD Opdyke, Chief Analytics Officer                  Page 36 of 88                      Beating the Correlation Breakdown  \\n TABLE A:  \\n \\nThe above all is well -established and straightforward,31 and demonstrates, as we know intuitively, that \\nscale does not (and should not) matter when it comes to dependence measures ;32 again, in this \\nsetting, this is because geometrically, the Cholesky factor places us on the UNIT hyper -(hemi)sphere.  \\nImportantly, the Cholesky factor also ensures that sampling based directly on the resulting angles will \\nyield only positive definite matr ices, as the Cholesky factor remains undefined otherwise.  This \\nautomatic  enforcement of positive definiteness makes this approach much more efficient than \\nothers that require ex post verification of positive definiteness, and subsequent resampling or \\nenforcement when this requirement is violated ( examples of this discussed above include  Makalic & \\nSchmidt, 2018 ; Cordoba et al. 2018 ; Marti, 2019; Papenbrock et al. , 2021; and Kubiak et al., 2024 ).  This \\n \\n31 Reliance on spherical angles and hypersphere  parameterizations  is increasingly c ommon in quantitative finance  (see for \\nsome examples Li, Q., 2018 ; Helton, 2020; Golts & Jones, 2009; Zhang, 2022; Saxena et al. , 2023; and Zhang & Yang, 2023 ), in \\nlarge part due to its scale invariance: it has even been used to define entire financial markets (see Kim and Lee, 2016).  \\n \\n32 Scale invariance is proved and widely cited for Pearson’s, Kendall’s, and Spearman’s (see Xu et al., 2013, and Schreyer et al. , \\n2017 examples).  \\n \\n\\nJD Opdyke, Chief Analytics Officer                  Page 37 of 88                      Beating the Correlation Breakdown  \\n inefficiency grows very rapidly with the size of the matrix/portfolio, as shown in the ratio below in ( 23) (see \\nBohn and Hornik, 2024, and Pourahmadi & Wang, 2015).   \\n(23) \\n( )()()1\\n121\\n1\\n12\\n11\\n2Pr \" \" ~ ;lim 022 2jp\\nj p pp\\nj\\npppjj\\nrand R PosDef X X\\uf070\\n\\uf070\\uf070−\\n−−\\n=\\n−→\\uf0a5=\\uf0e9\\uf0f9+\\uf0e6\\uf0f6\\uf047\\uf0ea\\uf0fa\\uf0e7\\uf0f7\\uf0e9 \\uf0f9 \\uf0e9 \\uf0f9 \\uf0e8\\uf0f8\\uf0eb\\uf0fb= = \\uf03c = = \\uf0e9\\uf0f9 \\uf0ea \\uf0fa \\uf0ea \\uf0fa \\uf0eb\\uf0fb\\uf0ea \\uf0fa \\uf0ea \\uf0fa\\uf0eb \\uf0fb \\uf0eb \\uf0fb\\uf0d5\\n\\uf0d5  \\nEven for relatively small matrices of dimension p=25, the odds of successfully randomly generating a \\nsingle valid positive definite correlation matrix, by uniformly sampling the off -diagonal correlation values \\nthemselves across values ranging from –1.0 to 1.0, are less then 2 in 10 quadrillion, leading to \\nprohibitively inefficient sampling.  Consequently, even when sampling -rejection algorithms achieve some \\nefficiency gains, realistically the sampling approach in this setting should possess automatic \\nenforcement of positive definiteness , ex ante.  Conceptually, an imperfect but apt analogy is to a rubiks \\ncube: the colored stickers on the cube cannot simply be peeled off and repasted, even some of the time, \\nto solve the cube.  The valid solution must be obtained by (always) following the rules gov erning shifts in \\nthe cube, and every move of each  of the small cubes (correlation cells) affects the positions of many of \\nthe other cubes (correlation cells), not just the one we need to reposition.  Similarly with sa mpling the \\ncorrelation/dependence matrix: converting to the Cholesky factor (en)forces positive definiteness by \\nforcing the matrix onto the UNIT hyper -(hemi)sphere, where we can subsequently use the distributions of \\nthe angles to perturb it and obtain, aft er re-translation, the distribution of the original \\ncorrelation/dependence matrix, without violating positive definiteness .  This is done  simply by following \\nsteps A., B., and C., and C., B., and A., above.  \\nAnother crucial characteristic of these angles is that they are random variables whose multivariate \\nrelationship is one of independence  (see Pourahmadi & Wang, 2015, Tsay & Pourahmadi, 2017, and \\nGhosh et al., 2020 ).  This is critically important for practical usage as it enables the straightforward \\nconstruction of the multivariate distribution of a matrix of angles, which is the more important objective \\nhere (vs merely sampling) and essential for the application of  NAbC below.  \\nFinally and most critically, the above demonstrates that the angles between pairwise data vectors \\ncontain ALL the information that exists regarding dependence between the two variables  because \\nthe only information we lose by translating  to the unit hyper(hemi -)sphere is scale (see Fernandez -Duren \\n& Gregorio -Dominguez, 2023, and Zhang & Songshan, 2023, as well as Opdyke, 2024 a).  This will be \\ncovered more extensively below.  \\nSo with all this in mind we proceed with the use of the angles as described and defined above.  The goal is \\nto use the angles as the basis for 1. sample generation of the correlation /dependence  matrix; and more \\nimportantly, 2. definition of the multivariate distribution of the correlation /dependence  matrix.  \\nFully Analytic Angles Density, and Efficient Sample Generation  \\nOnce we have the matrix of angles  (per (21) and Table A above) , one angle for each value in the  all-\\npairwise correlation /dependence measure  matrix, we use the well -established finding that, to sample \\nJD Opdyke, Chief Analytics Officer                  Page 38 of 88                      Beating the Correlation Breakdown  \\n uniformly from the space of positive definite matrices, the probability density function (pdf) must be \\nproportional to the determinant of the Jacobian of the Cholesky factor ( 24) (see Cordoba, 2018, \\nPourahmadi & Wang, 2015, Lewandowski et al., 2009).   \\n(24) \\n()1\\n1det 2   where  is the Cholesky factorizati on of correlation matrix p\\np i t\\nii\\niJ U u U R UU−\\n=\\uf0e9\\uf0f9==\\uf0eb\\uf0fb\\uf0d5   \\nWe see directly from ( 24) that \\n() sinkx , suitably normalized in ( 25), satisfies this requirement (see \\nPourahmadi & Wang, 2015, and Makalic & Schmidt, 2018).  \\n(25) \\n()()() ( )()\\n()21sin , 0, , 1,2,3, , #columns 1 , and \\n2 1 2k\\nX k kkf x c x x k c\\nk\\uf070\\n\\uf070\\uf047+= \\uf0d7 \\uf0ce = − =\\n\\uf047+\\n  \\nAlthough not explained  in Makalic & Schmidt (2018), importantly note that k = #columns – column# (so \\nfor the first column of a p=10x10 matrix, k=9; for the second column, k=8, etc.).  \\nBeyond (25), h owever, we need both the cumulative distribution function (cdf) and its inverse, the \\nquantile function, to make use of this density for sampling and other purposes.  The most widely used \\nand straightforward method of sampling is inverse transform, whereby the values of a uniform random \\nvariate are passed to the quantile function to generate  sampled  values.  Yet regarding the cdf \\ncorresponding to ( 25) above, Makalic & Schmidt (2018) state, “ Generating random numbers from this \\ndistribution is no t straightforward as the corresponding cumulative density [sic] function, although \\navailable in closed form, is defined recursively and requires O(k) operations to evaluate. The nature of the \\ncumulative density [sic] function makes any procedure based on i nverse transform sampling \\ncomputationally inefficient, especially for large k. ” \\nFortunately, that turns out not to be the case, as Opdyke (2020) derived an analytic, non -recursive \\nexpression of the cdf below in (26): \\n(26)  \\n() () ()2\\n211 1 1 3; ~ cos , ; ;cos  for ,2 2 2 2 2XkkF x k c x F x x\\uf070 −\\uf0e9\\uf0f9− \\uf0d7 \\uf0d7 \\uf03c\\uf0ea\\uf0fa\\uf0eb\\uf0fb\\n \\n      \\n() ()2\\n211 1 1 3~ cos , ; ;cos  for 2 2 2 2 2kkc x F x x\\uf070 −\\uf0e9\\uf0f9+ \\uf0d7 \\uf0d7 \\uf0b3\\uf0ea\\uf0fa\\uf0eb\\uf0fb    \\n      \\n\\uf05b\\uf05d()()\\n()21 where the Gaussian hypergeometric functi on , ; ;   !n\\nnn\\nnnab rF a b c rcn\\uf0a5\\n=\\uf0d7\\uf0e5  \\n      \\n()()()()()0where 1 2 1 ,  1,  1,  and 1,  0, 1, 2,...nh h h h h n n h r c= + + + − \\uf0b3 = \\uf03c \\uf0b9 − −\\n  \\nAs mentioned in a footnote above , the Gaussian hypergeometric function makes many interesting \\nappearances in this setting, but it is admittedly cumbersome mathematically.  Yet Opdyke (2022, 2023, \\nJD Opdyke, Chief Analytics Officer                  Page 39 of 88                      Beating the Correlation Breakdown  \\n and 2024) has shown that ( 26) can be simplified further, based on some arguably obscure \\nhypergeometric identities in (27) below: \\n(27)  \\n\\uf05b\\uf05d()() 21 For 1 and 0 1 simultaneously, which holds  in this setting, we have , ; ; ; ,1ac a r F a b c r B r a b a r= + \\uf03c \\uf03c = −\\n \\n()()1 1\\n0where ; , 1  = the incomplete beta functionr\\nb aB r a b u u du− −=−\\uf0f2\\n (see DLMF, 2024)  \\nIn addition we have \\n()()()()()\\n()( ; , ) ; , ,  where  , the complete beta functi on, soBetaabF r a b B r a b B a b B a bab\\uf047\\uf047= = =\\uf047+\\n \\n() () ; , ( ; , ) ,Beta B r a b F r a b B a b=\\uf0d7\\n  (see Weisstein, E. , 2024a and 2024b)  \\nCombining terms we have  \\n() () () \\uf05b\\uf05d() ( )2211\\n1 1 1 22; ~ cos cos ; , 1 2 cos  for ,2 2 2 2 2\\n2X k Betak\\nkF x k c x F x x xk\\uf070+\\uf0e6 \\uf0f6 \\uf0e6 \\uf0f6\\uf047\\uf047\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7+ \\uf0e9\\uf0f9 \\uf0e8 \\uf0f8 \\uf0e8 \\uf0f8− \\uf0d7 \\uf0d7 \\uf0d7 \\uf0d7 \\uf03c\\uf0ea\\uf0fa +\\uf0e6\\uf0f6 \\uf0eb\\uf0fb\\uf047\\uf0e7\\uf0f7\\uf0e8\\uf0f8\\n() () () \\uf05b\\uf05d() ( )2211\\n1 1 1 22; ~ cos cos ; , 1 2 cos  for 2 2 2 2 2\\n2X k Betak\\nkF x k c x F x x xk\\uf070+\\uf0e6 \\uf0f6 \\uf0e6 \\uf0f6\\uf047\\uf047\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7+ \\uf0e9\\uf0f9 \\uf0e8 \\uf0f8 \\uf0e8 \\uf0f8+ \\uf0d7 \\uf0d7 \\uf0d7 \\uf0d7 \\uf0b3\\uf0ea\\uf0fa +\\uf0e6\\uf0f6 \\uf0eb\\uf0fb\\uf047\\uf0e7\\uf0f7\\uf0e8\\uf0f8\\n \\nRecognizing that the complete Beta function is the inverse of the normalization factor of c(k) for these \\nvalues, their product equals 1 and cancels, as do the two cosine terms, and we obtain the following \\nsigned beta cdf : \\n() ()2 1 1 1 1; ~ cos ; ,  for ,2 2 2 2 2X BetakF x k F x x\\uf070 + \\uf0e6 \\uf0f6 \\uf0e9 \\uf0f9− \\uf0d7 \\uf03c\\uf0e7\\uf0f7\\uf0ea\\uf0fa\\uf0e8 \\uf0f8 \\uf0eb \\uf0fb\\n \\n     \\n()2 1 1 1 1~ cos ; ,  for 2 2 2 2 2BetakF x x\\uf070 + \\uf0e6 \\uf0f6 \\uf0e9 \\uf0f9+ \\uf0d7 \\uf0b3\\uf0e7\\uf0f7\\uf0ea\\uf0fa\\uf0e8 \\uf0f8 \\uf0eb \\uf0fb  \\nAnd now, with this straightforward, fully analytic, non -recursive cdf, we can obtain a  straightforward, fully  \\nanalytic quantile function of the angle distribution  in (28): \\n(28)  \\n() Let  Pr .  Then for ,2p x X x\\uf070= \\uf0b3 \\uf03c\\n \\nJD Opdyke, Chief Analytics Officer                  Page 40 of 88                      Beating the Correlation Breakdown  \\n \\n()2 1 1 1 1cos ; ,2 2 2 2Betakp F x+ \\uf0e6 \\uf0f6 \\uf0e9 \\uf0f9= − \\uf0d7\\uf0e7\\uf0f7\\uf0ea\\uf0fa\\uf0e8 \\uf0f8 \\uf0eb \\uf0fb \\n()2 112 1 cos ; ,22Betakp F x+ \\uf0e9\\uf0f9− =− +\\uf0ea\\uf0fa\\uf0eb\\uf0fb\\n \\n()2 111 2 cos ; ,22Betakp F x+ \\uf0e9\\uf0f9−=\\uf0ea\\uf0fa\\uf0eb\\uf0fb\\n \\n()12 111 2 ; , cos22BetakF p x− + \\uf0e6\\uf0f6−=\\uf0e7\\uf0f7\\uf0e8\\uf0f8\\n \\n()1 111 2 ; , cos22BetakF p x− + \\uf0e6\\uf0f6−=\\uf0e7\\uf0f7\\uf0e8\\uf0f8\\n \\n1 11arcos 1 2 ; ,22BetakF p x−\\uf0e6\\uf0f6 + \\uf0e6\\uf0f6−=\\uf0e7\\uf0f7\\uf0e7\\uf0f7\\uf0e7\\uf0f7\\uf0e8\\uf0f8\\uf0e8\\uf0f8\\n  (Note that arcos is arc -cosine, the inverse of the cosine function. ) \\nWe must reflect the symmetric angle density for p≥0.5, so we have  \\n1 11arcos 1 2 ; ,  for 0.5,22Betakx F p p−\\uf0e6\\uf0f6 + \\uf0e6\\uf0f6= − \\uf03c\\uf0e7\\uf0f7\\uf0e7\\uf0f7\\uf0e7\\uf0f7\\uf0e8\\uf0f8\\uf0e8\\uf0f8\\n \\n     \\n\\uf05b\\uf05d1 11arcos 1 2 1 ; ,  for 0.522BetakF p p \\uf070−\\uf0e6\\uf0f6 + \\uf0e6\\uf0f6= − − − \\uf0b3 \\uf0e7\\uf0f7\\uf0e7\\uf0f7\\uf0e7\\uf0f7\\uf0e8\\uf0f8\\uf0e8\\uf0f8  \\nImportantly, although often ignored in the sampl e-generation  literature  (see, for example,  Makalic & \\nSchmidt, 2018) , note that properly adjusting for sample size, n, and degrees of freedom gives  \\n#2 k k n cols\\uf0ac + − −\\n, so consequently, \\n#  2 k n column= − − .33 \\nSo now from (28) above we have for the angles distribution, under the Gaussian identity matrix, for the \\nfirst time together, the pdf, cdf, and quantile function  in (29): \\n(29)  \\n()()()()\\n()21sin , 0, , 1,2,3 #columns 1, and \\n2 1 2k\\nX k kkf x c x x k c\\nk\\uf070\\n\\uf070\\uf047+= \\uf0d7 \\uf0ce = − =\\n\\uf047+\\n \\n \\n33 So notably, the angles distributions vary systematically based on their (column) position in the matrix, even though the \\ndistributions of the correlations themselves do not, as is discussed in later chapters.  \\n \\nJD Opdyke, Chief Analytics Officer                  Page 41 of 88                      Beating the Correlation Breakdown  \\n \\n() ()2 1 1 1 1; ~ cos ; ,  for ,2 2 2 2 2X BetakF x k F x x\\uf070 + \\uf0e6 \\uf0f6 \\uf0e9 \\uf0f9− \\uf0d7 \\uf03c\\uf0e7\\uf0f7\\uf0ea\\uf0fa\\uf0e8 \\uf0f8 \\uf0eb \\uf0fb \\n        \\n()2 1 1 1 1~ cos ; ,  for 2 2 2 2 2BetakF x x\\uf070 + \\uf0e6 \\uf0f6 \\uf0e9 \\uf0f9+ \\uf0d7 \\uf0b3\\uf0e7\\uf0f7\\uf0ea\\uf0fa\\uf0e8 \\uf0f8 \\uf0eb \\uf0fb  \\n()11 11; arcos 1 2 ; ,  for 0.5;22BetakF p k F p p−−\\uf0e6\\uf0f6 + \\uf0e6\\uf0f6= − \\uf03c\\uf0e7\\uf0f7\\uf0e7\\uf0f7\\uf0e7\\uf0f7\\uf0e8\\uf0f8\\uf0e8\\uf0f8\\n \\n         \\n\\uf05b\\uf05d1 11= arcos 1 2 1 ; ,  for 0.522BetakF p p \\uf070−\\uf0e6\\uf0f6 + \\uf0e6\\uf0f6− − − \\uf0b3\\uf0e7\\uf0f7\\uf0e7\\uf0f7\\uf0e7\\uf0f7\\uf0e8\\uf0f8\\uf0e8\\uf0f8  \\nApparently the first (and only other) presentation of this quantile function result comes from an \\nanonymous blog post in March, 2018, although it was obtained via a different derivation, which serves to \\nfurther validate the result.34 \\nThe above ( 29) now provides a fully analytic solution,35 and in fact is so straightforward as to be readily \\nimplemented in a spreadsheet, and one is provided for download via the link below.  \\nhttp://www.datamineit.com/JD%20Opdyke --The%20Correlation%20Matrix -\\nAnalytically%20Derived%20Inference%20Under%20the%20Gaussian%20Identity%20Matrix --02-18-\\n24.xlsx  \\nSo contrary to the assertions of Makalic & Schmidt (2018), the straightforward approach of inverse \\ntransform sampling can be used in this setting, for this narrow case, to very efficiently generate samples \\nfrom the correlation matrix.  And in fact, this is the most efficient way to sample it.  Roman (2023) has \\ncompared Makalic and Schmidt (2018) to the above method and obtained over 30% decrease in runtime \\nwhen using Opdyke (2022, 2023, and 2024).   But of course, when used analytically, for example via the \\nlinked spreadsheet, these results are instantaneous.  \\nBut sampling arguably is the less important of our two goals, because with a fully analytic finite -sample \\ndistribution, we can define, exactly for a given sample size, the p -value of a given cell, and the confidence \\ninterval of a given cell.  The one -sided p-value simply is the CDF value for the lower tail, or [1 – (CDF \\n \\n34 See Xi’an, March, 2018 (https://stats.stackexchange.com/questions/331253/draw -n-dimensional -uniform-sample-from-a-\\nunit-n-1-sphere-defined-by-n-1-dime/331850#331850   \\nand  https://xianblog.wordpress.com/2018/03/08/uniform -on-the-sphere-or-not/ ).  In the interest of proper attribution, a \\nreference on the website to the book “The Bayesian Choice” hints that the Xi’an pseudonym is Christian Robert, a professor of  \\nStatistics at Université Paris Dauphine (PSL), Paris, France, since 2000 ( https://stats.stackexchange.com/users/7224/xian ). \\n \\n35 Note that I use the term ‘analytic’ as opposed to ‘closed -form’ because I am unaware of a closed -form algorithm for the \\ninverse cdf of the beta distribution (see Sharma and Chakrabarty, 2017, and Askitis, 2017).  However, for all practical purpo ses \\nthis is essentially a semantic distinction since this quantile function is hard -coded into all major statistical / econometric / \\nmathematical programming languages.  \\nJD Opdyke, Chief Analytics Officer                  Page 42 of 88                      Beating the Correlation Breakdown  \\n value)] for the upper tail (3 0), and due to this pdf’s symmetry, the two -sided p-value is simply two times \\neither one -sided value.  Correspondingly, the confidence interval for the critical value alpha is based on \\nthe quantile function as in ( 31) \\n(30) one-sided p-value = \\n();XF x k  or \\n() 1;XF x k−  where k = n – column# – 2;  \\n         two-sided p-value = 2 x one -sided p-value  \\n(31) \\n()12; Fk\\uf061−  and \\n()11 2;Fk\\uf061−−  where, for a 95% confidence interval for example, α = 0.05  \\nNotably, because the angles distributions are independent, the density of the entire matrix is simply the \\nproduct of the densities of all the cells.  This means we can readily define the p -value and confidence \\nintervals of the entire matrix such that they are analytically consistent with those of the cells, because \\nthey are determined based directly on the cell level p -values and confidence intervals, respectively, as \\nshown below.  \\nMatrix-level p-values and Confidence Intervals  \\nAs mentioned above, a key characteristic of the angles is that they are independent random variables, \\nwhich makes defining their multivariate distribution straightforward: it is simply the product of all the \\nangles’ pdf’s.  But what does this mean for the p-value and confidence intervals for the entire matrix?   \\nRecall that a p -value is simply the probability of observing , based on a given data sample, a statistic value \\nat least as large as what was observed , assuming the null hypothesis is true.  The p -values defined above \\nfor each correlation/dependence cell are the probabilities of observing , for a given sample, angle values \\nas large as what is observed assuming the null hypothesis is true.  The fact that the angles are \\nindependent random variables, i.e. each is independent vis -à-vis all the other angles, makes obtaining \\nthe p-value for the entire matrix very stra ightforward.  Note that the probability that none of the \\ncorrelation/dependence cells are as extreme as what was observed is simply the product of  one minus \\nevery p-value, because they are independent.  So the probability that one or more  of the  \\ncorrelation/dependence cells are as extreme or more extreme than what was observed is simply one \\nminus this value, shown in ( 32) below, and this is the p-value for the entire matrix.   \\n(32) \\n( )()12\\n1matrix (2-sided)  1 1 -pp\\ni\\nipvalue p value−\\n=\\uf0e9\\uf0f9= − −\\uf0ea\\uf0fa\\n\\uf0eb\\uf0fb\\uf0d5    where \\n-i p value  is the 2-sided p-value.  \\nAnother way of conceptualizing this is that if the null hypothesis of just one of the matrix cells is not true, \\nthen the null hypothesis for the entire matrix is not true, and this is what the matrix -level p-value \\nmeasures: the probability that at least one  of the cell -level null hypotheses is not true.  If instead of p -\\nvalues we were using critical values in p(p -1)/2 hypothesis tests, this would be exactly consistent with \\ncontrolling the familywise error rate  (FWER) of the joint hypothesis including all th e cells of the all -\\npairwise matrix.36  And just as no other approach to estimating FWER would increase statistical power in \\n \\n36 Note that this approach has been used in the literature for addressing very closely related problems (see Fang et al., 2024).  \\n \\nJD Opdyke, Chief Analytics Officer                  Page 43 of 88                      Beating the Correlation Breakdown  \\n this case due to the independence of the angles distributions,37 no other definition of the matrix -level p-\\nvalue will have greater power  for the same reason . \\nSimilarly, calculation of the confidence interval for the entire matrix ( 33) is essentially the same as that of \\nthe p-value, but of course it is divided in half to account for each tail, and the root of the critical values is \\ntaken, rather than the product.  Otherwise, the calculations are identical to obtain the critical alphas for \\nthese ‘simultaneous confidence intervals.’  \\n(33) \\n\\uf05b\\uf05d()( )( )1 1 21 1 2pp\\ncrit simult LOW\\uf061\\uf061−\\uf0e9\\uf0f9\\uf0eb\\uf0fb\\n−−= − −   and \\n1crit simult HIGH crit simult LOW\\uf061\\uf061− − − −=−   \\nThese critical alphas, when inserted in the quantile function (28) and applied to every cell , provide the \\ntwo correlation matrices that define and capture, say, (1 -alpha)=(1 -0.05)=95% of randomly sampled \\nmatrices under the null hypothesis , which in this case is the identity matrix .  Again, it is the i ndependence \\nof the angles that makes these simultaneous confidence intervals very straightforward to calculate.   \\nImportantly, again note that because we derived the quantile (inverse cdf) function in (2 8) above, we can \\ngo in either direction regarding these results: we can specify a correlation matrix and, under the null \\nhypothesis of the identity matrix, obtain its p -values, both for the individual cells and the entire matrix, \\nsimultaneously.  We also ca n specify a matrix of cdf values and obtain its corresponding correlation \\nmatrix, which is extremely useful and straightforward when constructing both stress and reverse stress \\nscenarios.  Finally, we can use simultaneous confidence intervals to obtain the two correlation matrices \\nthat form the matrix -level confidence interval.   \\nNote that all these calculations are included in the downloadable spreadsheet  (link provided above) , with \\nvisible formulae corresponding to each step of these calculations for full transparency.  In the next \\nsection below I expand these results for Pearson’s to apply to all data conditions, and all values of the \\nnull hypothesis (i.e. any values for the matrix, not just the identity matrix).  \\n \\nNAbC: Pearson’s Correlation, Real -World Financial Data, Any Matrix Values  \\nCurrently, the extant literature does not provide analytic forms for the angles distributions under general \\nconditions.  Deriving these appears to be a non -trivial problem.  Spectral (eigenvalue) distributions, \\nwhich many researchers turn to in this settin g, have been shown to vary dramatically when data is \\ncharacterized by different degrees of heavy -tailedness (see Burda et al., 2004, Burda et al., 2006, \\nAkemann et al., 2009; Abul -Magd et al., 2009, Bouchaud & Potters, 2015, Martin & Mahoney, 2018 ; Heiny \\nand Yao, 2022, and Opdyke, 2024 a), as well as by different degrees of serial correlation ( see Burda et al., \\n2004, 2011 , Hisakado and Kaneko , 2023, and Opdyke, 2024 a), and the literature provides no general \\n \\n37 Other approaches for calculating the FWER that rely on, for example, resampling methods (see Westfall and Young, 1993, \\nand Romano and Wolf, 2016)  exploit dependence structure to increase power; here, under independence , they would provide \\nno power gain over  the analogue to  (32) because there is no dependence structure for them to exploit.   \\n \\nJD Opdyke, Chief Analytics Officer                  Page 44 of 88                      Beating the Correlation Breakdown  \\n analytic form under general, real -world financial data conditions – certainly nothing that is analogous to \\nconvergence to the Marchenko -Pastur distribution under iid conditions  (see Marchenko  and Pastur, \\n1967),38 let alone under even more general, non -iid conditions.  If angles distributions are of similar \\ncomplexity, then deriving their analytic form under general conditions, if possible, currently appears to be \\na non-trivial, unsolved problem.  \\nHowever, this need not be a showstopper for our purposes, in part because angles distributions have \\nmany characteristics that make them useful here generally, and more useful specifically than spectral \\ndistributions in this setting, by multiple criteria, i ncluding structurally, empirically, and distributionally.   \\nStructurally : Aggregation level becomes  relevant and  important here.  For a given correlation \\nmatrix R there are many more angles than eigenvalues  (i.e. p(p-1)/2 cells vs p eigenvalues , for a factor of \\n(p-1)/2 more).  As a matrix approaches  singularity (non -positive definiteness ( NPD)), which arguably is the \\nrule rather than the exception for non -small investment portfolios , a much smaller proportion  of angles \\ndistributions will approach degeneracy (i.e. minimum /maximum  values of  zero and 𝛑) than is true f or \\neigenvalue distributions  (where more values will wrongly fall below zero) .  Consequently, the overall \\nconstruction of the correlation matrix via\\nTR BB= generally will remain much more stable than one based \\non an eigen  decomposition of\\n1RVΛV−= where V is a matrix with column eigenvectors and 𝛬 is a diagonal \\nmatrix of the corresponding eigenvalues.  \\nEmpirically : If an angle distribution approaches degeneracy, most of its values typically will \\napproach 0 or 𝛑.  But the  relevant trigonometric functions (sin and cos) of these values are stable, and \\nwill simply approach -1, 0, or 1.  This makes\\nTR BB= a relatively stable calculation empirically, even if it \\nproduces an R that is approaching NPD.  In contrast, e igenvalue/vector estimations are more numerically \\ninvolved compared to the application of simple trigonometric functions, and this, combined with the fact \\nthat empirically,  their upper bound is not well -bounded (in the general case),39 makes their computation \\ncomparatively less numerically stable as matrices approach NPD.  \\nDistributionally : As shown graphically below under challenging, real -world financial data \\nconditions, t he angles distributions are relatively “ well behaved ,” both in the general sense and relative to \\nspectral distributions.  T hey are relatively smooth and typically  unimodal, and clearly bounded on\\n()0,\\uf071\\uf070\\uf0ce\\n.  Spectral distributions , based on the same data, very often are spikey ,40 multimodal, and for \\n \\n38 Note that some exceptions to convergence to this celebrated distribution do exist (see Li and Yao (2018), Hisakado and \\nKaneko (2023),  Heiny and Yao (2022),  and Maltsev and Malysheva (2024) for examples).  \\n \\n39 Even though the largest eigenvalue is known to follow the Tracy -Widom distribution under certain sets of conditions, under \\nothers it can diverge, with unbounded support (see for example Li, 2025).  Even when the latter cases do not hold \\nmathematically, em pirically, in practice, the largest eigenvalue can become so large that it is essentially unbounded.  \\n40 In fact, one of the most commonly encountered covariance  (correlation ) matrices under real world financial data conditions \\nis the spiked matrix (see Johnstone, 2001), where one or few eigenvalues dominate and the majority of eigenvalues are close \\nto zero, i.e. not reliably estimated.  This further demonstrates that spectral approaches are far too limited and limiting to \\neffectively solve this problem under real -world conditions.  \\nJD Opdyke, Chief Analytics Officer                  Page 45 of 88                      Beating the Correlation Breakdown  \\n practical purposes, empirically unbounded (at least in higher dimensions ), all of which  translates into \\nlarger variances and less tail accuracy.  Simply put, t hey typically are much more complex  and \\nchallenging to estimate precisely and accurately compared to individual angles distributions for a given \\ncorrelation matrix R under real -world financial data . \\nAll of this adds up to a more robust and granular basis for inference and analysis when relying on angles \\ndistributions as opposed to spectral distributions.  Even more important that these considerations, \\nhowever, is the fact that spectral distributions simply are at the wrong level of aggregation for these \\npurposes: they remain at the (higher) level of the p assets of a portfolio – NOT at the granular level of  the \\np(p-1)/2 pairwise associations of that portfolio, which is where both the angles distributio ns, and those of \\nthe correlations/dependence measure values  themselves,  lie.  Consequently, while potentially very \\nuseful for things like portfolio factor analysis, spectral analysis simply is too blunt a tool for our purposes \\nhere: we need to be able to make inferences and monitor processes and conduct (reverse) scenario \\nanalyses and customized stress tests on ALL aspects of the dependence structure measured by the all -\\npairwise correlation/dependence matrix, at the granular level at which it is defined.  T he specific need for \\nthis in scenario and reverse scenario analyses is covered in more detail below.  \\nSo given the useful characteristics of the angles distributions, not to mention the fact that they remain at \\nthe right level of aggregation for granular analysis of the correlation /dependence  matrix, we are able to \\nproceed WITHOUT their analytic derivation .  Rather, we can use a time -tested nonparametric approach, \\nsuch as kernel estimation, to reliably define them.  All this requires is a single simulation (say, N=10,000) \\nbased on the known or well -estimated correlation /dependence  matrix, and its known or w ell-estimated \\ndata generating mechanism .  These are the two stated requirements for the application of NAbC  under \\ngeneral conditions .  Then, after translating all N simulated correlation matrices to N matrices of angles, \\nwe fit a kernel to each empirical angle distribution, i.e. the empirical distribution of each angle for each \\ncell of the matrix.  We now have not only the distributions  of all the individual angles, but also the \\nmultivariate distribution  of the matrix, which is just the product of all the i ndividual distributions  due to \\ntheir independence.  Note that this goes in both directions: we can perform ‘look -ups’ on the empirically \\ndefined distribution to obtain the cdf value(s) corresponding to particular angle value(s), or use cdf \\nvalue(s) to ‘look up’ corresponding ang le (quantile) value(s).  The kernel fitting smooths this empirical \\ndensity to all (continuous) values.  This process is described step by step below.  \\n1. Simulate samples (say, N=10k) based on the specified/known or well estimated all-pairwise \\ncorrelation/dependence  matrix and the specified/known or well estimated data generating \\nmechanism.  \\n2. Calculate the corresponding N all-pairwise correlation/dependence matrices , and their Cholesky \\nfactorizations, and transform each of these into a lower triangle matrix of angles (as described above \\nin (21)). \\n3. Fit kernel densities to each of the p(p-1)/2 empirical angle distributions, each having N observations.  \\n \\n \\nJD Opdyke, Chief Analytics Officer                  Page 46 of 88                      Beating the Correlation Breakdown  \\n 4. Generate samples (say, N=10k) based on  the densities in 3.41 \\n5. Convert the samples from 4. back to a re -parameterized Cholesky factorization, and then multipl y by \\nits transpose to obtain a set of N validly sampled correlation /dependence  matrices  (as described \\nabove in ( 22)).  Positive definiteness is enforced automatically as the Cholesky factor places us on the \\nunit hyper-hemisphere.   All sample generation hereafter uses just 4. And 5.  \\n \\nThe distribution of correlation matrices from 5. is identical to that of 2., but after the kernel densities are \\nfit once in 3., generating samples in 4. is orders of magnitude faster than relying on direct simulations in \\nsteps 1. and 2.  So one simulation gives us the distribution of each and every angle, corresponding to \\neach and every correlation/dependence cell.  And now going forward  using 4.-5., rather than 1. and 2. , \\nallows for correct probabilistic inference , both at the cell level and at the matrix level, due to the \\nindependence of the angles distributions (remember the correlations themselves are NOT independent , \\nso 1. and 2. provide no direct inferential capability ).  This reliance on the angles, and their subsequent \\ntransformation to correlations, allows us to isolate specifically the distribution of the entire \\ncorrelation /dependence  matrix, for probabilistic inference, without touching any other distributional \\naspect of the data, which is the point of the methodology.  Of course, either a direct s imulation (step 1. \\nabove) or a cavalier ‘bootstrap’ of correlation /dependence  matrices based on step 1.  fails at this \\nobjective, because the non -independence of the cells undermines the validity of any empirically -based \\ninference  based on simple metrics (e.g. distances) of each sample within the group of sample matrices .  \\nIn other words, direct simulation does not preserve inferential capabilities, but angles simulation does.  \\nSo this framework is essentially identical to that for the specific case of the Gaussian identity matrix, with \\nthe only difference being it is based on nonparametrically defined, as opposed to parametrically defined, \\nangles distributions.  Before covering implementation details below, I show some examples of graphs of \\nthe angles distributions and the corresponding spectral distribution under challenging, simulated \\nfinancial returns data.  The multivariate returns distribution of the portfolio is generated b ased on the t -\\ncopula of Church (2012), with p=5 assets, varying degrees of heavy -tailedness (df=3, 4, 5, 6, 7), skewness \\n(asymmetry parameter=1, 0.6, 0, -0.6, -1), non-stationarity (st andard deviation=3σ, σ/3, σ; n/3 \\nobservations  each), and serial correlation ( AutoRegressive1 =-0.25, 0, 0.25, 0.50, 0.75), with a block \\ncorrelation structure shown in ( 34) below and n=126 observations  for a half year of daily returns .42  The \\nspectral distribution is compared against Marchenko -Pastur as a referential baseline.   \\n \\n41 Algorithms for sample generation based on commonly used kernels (e.g. the Gaussian and Epanechnikov) are widely known.  \\nAn example of the latter is simply the median of three uniform random variates (see Qin and Wei -Min, 2024).  \\n \\n42 Note that this is only approximately Church’s (2012) copula, which incorporates varying degrees of freedom (heavy -\\ntailedness) and asymmetry, because I also impose serial correlation and non -stationarity on the data (and subsequently  \\nempirically rescale the marginal densities).  \\nJD Opdyke, Chief Analytics Officer                  Page 47 of 88                      Beating the Correlation Breakdown  \\n (34)    \\nGraph 1a: Spectral Distribution –Angles/Kernel Perturbation v Data Simulations v Marchenko Pastur  \\n \\n \\n \\nGraphs 1-10: Angles Distributions – Angles/Kernel Perturbation v Data Simulations v Independence  \\n  \\n  \\n1 -0.3 -0.3 0.2 0.2\\n-0.3 1 -0.3 0.2 0.2\\n-0.3 -0.3 1 0.2 0.2\\n0.2 0.2 0.2 1 0.7\\n0.2 0.2 0.2 0.7 1\\n\\nJD Opdyke, Chief Analytics Officer                  Page 48 of 88                      Beating the Correlation Breakdown  \\n   \\n  \\n  \\nSeveral points are worth noting and reemphasizing from these graphs.  First, the graphs of the angles \\ndistributions contain three densities: A. one based on angles perturbation (i.e. sampling from the fitted \\nkernel) as described above in steps 3. -4., B. one based on direct data simulations (steps 1. -2.), and C. the \\nanalytical density under the (Gaussian) identity matrix as a comparative baseline.  Note that the only \\nreason I include B. is to demonstrate the validity of A., and as expected, the angle s distributions from A. \\nand B. are empirically identical (with A. being orders of magnitude faster and more computationally \\nefficient, not to mention inferentially valid ).  The spectral distributions based on the samples generated in \\nboth A. and B. also are identical, as are a wide range of additional aggregated metrics not presented \\nherein (e.g. various norms, VaR -based economic capital, and ‘generalized entropy’ as in a later chapter  \\nbelow).  This empirically validates that the angles -perturbation approach  (steps 3.-5.) is an efficient and \\ncorrect method for isolating and generating the distribution  of the correlation /dependence  matrix, and \\nunlike steps 1. and 2., one that preserves inferential capabilities.  In other words, these results \\n\\nJD Opdyke, Chief Analytics Officer                  Page 49 of 88                      Beating the Correlation Breakdown  \\n empirically validate  that the angles contain all extant information regarding dependence structure (see \\nFernandez -Duren & Gregorio -Dominguez, 2023, and Zhang & Yang, 2023, as well as Opdyke, 2024 a). \\nSecond, note again that a nonparametric approach works in practice here at least in part because the \\nangles distributions are ‘well behaved.’  Since they are relatively smooth , typically if not always unimodal , \\nand well bounded, N=10,000 simulations almost always suffice to provide a precise and accurate \\nmeasure of their distributions .  Poorly behaved distributions that are very spikey, multi -modal, and  \\nessentially  unbounded  for all empirical, practical purposes  could require numbers of simulations orders \\nof magnitude larger.  If N=10,000,000 or even 1,000,000 for example, this approach could be \\ncomputationally prohibitive in many cases for real -world-sized portfolios, which often exceed p=100 with \\np(p-1)/2=4,950 pairwise associations/cells.  \\nFinally, as described above, note the multi -modal and  high-upper-bounded nature of the spectral \\ndistribution for this portfolio compared to the angles distributions, where the biggest thing approaching \\nan estimation challenge is a modest asymmetry.  But this speaks only to estimation issues.  More notable \\nis the fact that on a cell -by-cell basis, the angles distributions deviate materially i. not only from central \\nvalues of 𝛑/2, and less dramatically from perfect symmetry when compared to their (analytic) \\ndistributions under the (Gaussian) identity matrix, but also ii. from each other!   Each angle’s distribution \\ncan vary quite notably compared to the other angles’ distributions, especially under smaller sample \\nsizes.  There simply is no way that one spectral distribution for a matrix, or even p distributions for each \\neigenvalue individually , even if perfectly estimated, will be able to capture and reflect all the richness of \\ndependence structure reflected here at the granular level of all the p(p-1)/2 pairwise cells.  This remains \\ntrue regardless of their  use in this setting , whether for  cell-level attribution analyses, granular scenario \\nand reverse scenario analyses, cell -level intervention ‘what if’ analyses, or customized stress testing, let \\nalone precise and correct inference at either the cell level OR the matrix level.  I now leave comparisons \\nto spectral distributions behind43 to cover implementation issues below.  \\nNonparametric Kernel Estimation  \\nDue to the bounded nature of the angles distributions on\\n()0,\\uf071\\uf070\\uf0ce , the nonparametric kernel must be \\nappropriately reflected at the boundary (see Silverman, 1986)  via:\\n() if 0 then ;if  then 2\\uf071 \\uf071 \\uf071 \\uf071 \\uf070 \\uf071 \\uf070 \\uf071\\uf03c \\uf0ac− \\uf03e \\uf0ac −\\n, which is asymptotically valid.  As per the standard \\nimplementation, the kernel itself is defined as   \\n(35) \\n() ()()\\n1111NN\\nh h i h i\\niif K K hN hN\\uf071 \\uf071 \\uf071 \\uf071 \\uf071\\n=== − = − \\uf0e9\\uf0f9\\uf0eb\\uf0fb \\uf0e5\\uf0e5  with  \\n \\n43 Continued reliance on spectral approaches for this specific problem brings to mind a quotation attributed to John M. \\nKeynes: “the difficulty lies not so much in developing new ideas as in escaping from old ones.”  \\n \\nJD Opdyke, Chief Analytics Officer                  Page 50 of 88                      Beating the Correlation Breakdown  \\n \\n()()22Gaussian: 1 2 Ke\\uf071\\uf071\\uf070−=\\uf0d7,     \\n()()()2Epanechnikov: 3 4 1 , 1 K\\uf071 \\uf071 \\uf071= \\uf0d7 − \\uf0a3 . \\nI have tested b oth the Gaussian and the Epanechnikov kernel s extensively  in this setting ,44 along with \\nthree different bandwidth estimators, h, from Silverman (1986)  and one from Hansen (2014), \\nrespectively : \\n15ˆ 1.06hN\\uf073−= \\uf0d7 \\uf0d7 , \\n150.79 IQRhN−= \\uf0d7 \\uf0d7 , \\n( )15ˆ 0.9 min IQR 1.34,hN \\uf073−= \\uf0d7 \\uf0d7 , and lastly, \\n15ˆ 2.34hN\\uf073−= \\uf0d7 \\uf0d7\\nfor Epanechnikov only, where \\nˆ\\uf073 = sample standard deviation and IQR = sample \\ninterquartile range.  \\n \\nAs with virtually all kernel implementations, the choice of kernel matters less than the choice of \\nbandwidth, although in this setting, across a broad range of data conditions and correlation /dependence  \\nvalues, the Epanechnikov kernel appears to perform slightly ‘better’ (i.e. with slightly less variance, thus \\nproviding slightly more statistical power) than the Gaussian, perhaps because its sharp bounds require \\nreflection at the boundary less often than the Gaussian kernel  (although reflection at the bound ary is \\nquite uncommon, even for ‘extreme’ dependence matrices ).  The bandwidth that appears to perform best \\nacross wide -ranging conditions is \\n( )15ˆ 0.9 min IQR 1.34,hN \\uf073−= \\uf0d7 \\uf0d7 .  Additionally, f or larger matrices \\n(e.g. p=100), bandwidths need to be tightened by multiplying h by a factor of 0.15 .  When there are many \\ncells (e.g. for p=100, #cells=p(p -1)/2=4,950) this tightening avoids  a slight drift in metrics that are \\naggregated across all the cells (e.g. correlation matrix norms, spectral distributions, and LNP (a type of \\n‘generalized entropy’ defined below)) .  Multiplying by this factor for smaller matrices does not adversely \\naffect the density estimation in any way, so this factor always is used.  For matrices much larger than \\np=100, a further tightening of this factor may  be required , and this is readily determined by empirically \\ncomparing the distributions of these aggregated metrics under i. direct data simulation  (steps 1. and 2.)  \\nvs. ii. NAbC’s kernel -based sampling  (steps 3., 4. and 5.) . \\nOnce the kernels have been estimated and the angles distributions generated by perturbing/sampling \\nbased on those kernels, the p -values and confidence intervals for both the individual \\ncorrelation /dependence  cells and the entire correlation /dependence  matrix are the same as those \\nderived for the Gaussian identity matrix.  The only difference, aside from their now -nonparametric basis, \\nis that the angles distributions are no longer symmetric by definition, as is true under the (Gaussian) \\nidentity matrix.   This can be seen in the Graphs 1-10 of the angles distributions provided above.  The p -\\nvalue calculation, however, remains very straightforward, and it requires just a bit of care to properly \\naccount for asymmetry.  The one -sided p-value remains simply (3 6): \\n(36) one-sided p-value  = \\n();XF x k  or \\n() 1;XF x k−  for lower and upper tails, respectively,  \\n             where k = n – column# – 2 \\n \\n44 Note that the Epanechnikov kernel is used in very closely related problems in this setting (see  for example  Burda and Jarosz, \\n2022).  \\nJD Opdyke, Chief Analytics Officer                  Page 51 of 88                      Beating the Correlation Breakdown  \\n However, due to possible (probable) asymmetry, the two -sided p-value is different, not just two times the \\none-sied p-value, and requires first the calculation of the empirical mean correlation matrix from the \\nsimulations in step 2. of the five sampling steps above.  This mean correlation matrix is then translated \\ninto a matrix of angles, and we obtain the empirical cdf values corresponding  to these “mean angles” \\nwith a “look -up” on the entire angles distributions generated in step 4.  These cdf’s will be cl ose to 0.5 \\nwhen the angles distributions are close to symmetry, and they will deviate from 0.5 under asymmetry , \\nand will serve as the baseline off of which the two -sided p-values are calculated .  Specifically, the \\ndifference between the cdf values of each of the angles of the specified correlation matrix being ‘tested,’ \\nand those of the “mean angles ,” defines the two -sided p-values, which are simply the  sum of the \\nprobability in the tails BEYOND this difference.  Formulaically this is shown in  (37): \\n(37)    two-sided p-value = max[0, Mcdf – d] + max[0, 1 – (Mcdf + d)],  where  \\n       d = abs(Mcdf – cdf), Mcdf = mean angle cdf, cdf = cdf of specified angle  \\n \\nThis usually results in summing both tails, but under notable asymmetry, sometimes only one tail is used.  \\nBelow is a graphical example of both cases, where the cdf of the “mean angle” is 0.6 and the cdf of the \\nrelevant angle in the specified correlation m atrix (i.e. the correlation matrix for which we are obtaining p -\\nvalues) is cdf=0.1 in the single -tail case (Graph 11) and cdf=0.85 in the double -tail case (Graph 12).  In the \\nstatistical sense, however, both cases remain two -sided p-values.  \\n \\n \\nNote that while cdf=0.1 is hardly more ‘extreme’ than cdf=0.85 in absolute terms, relative to the mean \\nangle cdf=0.6, it is twice as ‘extreme,’ i.e. twice as far from the mean cdf=0.6  with a distance of 0.5  for \\nGraph 11, but a distance of only 0.25 for Graph 12.  Moreover, the tail probability of Graph 11 (0.1) is only \\n1/5 that of Graph 12  (0.5) (compare the red shaded areas).  This example demonstrates why asymmetry \\nmust be properly taken into account in this setting, but the two -sided p-value still remain s a very \\nstraightforward calculation, and the “mean angles” matrix is used for additional, important purposes \\nbelow, as discussed in the Scenarios section.  \\nCell-level confidence intervals still are simply calculated as in ( 31), which automatically takes \\nasymmetry into account  as we are using the empirical cdf .  This is identical to the same calculation \\nunder the (Gaussian) identity matrix  (sans the known symmetry of the cdf) .  And the matrix -level p-value, \\nagain, is simply one minus the probability  of observing the sample matrix that was observed or one ‘less \\n\\nJD Opdyke, Chief Analytics Officer                  Page 52 of 88                      Beating the Correlation Breakdown  \\n extreme,’  exactly as in (32).  This, again, is analogous to the definition of controlling the family -wise error \\nrate (FWER) where all the cells of the matrix  comprise the joint null hypothesis .  Finally, just as under the \\n(Gaussian) identity matrix, calculation of the confidence interval for the entire matrix remains ( 33) as \\npreviously.   \\nImportantly, again note that we can go in either direction regarding these results: we can specify a \\ncorrelation /dependence  matrix and, under the null hypothesis of the specified correlation matrix, obtain \\nthe p-values of an observed matrix, both for the individual cells and the entire matrix, simultaneously.  \\nWe also have the matrix -level quantile function: we can specify a m atrix of cdf values and obtain its \\ncorresponding, unique correlation /dependence  matrix, which can be extremely useful and \\nstraightforward when constructing reverse  (stress) scenarios.  Finally, we can use simultaneous \\nconfidence intervals to obt ain the two correlation matrices that form the matrix level confidence interval.  \\nAn example with all these results is shown in the “One Example” section below, but first I extend NAbC’s \\nrange of application beyond Pearson’s to all dependence measures with positive definite all -pairwise \\nmatrices . \\n \\nNAbC: Any (PD) Dependence Measure , Any Data, Any Matrix Values  \\nTo show that NAbC can be applied to any dependence measure  with a positive definite all -pairwise \\nmatrix, just as it is to Pearson’s, we just need to show 1. that the relationship between angles and \\nPearson’s holds not just for Pearson’s, but for any positive definite matrix; and 2. the all-pairwise matrices \\nof the dependence measures listed and described in the Background  all are positive definite.  As shown \\nin (20), (21), and (22) , 1. has been proven  and is well established in the literature (see  Joarder and Ali, \\n1992, Pinheiro and Bates, 1996; Rebonato and Jackel, 2000; Rapisarda et al., 2007; Pouramadi an d \\nWang, 2015; Cordoba et al., 2018; and Lan et al., 2020) .  For the “big 3” (Pearson’s, Kendall’s and \\nSpearman’s) , 2. has been proven  by many, including Sabato et al. (2007).  I prove 2. below for all \\ndependence measures with all -pairwise matrices with unit diagonals and values ranging from zero to one \\non the off -diagonals , i.e. all those discussed in the Background section above .  Embrechts et al. (2016) \\ndid this already specifically for the tail dependence matrix .  Recall the definition of positive defini teness \\n(for a matrix of dimension p):  \\nif 0 for all \\\\px Rx x\\uf0a2\\uf03e\\uf0ce 0\\n, then R is positive definite.  \\n \\nBecause all of the (0,1) dependence measures described above are defined by  \\n, , , , 0 1 for all  and 1 and i j i i i j j iR i j R R R\\uf0a3 \\uf0a3 \\uf0b9 = =\\n, \\nx Rx\\uf0a2 can be written in quadratic form as  \\n(38) \\n1\\n2\\n,\\n1 1 12p p p\\ni j i j\\ni i j ix Rx x R x x−\\n= = = +\\uf0a2=+\\uf0e5 \\uf0e5\\uf0e5  \\nAs long as \\n, 0 1 for all ijR i j\\uf03c \\uf03c \\uf0b9 , that is, the coefficients on the cross terms (the second term of ( 38)) all \\nremain BETWEEN 0 and 1, then  \\nJD Opdyke, Chief Analytics Officer                  Page 53 of 88                      Beating the Correlation Breakdown  \\n (39) \\n1\\n2\\n,\\n1 1 12 0  and so  0p p p\\ni j i j\\ni i j ix R x x x Rx−\\n= = = +\\uf0a2 + \\uf03e \\uf03e\\uf0e5 \\uf0e5\\uf0e5 , always, and so R is positive definite.  \\nIn the p = 2 case, for example, R is positive definite if \\n( )2\\n1,1 1,1 2,2 1,20 and 0 R R R R\\uf03e − \\uf03e , which is always true \\nwhen \\n,, 0 1 for all  and 1i j i iR i j R\\uf03c \\uf03c \\uf0b9 = .  For the boundary cases, if \\n,0 for all ijR i j=\\uf0b9 , R obviously \\nremains positive definite as the first term of ( 38) always is greater than zero and the second term \\ndisappears; if \\n,1 for some but not all ijR i j=\\uf0b9 then R is singular; and if \\n,1 for all ijR i j=\\uf0b9 then R is \\npositive semi -definite, although this case of perfect multivariate dependence is only textbook relevant.  \\nIn practice, empirically, positive semi -definiteness only is relevant as a boundary condition, as it relates \\nto empirical matrices that approach s ingularity.   Consequently, this means that all dependence \\nmeasures with values ranging from 0 to 1 are, in practice, positive definite, and that NAbC can be applied \\nto them to define their finite sample distributions.  Empirical examples of this are shown in following \\nchapters . \\nOperationally, implementing NAbC on these (0, 1) measures is no different from implementing it on \\nPearson’s or Kendall’s or Spearman’s; the (0, 1) instead of ( –1, 1) range does not even change how we \\nreflect at the boundary when fitting the nonparametric k ernel.  This is because specific cells of the \\nCholesky factor can validly be negative, making the assignation in the last line of the “Correlations to \\nAngles” code in Table A above sometimes assign an angle value slightly above 𝛑/2, even though 𝛑/2 \\ncorresponds to a measure value of zero.45  So this is a soft upper boundary in this case, even though the \\nmeasure’s range of (0,1) typically is not.46  So when NAbC generates angle 𝛉, we continue to reflect based \\non: \\n() if 0 then ;if  then 2\\uf071 \\uf071 \\uf071 \\uf071 \\uf070 \\uf071 \\uf070 \\uf071\\uf03c \\uf0ac− \\uf03e \\uf0ac − , since for measures with a (0,1) range, the upper bound \\nof 𝛑 will never be reached, and the lower bound of zero remains valid and hard.  So NAbC applies in \\nexactly the same way, for all of these positive definite dependence measures, whether their range of \\nvalues is ( –1, 1) or (0, 1).  \\nFinally, again note that the condition of symmetric positive definiteness holds not only for all relevant \\ndependence measures, as shown above, but also under all relevant real -world data conditions: that is, \\nmultivariate financial returns data whose margin al distributions typically are characterized by varying and \\ndifferent degrees of asymmetry, heavy -tailedness, (non -)stationarity, and serial correlation.  So this is a \\nvery weak and general condition, allowing for the extremely wide -ranging application of NAbC.  \\n \\n45 Note that angle values (which range from zero to 𝛑 on the hyper -hemisphere) decrease while dependence measure values \\nincrease, so a measure value of -1 corresponds to an angle value of 𝛑, a measure value of zero corresponds to an angle value \\nof 𝛑/2, and a measure value of 1 corresponds to an angle value of zero ( see Zhang et al. , 2015 and Lu et al. , 2019). \\n \\n46 On a related issue, note that Chatterjee’s correlation, for example, is bounded by (0,1) only asymptotically, and finite samp le \\nresults can exceed these bounds.  However, when applying NAbC to this and other measures in hundreds of thousands of data \\nsimulations under widely varying conditions, as an empirical matter such finite sample exceedences never caused NAbC’s \\nangles distributions to deviate from those of direct data simulations, nor did they ever make  empirical matrices not positive \\ndefinite.  \\nJD Opdyke, Chief Analytics Officer                  Page 54 of 88                      Beating the Correlation Breakdown  \\n Spectral and Angles Distributions, Examples from Other Dependence Measures  \\nI present below  graphs of  the spectral and angles distributions for some of the dependence measures \\ndiscussed above, beyond Pearson’s, under simulated data reflecting challenging, real -world data \\nconditions (see Opdyke, 2024 a, for the application of NAbC to a very wide range  of different data \\nconditions).  As in the above example, the multivariate returns distribution of the simulated portfolio is \\ngenerated based on the t -copula of Church (2012), with p=5 assets, varying degrees of heavy -tailedness \\n(df=3, 4, 5, 6, 7), skewness (asymmetry parameter=1, 0.6, 0, -0.6, -1), non-stationarity (standard \\ndeviation=3 σ, σ/3, σ; 1/3 observations each), and serial correlation (A utoRegressive 1=-0.25, 0, 0.25, \\n0.50, 0.75), with a block correlation structure shown in (34) below and n=126 observations, for half a year \\nof daily returns.47  \\n \\n(34)  \\n \\nFor verification purposes only, I compare those angles distributions based on the data simulation directly \\nagainst those based on NAbC’s kernels, and in all cases the results are empirically indistinguishable.  The \\nsame is true for the spectral distributio ns, which I also present below against the Marchenko -Pastur \\ndistribution as a(n independence) baseline (see Marchenko and Pastur, 1967).  The empirical results yield \\nboth expected, and some additional interesting findings.   \\nFirst, note that the spread, and the spread and shifts, of both the spectral and angles distributions, \\nrespectively, are larger for Pearson’s than for Kendall’s, which is consistent with the former’s relative \\nsensitivity to more extreme values under many c onditions.  The shifts and spread of both measures are \\nmuch larger than those of Chatterjee,48 although this is largely due to the fact that while Chatterjee is \\ngenerally more powerful under dependence that is cyclical or non -monotonic in some way, it is less \\npowerful under associations that are more monotonic, and the data conditions of this examp le fall more  \\n(but not entirely) into the latter category.  The story changes a bit when we use the dependence measure \\nsuggested by Zhang (2023), which is essentially a maximum between Spearman’s rho and Chatterjee’s \\ncorrelation, its objective being to obt ain large, if not the maximum power under both types of \\ndependence structures (i.e. strong monotonic dependence as well as cyclical or otherwise non -\\nmonotonic dependence).  This shows how readily NAbC can be applied to any (positive definite)  \\n \\n47 Note again that this is only approximately Church’s (2012) copula, which incorporates varying degrees of freedom (heavy -\\ntailedness) and asymmetry, because I also impose serial correlation and non -stationarity on the data (and subsequently  \\nempirically rescale the marginal densities).  \\n \\n48 The symmetric version of Chatterjee’s correlation coefficient is used here (see Chatterjee, 2021), with the finite sample bia s \\ncorrection proposed by Dalitz et. al., 2024.  \\n1 -0.3 -0.3 0.2 0.2\\n-0.3 1 -0.3 0.2 0.2\\n-0.3 -0.3 1 0.2 0.2\\n0.2 0.2 0.2 1 0.7\\n0.2 0.2 0.2 0.7 1\\nPage 9 of 19 \\n Graphs 13a: Angles Distributions --NAbC Angles Kernel v Data Simulations v Identity Matrix  \\n             Pearson’s Rho           Kendall’s Tau            Chatterjee’s            Spearman’s Rho+Chatterjee  \\n  \\n  \\n    \\n \\n \\nPage 55 of 88 JD Opdyke, Chief Analytics Officer  Beating the Correlation Breakdown  \\nPage 9 of 19 \\n Graphs 13b:  Angles Distributions --NAbC Angles Kernel v Data Simulations v Identity Matrix  \\n              Pearson’s Rho           Kendall’s Tau            Chatterjee’s            Spearman’s Rho+Chatterjee  \\n \\n \\n \\n \\nPage 56 of 88 JD Opdyke, Chief Analytics Officer  Beating the Correlation Breakdown  \\nJD Opdyke, Chief Analytics Officer                  Page 57 of 88                      Beating the Correlation Breakdown  \\n Graph 14: Spectral Distribution -NAbC Angles Kernel v Data Simulations v Marchenko Pastur  \\n          Pearson’s Rho           Kendall’s Tau     \\n   \\n    Chatterjee’s       Spearman’s Rho+Chatterjee  \\n  \\n \\ndependence measure, and its utility for making cross -measure comparisons, all else equal, using the  \\nsame, universally applicable method.  \\nBefore providing a complete example of NAbC’s application below , i.e. one that provides both matrix and \\ncell level p -values and confidence intervals, and checks all but one of the original objectives listed in the \\nIntroduction , I treat two of its  additional and  important capabilities: the first is its use a s a two-sample \\ntest of two correlation/dependence matrices, and the second is  as a framework for fully flexible scenario \\nanalytics, providing granular, realistic scenario analytics far beyond what any of its competitors  can \\nprovide.   \\n \\nNAbC: Fully General  Conditions , Statistical Comparison of Two Matrices  \\nThe above develop ment of NAbC’s sampling distribution for purposes of statistical inference – generating \\np-values and confidence intervals at both the cell and matrix levels,  with analytical  consistancy across \\nlevels – has so far covered only hypothesis tests against a matrix of fixed values, i.e. a one -sample test.  \\nBut we can use NAbC to perform two-sample tests of one sampled matrix against another  sampled \\nmatrix, say, from two different sectors or two different business lines, where the null hypothesis is no \\n\\nJD Opdyke, Chief Analytics Officer                  Page 58 of 88                      Beating the Correlation Breakdown  \\n difference between the dependence structures of the two sectors .  The implementation is very similar  to \\nthe one sample case, except that N=10,000 samples based on the estimates of each of the two angle \\nmatrices are generated separately .  Then the differences between the two groups of samples  of angles , \\nsample i from the first minus sample i from the second,  are calculated, and the N differences are tested \\nagainst the  values of the  identity matrix,  i.e. values of zero representing zero difference between the  two \\nmatrices,  similar to testing a single sample against the null hypothesis of the identity matrix.  The only \\ndifference is that we must use the “mean angles” cdfs to account for asymmetry in the angles \\ndistributions slightly differently: instead of averaging the correlation matrices and then converting this \\naverage matrix into the matrix of “mean angles” cdfs , we just calculate the average of the  difference -\\nbetween-angles distributions  directly across all the simulations, for each cell , to obtain the “me an \\nangles” cdfs.   This avoids incorporating into the asymmetry adjustment what are possibly true \\ndifferences between the two matrices, i.e. the hypothesis we are testing.  The only constraint s on this \\napproach are that the two matrices being compared should be  the same type of dependence measure \\n(e.g. Szekely’s vs. Szekely’s, not Szekely’s vs. Chatterjee’s)  and have the same dimension.49  I include in \\nthe “One Example” section below an empirical example of this two -sample application of NAbC.  This \\nprocedure is readily extended to the multi -sample case , but this is addressed in more detail in future \\nresearch .  Below, I briefly describe how NAbC remains “estimator agnostic” before moving on to NAbC’s \\napplication to fully flexible scenarios.  \\n \\nNAbC Remains “Estimator Agnostic”  \\nAs mentioned briefly in the Introduction, another important and useful characteristic of NAbC is that it \\nremains “estimator agnostic,” that is, valid for use with any reasonable estimator of any of the \\ndependence measures being modeled (e.g. Kendall’s or P earson’s or Chatterjee’s, etc.).  Different \\nestimators will have different characteristics under different data conditions.  For example, some will \\nprovide minimum variance / maximum power, while others may provide unbiasedness or less bias, while \\nothers m ay provide more robustness, and/or different and shifting combinations of these characteristics.  \\nIdeally, we would like to be able to use estimators that provide the best trade -offs for our purposes under \\nthe conditions most relevant to our given portfoli o.  Fortunately, NAbC “works” for any estimator, as the \\nrelationship between correlations /dependence measure values  and angles requires only symmetric \\npositive definiteness.  NAbC’s finite sample distribution and its resulting inferences obviously will inh erit \\nthe advantages and disadvantages of the estimator being used, but this is generally an advantage as it \\n \\n49 Note that, as an empirical method, the ability to implement NAbC relies on the degree to which the empirical distributions of  \\nthe angles approximate continuous distributions over their entire sample spaces.  This is largely controlled by the number of  \\nsimulations run, and fortunately the “good behavior” of the angles distributions renders N=10,000 simulations, which is \\ncomputationally feasible even for non -small matrices, more than sufficient ly large in most cases.  However, empirically \\nchallenging cases can arise.  For example, if we are comparing two sample matrices where some cell  values are quite \\ndifferent, the distribution of the difference between the two angles distributions  (corresponding to the same cell in each \\nmatrix) may not contain the value zero, in which case the empirically -based p-value would be exactly zero.  Like any empirical \\nmethod (e.g. bootstraps, permutation tests, etc.) care must be taken to ensure that the consequences of such results are \\nnoted, understood, and properly accounted for.  \\nJD Opdyke, Chief Analytics Officer                  Page 59 of 88                      Beating the Correlation Breakdown  \\n provides flexibility to use the ‘best’ estimator under the widest possible range of conditions.  Note that a ll \\nempirical results presented herein use the sample estimator s specified in the Background section , and \\nsample sizes in every example all exceed 10p (10 times the dimension of the matrix), which is a widely \\nused threshold for whether a more sophisticated, bias -correcting estimator is needed , at least for \\nPearson’s matrix  (see Bongiorno  et al., 2023).  As mentioned in the Estimation chapter above, I \\nrecommend for conditional (forecast) estimation the Average Oracle  (AO) of Bongiorno  et al. (2023) (see \\nalso Bongiorno  & Challet, 2023a, for an extensive empirical study against competitors) .  Further testing \\nmay show that AO can be applied to all positive definite dependence measures as well, although this \\ncurrently is the topic of my continuing research.   Now in the next section , I show how all of the other \\npreviously derived characteristics of NAbC remain valid for the scenario -restricted case, that is, w hen \\nselected cells of the all -pairwise matrix are ‘frozen’ as dictated by specific scenarios, while the rest are \\nallowed to vary.   \\n \\nNAbC: Granular, Fully Flexible Scenarios , Reverse Scenarios,  and Stress Testing  \\n \\n“Correlation is one of the most important, if not the most important, risk factor in finance, driving \\neverything … however, a unified and generally accepted correlation risk management framework does not \\nyet exist” (Packham & Woebbeking, 202 3, p.1).  \\nThe size, breadth, and surprise of the effects of correlation breakdowns are well documented  in the \\nliterature  (see Kim & Finger, 1998; Loretan & English, 2000; Li et al., 2024;  BIS, 2011a, 2011b; Nawroth et \\nal., 2014; Ng et al., 2014; Yu et al., 2014; Chmeilowski, 2014 ; Epozdemir, 2021; Feng & Zeng, 2022; and \\nParlatore and Philippon, 2024 ), if underappreciated during periods of relative market calm : “Furthermore, \\njoint distributions estimated over periods without panics will misestimate the degree of correlation \\nbetween asset returns during panics.” (FRB Chairman, Alan Greenspan,1999).   And yet despite its \\nimportance,  the ability to model, predict, and mitigate correlation breakdowns  effectively across very \\ndifferent scenarios , in a fully flexible way , has remained elusive.   \\nTo start with, although many approaches do otherwise, it is not enough to stress only the inputs to a \\ncorrelation/dependence matrix – the matrix itself must be stressed and evaluated under stressed \\nconditions of a particular (extreme) scenario:    “… in order to calculate stressed VaR accurately it is also \\nnecessary to stress the correlation matrix … most correlations tend to increase during market crises, \\nasymptotically approaching 1.0 during periods of complete meltdown, such as occurred in 1987, 1998 \\nand 2008. …Certain methods that could be meaningful [include e] mploying fat -tailed distributions for the \\nrisk factors and replacing the standard correlation matrix with a stressed one … .” (BIS, 2011a).  Secondly, \\nif a method perturbs eigen decompositions  and/or polar angles  to obtain correlation/dependence \\nmeasure distributions, this cannot be done on an ad hoc basis, using  mathematically  convenient \\ndistributions, like the Gaussian,  to perturb eigen values , or arbitrary bounded functions, like inverse \\ntangent, to perturb angles  (see Galeeva, 2007).  Spectral  and spherical  distributions follow specific and \\nJD Opdyke, Chief Analytics Officer                  Page 60 of 88                      Beating the Correlation Breakdown  \\n often known distributions under various conditions, and such approaches need methodological support, \\nwhether theoretical or empirical or both, to justify their use  when taking what is otherwise a smart \\napproach to generat ing scenario-specific correlation/dependence matrices .  Additionally, such methods \\nmust remain cognizant of all the characteristics of the conditions they are attempting to generate.  Hardin \\net al. (2013) , for example,  utilize a normalized vector of independent gaussian random variables to \\nperturb the observed correlation matrix , but correctly note that “The amount of noise that can be added \\nto the original matrix is determined by its smallest eigenvalue. … We provide the user with … a general \\nalgorithm to apply to any correlation matrix for which the smallest eigenvalue can be reasonably \\nestimated. ” (emphasis added) .  Unfortunately, as mentioned above, this eliminates what are arguably the \\nmost widely observed correlation matrices in finance – those based on  a ‘spiked’ covariance matrix (see  \\nJohnstone, 2001) where one or a few eigenvalues dominate and the majority of eigenvalues are close to \\nzero, i.e. not reliably estimated.   Robustness as dependence matrices approach singularity/NPD is an \\nimportant quality of any method, but it remains especially critical in financ ial applications.  \\nSeveral other  approaches avoid these limitations (see Packham and Woebbeking , 2021, Chmielowski , \\n2014, and Parlatore and Phillippon , 2024) but they have other  arguable limitation s (e.g. Packham and \\nWoebbeking , 2021, enforce positive definiteness ex post, which as described above distorts the desired \\ndistributions  of dependence measures ), and none provide granular, cell -level control , to restrict \\nperturbation on  any comb ination of cells, while still obtain ing a valid distribution of the remaining cells of \\nthe correlation/dependence matrix being used.  Yet this is exactly what is needed  for realistic scenario \\nanalytics and stress testing , let alone precise attribution analyses and ‘what if’ analysis capabilities .  \\nCorrelation /dependence matrices under a tech market bubble (2000) vs those under a housing bubble \\n(2008) vs those under Covid (2020) will change very different individual cells  of the all-pairwise matrix , \\nand very different combinations of cells, in very different ways, often in terms of both direction and \\nmagnitude, while leaving many cells strongly affected under one upheaval completely unaffected under \\nanother (see Feng & Zeng, 2022) .  But some  combinations of cells will change in similar ways, and \\ndistributional analyses must be able to accommodate every  possible combination of change s, in terms \\nof both magnitude and direction.  In other words, while correlation ‘breakdowns’ will occur under all of \\nthese extreme conditions, the granular nature of all-pairwise matrices ensures that the fundamentally \\ndifferent (and sometimes similar) nature of these breakdowns will be captured and reflected empirically \\nin all related analyses.  Although some approaches settle for stretching across several different \\ncovariance/correlation matrices , with fixed values,  representing several different scenarios ( see Parlatore \\nand Phillippon , 2024), this argua bly is simply too rigid and discrete and limited for realistic analyses  of \\nthe dynamic d istributions  of these matrices  in a way that  remains robust across qualitatively different, \\nand often as yet unobserved (future) breakdowns .  Neither does the separate matrix -by-matrix approach  \\nallow for flexible, all-else-equal, targeted ‘what if’ analyses, or granular attribution analyses.   If we are to \\nachieve the same level of flexibility in quantitatively modeling  dependence matrices  that has been \\nattained for the other parameters in the risk and investment models of our portfolios, practitioners and \\napplied researchers must be able to flexibly and realistically model dependence matrices  at the most \\ngranular level – that of the individual correlation cells  – without unnecessary restrictions.  \\nJD Opdyke, Chief Analytics Officer                  Page 61 of 88                      Beating the Correlation Breakdown  \\n Despite the research  on correlation breakdowns listed above, I am aware of o nly two other limited \\nattempts at this granular level of modeling in the literature (see Saxena et al., 2023, and Veleva, 2017 ), \\nand both are restricted in notable ways.50  Fortunately, NAbC allows for specifying ANY combination of \\ncells, within the framework of the all -pairwise matrix,  to be ‘frozen’ at their current values while allowing \\nall the rest to vary, providing full flexibility within th is framework.  \\nSeveral results allow for this  full flexibility .  First, 1. independence of the angles distributions allows us to \\nvary individual cells.  Second, 2. the distributions of individual correlation cells, as well as the distribution \\nof the entire correlation matrix, both remain invariant to the ordering of the rows and columns of the \\nmatrix (see Pourahmadi & Wang, 2015, and Lewandowski et al., 2009).  Third, 3. based on 1. and 2., we \\ncan exploit the simple mechanics of matrix multiplication so that only selected cells of the matrix are \\naffected, and the rest  frozen, as required for a given scenario.  \\nTo explain 3., I focus only on the lower triangle of the correlation matrices below in Graphs 1 5-17, since \\nthe upper triangle is just its reflection  due to symmetry .  Note again that using NAbC, we only perturb \\nangles.  We never perturb the correlation values directly.  We must always convert to angles, perturb the \\nangle values  using the fitted kernels , and then translate back to correlation values.  In doing so, when \\nmultiplying the Cholesky factor by its transpose, \\nTR BB= , changing a given angle cell in matrix B will \\naffect other cells, but only those cells to the right of it in the same row, and those below the diagonal of \\nthe corresponding column, as shown graphically for several examples in Graph 1 5 below.51 \\n \\nGRAPH 15: Mechanics of Matrix Multiplication  \\n \\n \\nThis means that we can simply reorder the matrix so that the targeted cells we want to vary all end up in \\nthe rightmost triangle of the lower triangle, according to the fill order in Graph 16 below.  \\n \\n \\n50 Saxena et al. (2023) explores the possibility of restricting individual covariance/correlation terms to zero,  although  they are \\nnot always able to enforce this restriction while maintaining positive definiteness.  Velena (2017) restricts the values of the \\ncorrelation matrix being simulated to specified ranges, but only for all off-diagonal cells ; in some cases, one algorithm allows \\nfor cell-level values to be specified, but without guarantees of positive definiteness . \\n \\n51 Note that not all of these (orange) cells will necessarily change if values of zero are involved, but none OTHER than these \\n(orange) cells CAN change when only the red cell changes.  \\n1 1 1 1 1 1\\n1 1 1 1 1 1\\n1 1 1 1 1 1\\n1 1 1 1 1 1\\n1 1 1 1 1 1\\n1 1 1 1 1 1\\nJD Opdyke, Chief Analytics Officer                  Page 62 of 88                      Beating the Correlation Breakdown  \\n GRAPH 16: Rightmost Triangle Fill Order  \\n \\nIf we only change in matrix B the angle values of cells 1, 2, and 3 above, no other cells in the correlation \\nmatrix R will be affected, simply by virtue of the mechanics of matrix multiplication from  \\nTR BB= .  Below \\nI show another example.  R eorder the correlation matrix so that rows 1 -6 are now 6 -1 and columns 1 -6 are \\nnow 6-1, so that the original cells 1,2 and 1,3 and 2,3 and 4,3 are now in the rightmost triangle of the \\nlower triangular matrix, in the fill order shown above . \\n \\nGRAPH 17: Example of Mechanics of Matrix Multiplication Applied to Rightmost Triangle Fill Order  \\n \\nChanges to the corresponding cells in the angles matrix B (the orange cells) will only change these same \\ncells , after \\nTR BB= , in the resulting correlation matrix , R, leaving the rest unaffected .  Note that the \\ngreen cells to be targeted for change do not even have to be contiguous, nor do they have to completely \\n‘fill’ the rightmost (orange) triangle (note that cells 5 and 6 in the center matrix are not targeted): they only \\nmust fill the rightmost triangle according to the order of the center matrix above.  Note also that the \\n“rightmost triangle” rule is nested/hierarchical: if I wanted to perform ‘what if’ analyses on only one of \\nthose cells (e.g. cell “1,2”) without changing the other three, I order the original correlation matrix to \\nplace that cell as the ‘first’ in the lower triangle of the B matrix, as shown.  Then, subsequent changes to it \\nwill not affect the other (orang e) cells, let alone any other non -orange cells .  In contrast, changes to cell \\n“4,3” will affect the values of the other orange cells  (but not the non -orange cells) .  Readers are \\nencouraged to test this in the interactive spreadsheet (url link provided above).  \\nSo we can exploit these four simultaneous conditions – 1. independence of the angles distributions; 2. \\n(correlation /dependence matrix ) distribution invariance to row and column order; 3. the mechanics of \\nmatrix multiplication; and 4. the granular, cell -level geometry of NAbC – to obtain great flexibility in \\ndefining scenarios wherein some cells vary and some do not.  No other approach a llows this degree of \\nRightmost Triangle Fill Order  \\nDetermine Targeted Change Cells  \\nReorder Rows/Cols to Fill Rightmost Triangle \\nwith Targets According to Fill Order  \\nChanges in Corresponding \\nAngles Cells ONLY change \\nSame in Resorted Matrix  \\n\\nJD Opdyke, Chief Analytics Officer                  Page 63 of 88                      Beating the Correlation Breakdown  \\n flexibility, which is what is required for defining correlation/dependence matrices for use in realistic, \\nplausible, and sometimes extreme stress market scenarios.  This also greatly simplifies attribution \\nanalyses, isolating and making transparent the ide ntification of effects due to specific pairwise \\nassociations, which is something spectral and more aggregated analyses cannot do in this setting.  \\nThe above allows for the specification of ANY scenario within the structure of the pairwise matrix.  Note, \\nhowever, that some scenarios can include combinations of cells which are forced to include (in the lower \\nright triangle) one or a few cells not affec ted by the scenario.  This is unavoidable due to the structure of \\nthe pairwise matrix: for example, in the matrix above, there are only p! (i.e.5!=120) ways to sort the rows \\nand columns, but there are [p(p1 -)/2]! (i.e.15!= 1,307,674,368,000) ways to sort t he 15 cells  freely.  The \\nmatrix obviously cannot accommodate freely sorting the individual cells in this way because it breaks the \\nstructure of the matrix.  Some scenarios, therefore, could conceivably be required to include for \\nperturbation some few additional cells in the  lower rightmost triangle that are not relevant to the scenario \\nand otherwise should be held constant.  Fortunately, in practice, especially with large matrices, this \\nappears to be a relatively rare occurrence, and when it happens, the eff ects are identifiable so that \\nmateriality can be assessed  via ‘what if’ analyses on the specific cells .  But dealing with these potential \\ncases appears to be well worth the price of the unmatched flexibility that using NAbC and the all -pairwise \\nmatrix provides,52 not to mention the other advantages it maintains over more complex, strictly \\nmultivariate dependence structures.  For usage with actual market data, the latter typically are more \\ndifficult to estimate with the same levels of precision  and statistical power , let alone to manipulate for \\npurposes of intervention or mitigation.  In contrast, pairwise associations are directly identifiable, \\ntypically more easily and accurately estimated,53 and intervention  tests are more targeted and \\ntransparent.  \\nTo conclude this section , I deal with one final implementation issue.  When the matrix is scenario -\\nrestricted, and we only perturb a subset of the matrix while keeping the remaining cells fixed, what values \\ndo we use for those ‘frozen’ cells?  This is where the mean angles matrix,  used to account for asymmetry \\nwhen calculating the two -sided p-values in the previous section , comes into play.  When the matrix \\nangles are sampled using the fitted kernel densities, a sample is drawn from the entire matrix, and if it is \\nscenario restrict ed, the sampled values for those cells that are ‘frozen’ are simply overwritten with their \\nmeans.  So after N=10,000 samples, all 10,000 values of the ‘frozen’ cells unaffected by the scenario will \\n \\n52 Most of the related scenario literature perturbs scenario -based cells and simply ignores their ( often notable) effects on the \\nrest of the matrix (which should remain ‘frozen,’ but isn’t), not to mention the effects of the rest of the matrix on the scenario -\\nrelated cells .  These papers  euphemistically refer to the former as ‘peripheral’ correlations (see  Ng et al. (201 4) and Yu et al. \\n(2014)).  NAbC is the only method that fully controls the values and thus, the indirect effects of these so -called ‘peripheral’ \\ncorrelations.  \\n \\n53 They also can be estimated  rigorously, and with targeted precision and flexibility, with well -established methods such as \\nvine copulas (see Czado and Nagler, 2022)).  Ironically, however, when used for inference or sampling  for this problem  \\nspecifically, vine copulas and similar methods become extremely unwieldy and much more complex and less transparent \\nthan NAbC, not to mention ungeneralizable beyond Pearson’s (see the vine and extended onion algorithms of Lewandowski et \\nal. (2009), and the similar chordal sparsity method of Kurowicka (2014)).  \\n \\nJD Opdyke, Chief Analytics Officer                  Page 64 of 88                      Beating the Correlation Breakdown  \\n have the same mean value  for that specific cell , and when translated via \\nTR BB=  back into correlation \\nmatrices, all the correlation values for those cells will be their mean  correlation  value.  In other words, \\ntheir values will not change, and will remain ‘frozen ,’ based on a reasonably robust estimator of their true \\nvalue (note that these ‘frozen’ values are not based on a single estimated matrix, but rather , they each are \\nthe mean s of N=10,000 matrices).  The order of magnitude of empirical accuracy of these values is \\ninversely related to the number of samples drawn, N.  In the example in the “One Example” section \\nbelow, we observe accuracy to the fourth decimal place for th ese frozen cells when N=25,000  \\nsimulations , as expected.  Alternately, the values could be treated as truly known constants from the \\nbeginning, but it is more conservative (and realistic) to use estimates based on the mean of all the \\nsamples.54 \\nI end this section  by reemphasizing that this matrix sorting method for providing fully flexible scenarios, \\nwithin the framework of the all -pairwise matrix, applies not only to Pear son’s, but also to all positive \\ndefinite dependence measures , under the fully general data conditions for financial portfolios described \\nabove.  One complete, empirical example of  this, and all of NAbC’s applications, covering all but one of \\nits original objectives described in the Introduction, is shown below.  \\n \\nNAbC Example: Kendall’s Tau p -values & Confidence Intervals, Unrestricted & Scenario -restricted  \\n \\nNow, with NAbC’s characteristics and broad range of application described above, I can present a \\ncomplete example of its implementation here.55  This example will check seven of the eight original \\nobjectives boxes listed in the Introduction  above (solely for ease of results replication, the data \\ngenerating mechanism for th is example is simply multivariate standard normal , but the example ‘works’ \\nwhen a ‘real world’ data generating mechanism is used ).  The dependence measure chosen is Kendall’s \\nTau, under two cases: unrestricted, and scenario -restricted.  NAbC provides both p -values and \\nconfidence intervals, at both the cell level and matrix level , with N=25k simulations and number of \\nobservations n = 160 , representing about eight months of daily market returns .  The values of the matrix \\n[A] are based on a Pearson’s matrix from A’ below, translated to A via \\n()() 2 arcsin r \\uf074\\uf070= , which is valid \\n \\n54 Note that when NAbC is being used as a two sample test in the scenario -restricted setting , we are only testing, by design, \\nthe scenario -relevant cells.  So the mean values  of each matrix  are inserted in to the ‘frozen’ cells of each matrix just as in the \\none-sample test, but then those cells are ignored  thereafter, i.e.  when calculating cell -level p-values, as well as the (restricted) \\nmatrix-level p-values, just as in the one -sample test .  This ensures that the two-sample test is conducted only for the \\nscenario -relevant cells , even as we properly perturb each entire matrix  when generating the samples . \\n \\n55 See Opdyke (2024 a) for extensive additional examples under wide ranging data conditions.  \\n \\nJD Opdyke, Chief Analytics Officer                  Page 65 of 88                      Beating the Correlation Breakdown  \\n under elliptical data (see McNeil et al., 2005),56 and approximately valid under some broader class es of \\ndistributions (see Hansen & Luo (2024) and Hamed (2011) for examples) . \\n[A’] =  \\n \\nUNRESTRICTED CASE : Given a specified or well -estimated dependence  matrix [A], and its specified or \\nwell-estimated data generating mechanism :\\n \\nQ1. Confidence Intervals : What are the two dependence  matrices that correspond to the lower – and \\nupper–bounds of the 95% confidence interval for [A]?  What are, simultaneously, the individual 95% \\nconfidence intervals for each and every cell of [A]?  \\nQ2. Quantile Function : What is the unique dependence  matrix associated with [B], a matrix of \\ncumulative distribution function values associated with the corresponding cells of [A]?  \\nQ3. p-values: Under the null hypothesis that observed dependence  matrix [C] was sampled from the \\ndata generating mechanism of [A], what is the p -value associated with [C]?  And simultaneously, \\nwhat are the individual p -values associated with each and every cell of [C]?  \\nQ4. p-values: Under the null hypothesis that observed dependence matrix [A] and observed \\ndependence matrix [C] each were sampled from the same population, and therefore have the same \\nvalues, what is the matrix -level p-value?  And simultaneously, what are the individual p -values \\nassociated with each and every cell of the matrix?  \\nSCENARIO -RESTRICTED CASE : Under a specific scenario only selected  pairwise dependence  cells of [A] \\nwill vary (green), while the rest (red) are held constant, unaffected by the scenario (e.g. COVID).  This is \\n \\n56 See Koike et al. (2024) for a sophisticated paper defining the conditions under which Pearson’s retains the invariance \\nproperty under marginal transformations.  \\n\\nJD Opdyke, Chief Analytics Officer                  Page 66 of 88                      Beating the Correlation Breakdown  \\n matrix [D].\\n \\nQ5. Confidence Intervals : What are the two dependence  matrices that correspond to the lower – and \\nupper–bounds of the 95% confidence interval for [D] (holding constant the non -selected red cells)?  \\nWhat are, simultaneously, the individual 95% confidence intervals for only those cells of [D] that are \\nrelevant to the scenario (green)?  \\nQ6. Quantile Function : What is the unique dependence  matrix associated with [E], a matrix of \\ncumulative distribution function values associated with the corresponding  (green) cells of [D]?  \\nQ7. p-values: Under the null hypothesis that observed dependence  matrix [F] was sampled from the \\n(scenario -restricted) data generating mechanism of [D], what is the p -value associated with [F] (with  \\nred cells held constant)?  And simultaneously, what are the individual p -values associated with \\nevery (non -constant, green) cell of [F]?  \\nQ8. p-values: Under the null hypothesis that observed dependence matrix [D] and observed \\ndependence matrix [F] each were sampled from the same population, and therefore have the same \\nvalues (for their unrestricted green cells), what is the matrix -level p-value (for the unrestricted \\nportion of the matrix)?  And simultaneously, what are the individual p -values associated with each \\nand every (green, unrestricted) cell of the matrix?  \\nAnswers to these questions require inference at both the cell and matrix levels, simultaneously and with \\ncross-level consistency, as well as requiring the matrix -level quantile function, all under both the \\nunrestricted and scenario -restricted cases, under any data conditions.  Only NAbC can simultaneously \\nanswer Q1.-Q8. above under general data conditions , as shown below in Tables B1 and B2.  \\nFor Q1 and Q 5, the two top matrices correspond to the first (matrix -level) question , and the bottom two \\nmatrices correspond to the second (cell -level) question.  Note the wider intervals on a cell -by-cell basis \\nfor the matrix -level confidence intervals compared to the cell -level confidence intervals, as expected.  \\nAlso note, for Q3 and Q 7, the smaller p -values for the individual cells compared to the respective matrix -\\nlevel p-values, which are larger, as expected, as they are analogous to family-wise error rate (FWER)  of a \\njoint hypothesis covering all cells of the matrix .  Note also that  the green cells of Q6 differ from the \\ncorresponding cells in Q2: even though the (green) angles distributions themselves remain unaffected by \\nscenario restrictions, the ultimate correlation values of those cells ARE affected due to the matrix \\nmultiplication of the Cholesky factor, \\nTR BB= .  Comparing the two -sample test of Q4 to the one -sample \\ntest of Q3, we find, as expected, the increased variability from two samples increases all the cell -level p- \\nvalues, as well as the matrix -level p-value.  Only when we double the sample size (as well as the number  \\n\\nTABLE B1: NAbC Provides Complete Inference , Example of  Kendall’s Tau – Cell and Matrix Level p -values and Confidence Intervals  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nPage 67 of 88 JD Opdyke, Chief Analytics Officer  Beating the Correlation Breakdown  \\n1\\n0.2735 1\\n0.0910 0.3475 1\\n0.3323 0.0127 0.1182 1\\n0.5250 0.4370 0.2335 0.2789 1\\n1\\n0.0250 1\\n-0.1661 0.0986 1\\n0.0926 -0.3131 -0.1427 1\\n0.3210 0.1410 -0.1396 -0.0478 1\\n1\\n0.2300 1\\n0.0424 0.3013 1\\n0.2920 -0.0525 0.0570 1\\n0.4929 0.3849 0.1611 0.2103 1\\n0.0006\\n0.0090 0.0218\\n0.0222 0.0315 0.0269\\n0.0170 0.0157 0.0088 0.0077\\n1\\n0.1729 1\\n-0.0355 0.2369 1\\n0.2374 -0.1510 -0.0392 1\\n0.4335 0.3159 0.0614 0.1040 1\\n1\\n0.0042 1\\n0.0078 0.0072 1\\n0.0056 0.0319 0.0263 1\\n0.0282 0.0110 0.0008 0.0131 1p-value=0.1503  p-value=0. 5370  \\np-value=0. 1285  \\n(n=320, N=50k)  \\n1\\n0.0213 1\\n0.0342 0.0723 1\\n0.0593 0.1350 0.1148 1\\n0.1194 0.0922 0.0043 0.0789 1\\nTABLE B2: NAbC Provides Complete Inference , Example of  Kendall’s Tau – Cell and Matrix Level p -values and Confidence Intervals  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nPage 68 of 88 JD Opdyke, Chief Analytics Officer  Beating the Correlation Breakdown  p-value=0. 0436  p-value=0. 4436  \\np-value=0. 2251  \\n(n=320, N=50k)  \\n1\\n0.1282  1\\n-0.0636 0.0478 1\\n0.1942 -0.1940 -0.0639 1\\n0.4098 0.1757 -0.1144 -0.0541 1\\n1\\n0.1282  1\\n-0.0636 0.3492 1\\n0.1942 -0.1940 -0.0639 1\\n0.4098 0.3425 0.1150 0.1841 1\\n1\\n0.1282  1\\n-0.0636 0.0904 1\\n0.1942 -0.1940 -0.0639 1\\n0.4098 0.2028 -0.0789 -0.0184 1\\n1\\n0.1282  1\\n-0.0636 0.3006 1\\n0.1942 -0.1940 -0.0639 1\\n0.4098 0.3165 0.0809 0.1486 1\\n1\\n0.1282  1\\n-0.0636 0.2398 1\\n0.1942 -0.1940 -0.0639 1\\n0.4098 0.2895 0.0362 0.0867 1\\n0.0047\\n0.0077 0.0148 0.0171\\n0.0328\\n0.3992 0.0290 0.0140\\n0.0017\\n0.2231 0.0008 0.0001\\nJD Opdyke, Chief Analytics Officer                  Page 69 of 88                      Beating the Correlation Breakdown  \\n of simulations to cautiously and accurately account for smaller p -values) do we obtain similarly small p -\\nvalues of the same order of magnitude.  Similar patterns hold for the one -sample vs two -sample test \\nresults under the scenario -restricted cases (Q7 and Q8, respectively).  Finally, note that the empirical \\nvalues of the red cells in  Q5-Q6 differ slightly from those in [D] and [F].  This is due to NAbC’s \\nconservative use of the mean of the estimated angles (correlation) matrices, rather than presuming we \\nknow the absolute ‘true’ values of these cells (although this is justified in some specific cases).  \\nIn terms of actual runtimes, note that NAbC is somewhat  computationally intensive, but not prohibitively \\nso.  Implementing NAbC on synthetic data representing real -world data conditions (e.g. margins with \\ndifferent and varying degrees of asymmetry, (non)stationarity, serial correlation, and heavy -tailedness) \\nfor non-small portfolios of dimension 100x100, o n a six-year old commodity laptop with RAM=32GB but \\nno multi-threading, NAbC generates a full set of results , based on N samples = 10,000, in about 2.4 \\nhours.  However, in a multithreaded environment, let alone one with more memory, NAbC could be \\napplied on similarly non -small matrices in minutes.  For the specific case of the gaussian identity matrix, \\napplying inverse probabili ty transform sampling as described above , on a 100x100 matrix with N samples \\n= 10,000, NAbC takes less than  25 minutes to run on the same laptop .  Notably, Roman (2023)  \\nbenchmarked NAbC’s sampling under the gaussian identity matrix against that of Makalic & Schmidt \\n(2018) and obtained up to a 30% reduction in runtime under NAbC .  But of course, NAbC’s analytic \\nsolution under these conditions is instantaneous  (see url for excel workbook above) .  So while NAbC is \\nnot a result  that currently can be used “real -time” for, say, high frequency trading  (except for when the \\nfully analytic  solution is valid ), its runtimes remain reasonable given its very generalized application and \\nwidely available modern computing resources.  \\n \\nNAbC: Beyond ‘Distance’ — LNP, A Generalized Entropy  \\n \\nIn a relevant and validating digression, it is intriguing and important to note that the (two -sided) cell -level \\np-values NAbC provides (see Q3 and Q7 in Table B above) actually can be used to construct a competitor \\nto commonly used distance metrics, such a s norms, and it has a number of advantages over them in this \\nsetting.  Some commonly used norms for measuring correlation ‘distances’ include the Taxi, \\nFrobenius/Euclidean, and Chebyshev norms (collectively, the Minkowski norm), shown below in (40).  \\n         where x is a distance from a presumed or baseline correlation value,  \\n(40)                 d=number of observations, and m=1, 2, and  \\n\\uf0a5correspond to the  \\n                 Taxi, Frobenius/Euclidean, and Chebyshev norms, respectively.  \\nAll of these norms measure absolute distance from a presumed or baseline correlation /dependence  \\nvalue.  But the range of all relevant and widely used dependence measures is bounded, either from –1 to \\n1 or 0 to 1, and the relative impact and meaning of a given distance at the boundaries are not the same as \\nthose in the middle of the range.  In other words, a shift of 0.02 from an original or presumed \\ncorrelation /dependence  value of, say, 0.97, means something very different than the same shift from \\n1\\n1mdm\\ni\\nixx\\n=\\uf0e6\\uf0f6=\\uf0e7\\uf0f7\\uf0e8\\uf0f8\\uf0e5\\nJD Opdyke, Chief Analytics Officer                  Page 70 of 88                      Beating the Correlation Breakdown  \\n 0.37.  NAbC’s p -values attribute probabilistic MEANING to these two different cases, while a norm would \\ntreat them identically, even though they very likely indicate what are very different events of very different \\nrelative magnitudes with potentially very di fferent consequences.  \\nTherefore, a natural, PROBABILISTIC distance measure based directly on NAbC’s cell -level p-values is \\nthe natural log of the product of the p -values, dubbed ‘LNP’ in (41) below:  \\n(41) \\n\\uf05b\\uf05d ()\\n11\"LNP\" ln - = ln -  where 1 2qq\\nii\\niip value p value q p p\\n==\\uf0e6\\uf0f6= = −\\uf0e7\\uf0f7\\uf0e8\\uf0f8\\uf0d5\\uf0e5  and \\n-i p value is 2-sided.  \\nUsing a Pearson’s correlation matrix under the (Gaussian) identity matrix, LNP shows a  very strong \\ncorrespondence with the entropy of the correlation matrix, defined by Felippe et al. (2021 and 2023) as \\n(42) below:  \\n(42) \\n()()\\n1Entropy lnp\\njj\\njEnt R p \\uf06c\\uf06c\\n== =− \\uf0e5  \\nwhere R is the sample correlation matrix and \\nj\\uf06c are the p eigenvalues of the correlation matrix after it is \\nscaled by its dimension, R/p.  Importantly, this result (42), like NAbC, is valid for ANY positive definite \\nmeasure of dependence, not just Pearson’s.  Graph 18 below compares LNP  of Kendall’s Tau matrix  to the \\nentropy of Kendall’s Tau matrix in 10,000 simulations  (with n=126 for half a year of daily returns)  under the \\nGaussian identity matrix , and the Pearson’s correlation between them (0.98) is virtually identical to the \\nsame comparison based on Pearson’s rather than Kendall’s  (just under  0.99). 57 \\nIt is important to note, however, that entropy here is limited to being calculated relative to the case of \\nindependence, which for many dependence measures corresponds only with the identity matrix.58  In \\ncontrast, LNP can be calculated, and retains its meaning, in all cases, based on ANY values of the \\ndependence matrix, not just the case of independence.  Yet the correspondence of LNP to entropy under \\nthis specific case speaks to LNP’s natural interpr etation as a meaningful measure of deviation/ distance/ \\ndisorder (depending on your interpretation), and one that also is more flexible and granular than entropy \\nas it is measured cell -by-cell, p(p-1)/2 times, as opposed to only p times for p eigenvalues.  As such, LNP \\nmight be considered a type of ‘generalized entropy’ relative to any baseline of the dependence measure, \\nas specified by the researcher, that is not necessarily perfect (in)dependence.  Such entropy -related \\nmeasures certainly are relevant in t his setting as entropy has been used increasingly in the literature to \\nmeasure, monitor, and analyze financial markets (see Meucci, 2010b, Almog and Shmueli, 2019,  \\n \\n57 In addition, the Pearson’s correlation between LNP and the entropy of Felippe et al. (2021 and 2023) , under these conditions \\nof the Gaussian identity matrix,  was the same – 0.98 – for both Spearman’s and Chatterjee’s.  \\n \\n58 Recall, of course, that a zero value for Pearson’s or Kendall’s or Spearman’s does not imply independence, but \\nindependence does imply a zero value for these measures.  \\nJD Opdyke, Chief Analytics Officer                  Page 71 of 88                      Beating the Correlation Breakdown  \\n Graph 18: Identity Matrix Simulations  for Kendall’s Tau  – LNP v Matrix Entropy \\n  \\nChakraborti et al., 2020, and Vorobets , 2024, 202 5, for several examples).   So the use of LNP here \\nwarrants further investigation as a matrix -level measure that, unlike widely used distance measures such \\nas norms, has a solid and meaningful probabilistic foundation.  Its calculation applies not only beyond \\nthe independence case gene rally, but also to ALL positive definite measures of dependence, regardless \\nof their values.  LNP’s range of application is as wide as that of NAbC’s matrix -level p-value, and the two \\nare readily calculated side -by-side as they are both based on NAbC’s cel l-level (two -sided) p-values for \\nthe entire matrix.  These are intriguing results with possibly far -reaching implications.  \\n \\nNAbC: Future Research  and Applications  \\n \\nThere are a number of areas where additional  research can further validate and  potentially  increase the \\nutility and/or breadth  of application of NAbC.   \\nAnalytic Angles Distributions : I provide above the derivation of NAbC’s fully analytic solution under the \\nGaussian identity matrix, but this is a narrow (albeit foundational) case.  Although NAbC ’s general \\nsolution remains ‘runtime reasonable’ for its purposes and generality, e xpanding the range of conditions \\nfor an analytic solution for the angles distributions would dramatically speed NAbC’s implementation.  \\nDeriving an “all cases” analytic solution currently appears to be a nontrivial problem, but even providing \\nthis under additional,  specific cases would be useful and directly useable in NAbC’s application . \\n\\nJD Opdyke, Chief Analytics Officer                  Page 72 of 88                      Beating the Correlation Breakdown  \\n Competing Distributional Methods : Implementing and comparing NAbC’s results to those of competing, \\nif less flexible methods, like Hansen & Archakov (2021) and the Bayesian approaches of Lan et al. (2020) \\nand Ghosh et al. (2020) , likely would be useful and insightful exercise s, especially if the focus is on power \\nstudies and tests of robustness under common dependence structures in finance (e.g. spiked covariance \\nmatrices and otherwise near-singular matrices).   The same goes for the two -sample case, where NA bC \\ncompares two sample matrices against the null hypothesis of no difference between them: comparing \\nNAbC’s results against those from some of the purportedly more generalizable competitors, like Ding et \\nal. (2023) (after covariances are converted to matrices of Pearson’s correlations)  and Wang et al. (2025) , \\nwould add to our knowledgebase.  \\nStatistical Process Control : A full implementation of NAbC within a statistical process control (SPC) \\nmonitoring framework would be useful to compare against potential competitors like  those of Adegoke et \\nal. (2022), Ajadi et al. (2021), Bours & Steland (2020), Wang et al. (2019), Choi and Shin (2021), and those \\nreviewed in Ebadi et al. (2021) .  While a major focus should be on power-related metrics like average run \\nlength, special scrutiny should be placed on robustness and the (nonparametric ) generalizabi lity of \\nNAbC vs these competitors, since these characteristics arguably are areas of weakness in the SPC \\nliterature , and where NAbC might make its most meaningful contributions . \\nRecovering DAGs : Causal model frameworks typically are defined, in part or in whole, by directed acyclic \\ngraphs (DAGs), and the recovery of the ‘ground truth’ DAG, assuming it is rightly specified,59 is one of \\ncausal modeling’s fundamental tasks.60  While NAbC obviously is not designed to provide “all else equal” \\nestimates of the magnitudes of treatment effects that regression approaches within causal frameworks \\ncan provide (see MacKinnon & Lamp, 2022), it may be able to enhance covariate classifica tion efforts for \\naccurate DAG recovery.  For example, when using a directional dependence measure, say, Chatterjee’s \\nimproved correlation (see Xia et al., 2024), we can apply NAbC twice, once with the treatment variable (X) \\nand dependent variable (Y) and r elevant covariates (V1, V2, V3) in one order in the matrix (e.g. X, V1, V2, \\nV3, Y), and once in the reverse order in the matrix (Y, V3, V2, V1, X).  The two resulting matrices will \\ntogether capture all potential associations, in both directions, of all the  variables.  And all the cells of the \\ntwo estimated dependence matrices will fully map to the relevant causal categories that make up a DAG \\n(e.g. the confounders, colliders, mediators, independent variables, causes of X, consequences of X, \\ncauses of Y, and  consequences of Y).   \\n \\n59 “The correct causal model is an exacting qualification,  requiring a program of research with precise definition of causal \\neffects, specification of assumptions, and sensitivity analysis for how violating  assumptions affects results.  Statistical \\nanalysis is useful for demonstrating associations between variables that are consistent or inconsistent with a causal model. ” \\n(MacKinnon & Lamp, 2022).  \\n \\n60 Note that Czado (2025) demonstrates that vine copulas, described above as being a very flexible and effective method for \\nestimating  dependence structure under real -world conditions (if not for inference  regarding all -pairwise matrices ), also can be \\nremarkably effective in the causal discovery setting.   See also the innovative causal modeling approaches of Rodriguez \\nDominguez & Yadav  (2024), and Rodriguez Dominguez  (2024). \\n \\nJD Opdyke, Chief Analytics Officer                  Page 73 of 88                      Beating the Correlation Breakdown  \\n What NAbC could provide here is two things: first, p -values associated with each of these categories, for \\neach variable, to assist in their classification.  Unlike an approach that would analyze each variable pair \\nseparately, NAbC’s p -values would properly  take into account the entire dependence matrix, with all the \\npairwise relationships, simultaneously.  Secondly, many asymmetric dependence measures are not \\nreadily useable within regression frameworks, even when such frameworks are appropriately direction al \\n(see MacKinnon & Lamp, 2022 ; however, see Marqui et al., 2024, for an exception ).  For example, I am not \\naware of any regression, directional or otherwise, that allows for the use of Zhang’s (2023) combined \\ncorrelation or the asymmetric tail dependence measure of Deidda et al, (2023 ) when estimating \\n(directional) covariate effects.61  Yet these directional dependence measures may have more power \\nunder certain data conditions for identifying, and thus classifying, these relationships, and thus, when \\nused alongside causal models, could enhance their power for accurate DAG recovery.  To reemphasize, \\nthis is not a proposal for a standalone causal model , but rather, a possible way that NAbC could be used \\nto augment the accurate DAG recovery provided by an existing causal model framework.  But of course, \\nthis begs the bigger question of w hether DAGs can be used reliably within “self -referencing open systems \\nlike capital markets”  to begin with  (Polakow et al., 2023) .  Importantly, m any express strong caution , \\nbased on recent and rigorous research,  regarding  its application in this setting (see de Lara, 2023; Gong \\net al., 2024).62  I propose only that NAbC can play an effective  role here if the answer to this question \\nturns out to be  “yes” or “under some conditions.”  \\n \\nConclusions  \\n \\nNAbC defines the finite sample distributions of an extremely broad range of dependence measures – all \\nthose whose pairwise matrices are positive definite – under challenging, real -world financial data \\nconditions.  This enables robust inference and ceteris paribus analyses where none before were possible.  \\nMotivation for its development has been the need for a method that satisfies all eight of the objectives \\n \\n61 However, Andu et al. (2021) take a very interesting approach using adaptive elastic net regression wherein Szekely’s ( 2007) \\ndistance correlation is used to weight parameter estimates in the L1 penalty term of the regression.   What’s more , Pascual -\\nMarqui et al. (2024) combine  their multivariate distance -based Chatterjee correlation  with the regression approach of \\nBlömbaum et al. (2019)  to extend and robustify association -based results to causal results, thus supporting the utility of using \\nsuch measures in the causal modeling setting.  \\n \\n62 From Polakow et al. (2023): “The clarion call for causal reduction in the study of capital markets is intensifying.  However, in \\nself-referencing and open systems such as capital markets, the idea of unidirectional causation (if applicable) may be limiting \\nat best, and unstable or fa llacious at worst.” Polakow et al.  (2023).  From Gong et al. (2024): “… potential outcomes (PO) and \\nstructural causal models (SCMs) stand as the predominant frameworks.  However, these frameworks face notable challenges \\nin practically modeling counterfactuals … we identify an inherent model capacity limitation, termed as th e ‘degenerative \\ncounterfactual problem ’, emerging from the consistency rule that is the cornerstone of both frameworks. ”  And from De Lara \\n(2024): “Most of the literature on causality considers the structural framework of Pearl and the potential -outcomes framework \\nof Neyman and Rubin to be formally equivalent, and therefore interchangeably uses the do -notation and the potential -\\noutcome subscript nota tion to write counterfactual outcomes.  In this paper, we … prove that structural counterfactual \\noutcomes and potential outcomes do not coincide in general – not even in law.”   See Opdyke (2024b) for a more complete \\nreview of this literature.  \\nJD Opdyke, Chief Analytics Officer                  Page 74 of 88                      Beating the Correlation Breakdown  \\n listed below, because to date, no extant method  has addressed all of these “real -world necessary” \\nrequirements simultaneously.  Yet anything less than this, when modeling dependence structure in our \\nrisk and investment portfolios, fails to rise to the same level of analytical rigor as has been applied to t he \\nother parameters of these models: that is indefensible given that , as recognized in the literature,  its \\neffects can be larger than many, if not all of the other parameters combined.  I list again the eight \\nobjectives below for the reader’s convenience:  \\n1. NAbC remains valid under challenging, real -world data conditions, with marginal asset distributions \\ncharacterized by notably  different and  varying degrees of serial correlation, (non-)stationarity, heavy -\\ntailedness, and asymmetry  \\n2. NAbC can be applied to ANY positive definite dependence measure  \\n3. NAbC remains “estimator agnostic,” that is, valid regardless of the sample -based estimator used to \\nestimate any of the above -mentioned dependence measures  \\n4. NAbC provides valid confidence intervals and p -values at both the matrix  level and the pairwise cell  \\nlevel, with analytic consistency between these two levels (i .e. the confidence intervals for all the cells \\ndefine that of the entire matrix, and the same is true for the p -values; this effectively facilitates , and in \\nmany cases makes possible,  attribution analyses)  \\n5. NAbC provides valid confidence intervals and p -values not only for  one-sample tests against matrices \\nof fixed, assumed ‘true’ values, but also for two-sample tests comparing two matrices, so that we can \\nassess inferentially whether dependence structures truly are different across different sectors or \\nsegments of our business es. \\n6. NAbC provides a one -to-one quantile function, translating a matrix of all the cells’ cdf values to a \\n(unique) correlation /dependence measure matrix, and back again, enabling precision in reverse \\nscenarios and stress testing  \\n7. all the above results remain valid even when selected cells in the matrix are ‘frozen’ for a given scenario \\nor stress test  – that is, unaffected by the scenario – thus enabling flexible, granular, and realistic \\nscenarios  \\n8. NAbC remains valid not just asymptotically, i .e. for sample sizes presumed to be infinitely large, but \\nrather, for the specific sample sizes we have in reality  (for full-rank matrices with n>p) , enabling reliable \\napplication in actual, real-world, non-textbook settings  \\nFor the fundamental but narrow  case of Pearson’s correlation under the Gaussian identity matrix,  I derive \\nNAbC’s fully analytic solution, with p -values and confidence intervals at both the cell and matrix levels \\n(along with a measure of generalized entropy) , provided in an interactive spreadsheet . \\nhttp://www.datamineit.com/JD%20Opdyke --The%20Correlation%20Matrix -\\nAnalytically%20Derived%20Inference%20Under%20the%20Gaussian%20Identity%20Matrix --02-18-\\n24.xlsx  \\nJD Opdyke, Chief Analytics Officer                  Page 75 of 88                      Beating the Correlation Breakdown  \\n But way beyond Pearson’s, the fully general NAbC solution presented herein checks all of the eight \\nobjectives boxes above, simultaneously.  The list of critically import ant, applied research that NAbC now \\nfacilitates, if not makes possible, is not only expansive, but also feasible with an ease of use and \\ninterpretability, broad range of application, scalability, and robustness not found in other more limited \\n(spectral) me thods with narrow ranges of application.  It even appears that NAbC can increase the po wer \\nof causal models when recovering DAGs  under some conditions , further expanding its already \\ncomprehensive scope.  \\nWith NAbC, we now have a powerful, applied research tool  enabling the treatment an extremely broad \\nclass of ubiquitous  dependence measures with the same level of analytical rigor as the other major \\nparameters in our financial portfolio models.  We can use NAbC in frameworks that identify, \\nprobabilistically measure and monitor, and even anticipate critically important even ts, such as \\ncorrelation breakdowns, and mitigate and manage their effects.  Correlation breakdowns are widely \\ndocumented, arguably endemic  characteristics of major financial markets, and their destructive \\npotential on our attempts to estimate and forecast market behavior is difficult to overestimate.  Modeling \\nefforts in this area simply cannot be effective without knowledge of, and the ability to implement and \\nutilize, the true sampling distributions of the relevant dependence measures under real world conditions.  \\nIn providing exactly these distributions, in a useable, transparent, and straightforward way, NAbC should \\nprove to be a very use ful means by which we can better understand, predict, and manage portfolios in \\nour multivariate world.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nJD Opdyke, Chief Analytics Officer                  Page 76 of 88                      Beating the Correlation Breakdown  \\n References  \\nAbul-Magd, A., Akemann, G., and Vivo, P., (2009), “Superstatistical Generalizations of Wishart -Laguerre \\nEnsembles of Random Matrices,” Journal of Physics A Mathematical and Theoretical , 42(17):175207.  \\nAdams, R., Pennington, J., Johnson, M., Smith, J, Ovadia, Y., Patton, B., Saunderson, J., (2018), \\n“Estimating the Spectral Density of Large Implicit Matrices” https://arxiv.org/abs/1802.03451 . \\nAdegoke, N., Ajadi, J., Mukherjee, A., and Abbasi, S., (2022), “Nonparametric Multivariate Covariance \\nChart for Monitoring Individual Observations,”  Computers & Industrial Engineering , Vol 167.  \\nAghaKouchak, A., Easterling, D.,Hsu, K., Schubert, S., and Sorooshian, S., eds, (2013), Extremes in a \\nChanging Climate: Detection, Analysis and Uncertainty , Ch. 6: Methods of Tail Dependence Estimation \\n(pp.163-179), Springer Nature, Part of the book series: Water Science and Technology Library (WSTL, \\nvolume 65) . \\nAjadi, j., Wong, A., Mahmood, T., and Hung, K., (2021), “A new multivariate CUSUM chart for monitoring of \\ncovariance matrix with individual observations under estimated parameter,” Quality and Reliability \\nEngineering International , 38(2), 834 -847.  \\nAkemann, G., Fischmann, J., and Vivo, P., (2009), “Universal Correlations and Power -Law Tails in Financial \\nCovariance Matrices,” https://arxiv.org/abs/0906.5249 . \\nAlmog, A., and Shmueli, E., (2019), “Structural Entropy: Monitoring Correlation -Based Networks over time \\nWith Application to Financial Markets,” Scientific Reports , 9:10832.  \\nAlpay, D., and Mayats -Alpay, L., (2023), “Similary Metrics, Metrics, and Conditionally Negative Definite \\nFunctions,” arXiv:2307.10446v1 [math.FA].  \\nAndu, Y., Lee, M., and Algamal, Z., (2021), “Adaptive Elastic Net with Distance Correlation on the Group \\nEffect and Robust of High Dimensional Stock Market Price,” Sains Malaysiana , 50(9), 2755 -2764.  \\nArchakov, I.  and Hansen, P., (2021), “A New Parametrization of Correlation Matrices,” Econometrica , \\n89(4), 1699 -1715.  \\nAznar, D., (2023), “ Portfolio Management: A Deep  Distributional RL Approach ,” University of Barcelona, \\nThesis.  \\nBabić, S., Ley, C., Ricci, L., and Veredas, D., (2023), “TailCoR: A new and simple metric for tail \\ncorrelations that disentangles the linear and nonlinear dependencies that cause extreme comovements.” \\nPLoS ONE , 18(1): e0278599  \\nBank for International Settlements  (BIS), (2011a), Basel Committee on Banking Supervision, Working \\nPaper 19, (1/31/11), “Messages from the academic literature on risk measurement for the trading book.”  \\nBank for International Settlements  (BIS), (2011b) , Basel Committee on Banking Supervision, “ Operational \\nRisk – Supervisory Guidelines for the Advanced Measurement Approaches,” June, 2011.  \\nJD Opdyke, Chief Analytics Officer                  Page 77 of 88                      Beating the Correlation Breakdown  \\n Barber, R., and Kolar, M., (2018), “ROCKET: Robust Confidence Intervals via Kendall’s Tau for \\nTranselliptical Graphical Models,” The Annals of Statistics , 46(6B), 3422 -3450.  \\nBlömbaum, P., Janzing, D., Washio, T., Shimizu, S., and Schölkopf, B., (2019), “Analysis of Cause -Effect \\nInference by Comparing Regression Errors,” Peer Journal of Computational Science , 5:e169. \\ndoi:  10.7717/peerj -cs.169. \\nBlomqvist, N. (1950) \"On a Measure of Dependence between Two Random Variables\", Annals of \\nMathematical Statistics , 21(4): 593 -600.  \\nBongiorno , C., Challet, D., and Loeper, G., (2023), “ Filtering time -dependent covariance matrices using \\ntime-independent eigenvalues ,” Journal of Statistical Mechanics: Theory and Experiment , Vol. 2023 . \\nBongiorno , C., and Challet, D., (2023 a), “Covariance Matrix Filtering and Portfolio Optimization: The \\nAverage Oracle vs Non -linear Shrinkage and All the Variants of DCC -NLS”, arXiv:2309.17219v1 [q -fin.ST].  \\nBongiorno , C., and Challet, D., (2023b), “Non -linear Shrinkage of the Price Return Covariance Matrix is \\nFar from Optimal for Portfolio Optimization,” Finance Research Letters , Vol. 52, 103383.  \\nBouchaud, J, & Potters, M., (2015), “Financial applications of random matrix theory: a short review,” The \\nOxford Handbook of Random Matrix Theory , Eds G. Akemann, J. Baik, P. Di Francesco.  \\nBouchaud, J., (2021), “Radical Complexity,” Entropy, Vol. 23.  \\nBours, M., and & Steland, A., (2020), “Large -sample approximations and change testing for high -\\ndimensional covariance matrices of multivariate linear time series and factor models,” Scandinavian \\nJournal of Statistics , 48(2), 610 -654.  \\nBurda, Z., Jurkiewicz, J., Nowak, M., Papp, G., and Zahed, I., (2004), “Free Levy Matrices and Financial \\nCorrelations,” Physica A: Statistical Mechanics and its Applications . \\nBurda, Z., Gorlich, A., and Waclaw, B., (2006), “Spectral Properties of empirical covariance matrices for \\ndata with power -law tails,” Phys. Rev., E 74 , 041129.  \\nBurda, Z., Jaroz, A., Jurkiewicz, J., Nowak, M., Papp, G., and Zahed, I., (2011), “Applying Free Random \\nVariables to Random Matrix Analysis of Financial Data Part I: A Gaussian Case,” Quantitative Finance , \\nVolume 11, Issue 7, 1103 -1124.  \\nBurda, Z., and Jarosz, A., (2022), “Cleaning large -dimensional covariance matrices for correlated \\nsamples,” Phys. Rev. E , 105, 034136.  \\nCardin, M., (2009), Multivariate Measures of Positive Definiteness,” International Journal of Contemporary \\nMathematical Sciences , 4(4), 191 -200.  \\nCiciretti, V., and Pallotta, A., (2023), “Network Risk Parity: Graph Theory -based Portfolio Construction,” \\nJournal of Asset Management , 25:136–146. \\nJD Opdyke, Chief Analytics Officer                  Page 78 of 88                      Beating the Correlation Breakdown  \\n Chakraborti, A., Hrishidev, Sharma, K., and Pharasi, H., (2020), “Phase Separation and Scaling in \\nCorrelation Structures of  Financial Markets,” Journal of Physics: Complexity , 2:015002.  \\nChatterjee, S., (2021), “A New Coefficient of Correlation,” Journal of the American Statistical Association , \\nVol 116(536), 2009 -2022.  \\nChatterjee, S., (202 4), “A Survey of Some Recent Developments in Measures of Association,” Probability \\nand Stochastic Processes , Springer Nature, Singapore.  \\nChmeilowski, P., (2014), “General Covariance, the Spectrum of Riemannium and a Stress Test \\nCalculation Formula,” Journal of Risk , 16(6), 1-17. \\nChoi, J., and Shin, D., (2021), “A self -normalization break test for correlation matrix,” Statistical Papers , \\n62(5).  \\nChurch, Christ (2012). \"The asymmetric t -copula with individual degrees of freedom\", Oxford, UK: \\nUniversity of Oxford Master Thesis, 2012.  \\nCordoba, I., Varando, G., Bielza, C., and Larranaga, P., (2018), “A fast Metropolis -Hastings method for \\ngenerating random correlation matrices,” IDEAL, pp. 117-124, part of Lec Notes in Comp Sci., Vol 11314.  \\nCota, R., (2019), “Shortfalls of theHierarchical Risk Parity,” https://www.linkedin.com/pulse/shortfalls -\\nhierarchical -risk-parity-rafael-nicolas-fermin-cota/  \\nCotton, P., (2024), “ Schur Complementary Allocation: A Unification of Hierarchical Risk Parity and \\nMinimum Variance Portfolios,” arXiv:2411.05807v1 [q -fin.PM]  \\nCzado, C., (2025), “Vine Copula Based Structural Equation Models,”  Computational Statistics and Data \\nAnalysis, pp.453-477.  \\nCzado, C., and Nagler, T., (2022), “Vine Copula Based Modeling,”  Annual Review of Statistics and Its \\nApplication , pp.453-477.  \\nDalitz, C., Arning, J., and Goebbels, S., (2024), “A Simple Bias Reduction for Chatterjee’s Correlation,” \\narXiv:2312.15496v2.  \\nDe Lara, L., (2023), “On the (in)compatibility between potential outcomes and structural causal models \\nand its signification in counterfactual inference,” arXiv:2309.05997v3 [math.ST].  \\nDeidda, C., Engelke, S., and De Michele, C., (2023), “Asymmetric Dependence in Hydrological Extremes,” \\nWater Resources Research , Vol. 59, Issue 12.  \\ndu Plessis, H., and van Rensburg, P., (2020), “Risk -based Portfolio Sensitivity to Covariance Estimation,” \\nInvestment Analysts Journal , 49(3), 243 -268.  \\nEbadi, M., Chenouri, S., Lin, D., and Steiner, S., (2021), “Statistical monitoring of the covariance matrix in \\nmultivariate processes: A literature review,” Journal of Quality Technology , 54(3), 269 -289.  \\nJD Opdyke, Chief Analytics Officer                  Page 79 of 88                      Beating the Correlation Breakdown  \\n Embrechts, P., Hofert, M., and Wang, R., (2016), “Bernoulli and Tail -Dependence Compatibility,” The \\nAnnals of Applied Probability , Vol. 26(3), 1636 -1658.  \\nEngle, R., Ledoit, O., and Wolf , M., (2019), “Large dynamic covariance matrices ,” Journal of Business &  \\nEconomic Statistics , 37(2):363 –375. \\nEpozdemir, M., (2021), “ Reverse Stress Testing: A critical assessment tool for risk managers and \\nregulators ,” S&P Global, BLOG — Aug 10, 2021 , https://www.spglobal.com/market -intelligence/en/news -\\ninsights/research/reverse -stress-testing-assessment -tool-risk-managers -regulators  \\nEuropean Banking Authority, (2013), REGULATION (EU) No 575/2013 OF THE EUROPEAN PARLIAMENT \\nAND OF THE COUNCIL , Capital Requirements Regulation (CRR) , Articles 375(1), 376(3)(b), 377.  \\nhttps://www.eba.europa.eu/regulation -and-policy/single -rulebook/interactive -single-rulebook/12674  \\nFang, Q., Jiang, Q., and Qiao, X., (2024), “Large -Scale Multiple Testing of Cross -Covariance Functions \\nwith Applications to Functional Network,” arXiv:2407.19399v1 [math.ST] 28 Jul.  \\nFelippe, H., Viol, A., de Araujo, D. B., da Luz, M. G. E., Palhano -Fontes, F., Onias, H., Raposo, E. P., and \\nViswanathan, G. M., (2021), “The von Neumann entropy for the Pearson correlation matrix: A test of the \\nentropic brain hypothesis,” working paper, a rXiv:2106.05379v1  \\nFelippe, H., Viol, A., de Araujo, D. B., da Luz, M. G. E., Palhano -Fontes, F., Onias, H., Raposo, E. P., and \\nViswanathan, G. M., (2023), “Threshold -free estimation of entropy from a Pearson matrix,” working paper, \\narXiv:2106.05379v2.  \\nFeng, C., and Zeng, X., (2022), “The Portfolio Diversification Effect of Catastrophe Bonds and the Impact \\nof COVID -19,” working paper , https://ssrn.com/abstract=4215258  \\nFernandez -Duran, J.J., and Gregorio -Dominguez, M.M., (2023), “Testing the Regular Variation Model for \\nMultivariate Extremes with Flexible Circular and Spherical Distributions,” arXiv:2309.04948v2.  \\nFisher, R. A., (1915), “Frequency distribution of the values of the correlation coefficient in samples from \\nan indefinitely large population,” Biometrika , 10, 507-521.  \\nFisher, R. A., (1921), “On the ‘probable error’ of a coefficient of correlation deduced from a small sample,” \\nMetron, 1(4), 1-32. \\nFisher, R. A., (1928), “The General Sampling Distribution of the Multiple Correlation Coefficient,” \\nRothamsted Experimental Station, Harpenden, Herts.  \\nFranca, W., and Menegatto, V., (2022), “Positive definite functions on products of metric spaces by \\nintegral transforms,” Journal of Mathematical Analysis and Applications , 514(1).  \\nGaleeva, R., Hoogland, J., & Eydeland, A., (2007), “Measuring Correlation Risk,” publicly available \\nmanuscript.  \\nJD Opdyke, Chief Analytics Officer                  Page 80 of 88                      Beating the Correlation Breakdown  \\n Gao, M., Li, Q., (2024), “A Family of Chatterjee’s Correlation Coefficients and Their Properties,” \\narXiv:2403.17670v1 [stat.ME]  \\nGebelein, H. (1941), “Das statistische problem der korrelation als variations -und eigenwertproblem und \\nsein zusammenhang mit der ausgleichsrechnung,” ZAMM-Journal of Applied Mathematics and \\nMechanics/Zeitschrift fur Angewandte Mathematik und Mechanik , 21 (6), 364 –379.  \\nGenest, C., Neslehova, J., and Ghorbal, N., (2010), “Spearman’s footrule and Gini’s gamma: a review with \\ncomplements,” Journal of Nonparametric Statistics , 22(8), 937 -954.  \\nGhosh, R., Mallick, B., and Pourahmadi, M., (2021) “Bayesian Estimation of Correlation Matrices of \\nLongitudinal Data,” Bayesian Analysis , 16, Number 3, pp. 1039 –1058.  \\nGini, C., (1914), “L’Ammontare e la Composizione della Ricchezza delle Nazione,” Torino: Bocca.  \\nGolts, M., and Jones, G., “A Sharper Angle on Optimization,” ssrn.com, 1483412.  \\nGong, H., Lu, C., and Zang, Y., (2024), “Distribution -consistency Structural Causal Models” \\narXiv:2401.15911v2 [cs.AI]  \\nGreenspan, A., (1999), “New Challenges for Monetary Policy,” Remarks before a symposium sponsored \\nby the Federal Reserve Federal Reserve Bank of Kansas City , Jackson Hole, Wyoming , August 27, 1999.  \\nGreiner, R. (1909), ‘Über das fehlersystem kollektivmaßlehre’,  Zeitschrift für Mathematik und \\nPhysik  57, 121–158,225–260,337–373.  \\nGretton, A., Fukumizu, K., Teo, C., Song, L., Schölkopf, B., and Smola, A. (2007), ‘A Kernel Statistical Test \\nof Independence’, Advances in Neural Information Processing Systems , 20.  \\nGrothe, O., Schnieders, J., and Segers, J., (2014), “Measuring Associatoin and Depenence Between \\nRandom Vectors,” Journal of Multivariate Analysis, Vol. 123, 96 -110.  \\nGupta, A.K. and Nagar, D.K. , (2000), Matrix Variate Distribution , Hall/CRC, Boca Raton.  \\nHamed, K., (2011), “The distribution of Kendall’s tau for testing the significance of cross -correlation in \\npersistent data,” Hydrological Sciences Journal , 56:5, 841 -853.  \\nHan, F., (2021), “On extensions of rank correlation coefficients to multivariate spaces,” Bernoulli News , \\n28(2): 7-11. \\nHardin, J., Garcia, S., and Golan, D., (2013), “A method for generating realistic correlation matrices,” \\nAnnals of Applied Statistics , 7(3): 1733 -1762.  \\nHeller, R., Heller, Y., and Gorfine, M., (2013), ‘A Consistent Multivariate Test of Association Based on \\nRanks of Distances’, Biometrika , 100(2), 503 –510.  \\nHansen, P., and Luo, Y., (2024), “Robust Estimation of Realized Correlation: New Insight about Intraday \\nFluctuations in Market Betas,”  arXiv:2310.19992v1 . \\nJD Opdyke, Chief Analytics Officer                  Page 81 of 88                      Beating the Correlation Breakdown  \\n Heinen, A., and Valdesogo , A., (2022), “ The Kendall and  Spearman  rank correlations of the bivariate skew \\nnormal distribution ,” Scandinavian Journal of Statistics ,49:1669–1698.  \\nHeiny, J., and Yao, J., (2022), “Limiting Distributions fro Eigenvalues of Sample Correlation Matrices from \\nHeavy-tailed Populations,” arXiv:2003.03857v2 [math.PR].  \\nHellton, K., (2020), “Penalized Angular Regression for Personalized Predictions,” arXiv:2001.09834v2 \\n[stat.ME].  \\nHigham, N., (1988), “Computing the nearest symmetric positive semi -definite matrix,” Linear algebra and \\nits applications , 103 (1988), 103 –118.  \\nHigham, N., (2002), “Computing the nearest correlation matrix – a problem from finance,” IMA Journal of \\nNumerical Analysis , 22, 329-343.  \\nHirschfeld, H., (1935), “A connection between correlation and contingency,” Mathematical Proceedings \\nof the Cambridge Philosophical Society , 31 (4), 520 –524.  \\nHisakado, M. and Kaneko, T., (2023), “Deformation of Marchenko -Pastur distribution for the correlated \\ntime series,” arXiv:2305.12632v1.  \\nHoeffding, W. (1948). “A Non -parametric Test of Independence.” Annals of Mathematical Statistics , \\n19:546–557.  \\nHolzmann, H., and Klar, B., (2024) “Lancaster Correlation - A New Dependence Measure Linked to \\nMaximum Correlation,” arXiv:2303.17872v2 [stat.ME] . \\nJoarder, A., and Ali, M., (1992), “Distribution of the Correlation Matrix for a Class of Elliptical Models,” \\nCommunications in Statistics – Theory and Methods, 21(7), 1953 -1964.  \\nJohnstone, I., (2001), “On the distribution of the largest eigenvalue in principal components analysis,” The \\nAnnals of Statistics , 29(2): 295 –327, 2001.  \\nJondeau, E., (2016), “Asymmetry in Tail Dependence of Equity Portfolios,” Computational Statistics & \\nData Analysis , Vol 100, pp351 -368.  \\nJunker. R., Griessenberger, F., and Trutschnig, W., (2021), “Estimating scale -invariant directed \\ndependence of bivariate distributions,” Computational Statistics & Data Analysis , Volume 153.  \\nKelly, B., Malamud, S., Pourmohammadi, M., and Trojani, F., (2024), “Universal Portfolio Shrinkage,” NBER \\nWorking Paper Series, Working Paper 32004 , http://www.nber.org/papers/w32004  \\nKe, C., (2019), “A New Independence Measure and its Applications in High Dimensional Data Analysis,” \\nDoctoral Dissertation, University of Kentucky.  \\nKendall, M. (1938), \"A New Measure of Rank Correlation,\" Biometrika , 30 (1–2), 81–89. \\nJD Opdyke, Chief Analytics Officer                  Page 82 of 88                      Beating the Correlation Breakdown  \\n Kim, J., and Finger, C., (1998), “A Stress Test to Incorporate Correlation Breakdown,” Journal of Risk , 2(3), \\n5-19. \\nKim, W., and Lee, Y., (2016), “A Uniformly Distributed Random Portfolio,” Quantitative Finance , Vol. 16, \\nNo. 2, pp.297 -307.  \\nKoike, T., Lin, L., and Wang, R., (2024), “Invariant Correlation Under Marginal Transforms,” Journal of \\nMultivariate Analysis , 204, 105361.  \\nKrupskii, P., and Joe, H., (2014), “Tail -weighted Measures of Dependence,” Journal of Applied Statistics , \\n42(3), 614 -629.  \\nKubiak, S., Weyde, T., Galkin, O., Philps, D., and Gopal, R., (2024), “Denoising Diffusion Probabilistic \\nModel for Realistic Financial Correlation Matrices,” ICAIF \\'24: Proceedings of the 5th ACM International \\nConference on AI in Finance , pp. 1-9.  Also see https://github.com/szymkubiak/DDPM -for-Correlation -\\nMatrices  \\nKurowicka, D., (2014). “Joint Density of Correlations in the Correlation Matrix with Chordal Sparsity \\nPatterns,” Journal of Multivariate Analysis , 129 (C): 160 –170.  \\nLan, S., Holbrook, A., Elias, G., Fortin, N., Ombao, H., andShahbaba, B. (2020), “Flexible Bayesian \\nDynamic Modeling of Correlation and Covariance Matrices,” Bayesian Analysis , 15(4), 1199–1228.  \\nLatif, S., and Morettin, P., (2014), “Estimation of a Spearman -Type Multivariate Measure of Local \\nDependence,” International Journal of Statistics and Probability , 3(2).  \\nLauria, D., Rachev, S., and Trindade, A., (2021), “Global and Tail Dependence: A Differential Geometry \\nApproach,” arXiv:2106.05865v1 [stat.AP].  \\nLedoit, O., and Wolf , M., (2017), “ Nonlinear shrinkage of the covariance matrix for portfolio selection: \\nMarkowitz  meets goldilocks ,” The Review of Financial Studies , 30(12):4349 –4388. \\nLedoit, O., and Wolf , M., (2022a), “ Markowitz portfolios under transaction costs ,” Working paper series , \\nDepartment of  Economics, (420).  \\nLedoit, O., and Wolf , M., (2022b), “ Quadratic shrinkage for large covariance matrices ,” Bernoulli , \\n28(3):1519 –1547.  \\nLee, Y., Kim, J., Kim, W., and Fabozzi, F., (2024), “An Overview of Machine Learning for Portfolio \\nOptimization,” The Journal of Portfolio Management, 51(2), 131 -148.  \\nLewandowski, D.; Kurowicka, D.; Joe, H. (2009). \"Generating random correlation matrices based on vines \\nand extended onion method\". Journal of Multivariate Analysis , 100 (9): 1989 –2001.  \\nLi, D., Cerezetti , F., and Cheruvelil , R., (2024), “ Correlation breakdowns, spread positions and central \\ncounterparty margin models ,” Journal of Financial Market Infrastructures ,11(3), 2049-5404. \\nJD Opdyke, Chief Analytics Officer                  Page 83 of 88                      Beating the Correlation Breakdown  \\n Li, Q., (2018), “Covariance Modelling with Hypersphere Decomposition Method and Modified \\nHypersphere Decomposition Method,” Doctoral Thesis, University of Manchester.  \\nLi, G., Zhang, A., Zhang, Q., Wu, D., and Zhan, C., (2022), “ Pearson Correlation Coefficient -Based \\nPerformance Enhancement of Broad Learning System for Stock Price Prediction ,” IEEE Transactions on \\nCircuits and Systems —II: Express Briefs , Vol 69(5), 2413 -2417.  \\nLi, W. ,Yao, J., (2018), “On structure testing for component covariance matrices of a high -dimensional \\nmixture,” Journal of the Royal Statistical Society Series B (Statistical Methodology) , 80(2):293 -318.  \\nLi, X., and Joe, H., (2024), “Multivariate Directional Tail -weighted Dependence Measures,” Journal of \\nMultivariate Analysis , Vol 203.  \\nLi, Y., (2025), “Large Sample Correlation Matrices with Unbounded Spectrum,” Journal of Multivariate \\nAnalysis, 205, 105373.  \\nLin, Z., and Han, F., (2023), “On Boosting the Power of Chatterjee’s Rank Correlation,” Biometrika ,110(2), \\n283–299.  \\nLindskog, F., McNeil, A., Schmock, U. , (2003), “Kendall’s Tau for Elliptical Distributions ,” In: Bol, G., \\nNakhaeizadeh, G., Rachev, S.T., Ridder, T., Vollmer, KH. (eds) Credit Risk. Contributions to Economics , \\nPhysica-Verlag HD.  \\nLoretan, M., and English, W., (2000), “ Evaluating Correlation Breakdowns during Periods of Market \\nVolatility,” International Finance Discussion Papers (IFDP), \\nhttps://www.federalreserve.gov/econres/ifdp/evaluating -correlation -breakdowns -during-periods-of-\\nmarket-volatility.htm  and https://www.bis.org/publ/confer08k.pdf  \\nLu, F., Xue, L., and Wang, Z., (2019), “Triangular Angles Parameterization for the Correlation Matrix of \\nBivariate Longitudinal Data,” J. of the Korean Statistical Society , 49:364-388.  \\nMacKinnon, D., and Lamp, S., (2021), “A Unification of Mediator, Confounder, and Collider Effects,” Prev \\nSci., 22(8), 1185 -1193.  \\nMadar, V., (2015), “Direct Formulation to Cholesky Decomposition of a General Nonsingular Correlation \\nMatrix,” Statistics & Probability Letters , Vol 103, pp.142 -147.  \\nMakalic, E., Schmidt, D., (2018), “An efficient algorithm for sampling from sin(x)^k for generating random \\ncorrelation matrices,” arXiv: 1809.05212v2 [stat.CO].  \\nMaltsev, A., and Malysheva, S. (2024), “Eigenvalue Statistics of Elliptic Volatility Model with Power -law \\nTailed Volatility,” arXiv:2402.02133v1 [math.PR].  \\nManistre, J., (2008), “A Practical Concept of Tail Correlation,” Society of Actuaries . \\nMarchenko, A., Pastur, L., (1967), \"Distribution of eigenvalues for some sets of random matrices,” \\nMatematicheskii Sbornik , N.S. 72 (114:4): 507 –536.  \\nJD Opdyke, Chief Analytics Officer                  Page 84 of 88                      Beating the Correlation Breakdown  \\n Markowitz, H., (1952), “Portfolio Selection,” The Journal of Finance , 7, 77-91. \\nMarti, G., (2019), “CorrGAN: Sampling Realistic Financial Correlation Matrices Using Generative \\nAdversarial Networks,” arXiv:1910.09504v1 [q -fin.ST].  \\nMarti, G., Goubet, V., and Nielsen, F., (2021), “ cCorrGAN: Conditional Correlation GAN for  Learning \\nEmpirical Conditional Distributions in  the Elliptope ,” arXiv:2107.10606v1 [q -fin.ST]  \\nMarti, G. (2020) TF 2.0 DCGAN for 100×100 financial correlation matrices [Online]. Available \\nat: https://marti.ai/ml/2019/10/13/tf -dcgan-financial-correlation -matrices.html . (Accessed: 17 Aug \\n2020)  \\nMartin, C. and Mahoney, M., (2018), “Implicit Self -Regularization in Deep Neural Networks: Evidence from \\nRandom Matrix Theory and Implications for Learning,” Journal of Machine Learning Research , 22 (2021) 1 -\\n73. \\nMcNeil, A , Frey, R., and Embrechts, P. , (2015), Quantitative risk management: Concepts, techniques and \\ntools, Princeton University Press.  \\nMeckes, M., (2013), “Positive Definite Metric Spaces,”  arXiv:1012.5863v5 [math.MG] . \\nMetsämuuronen, J., (2022), “Reminder of the Directional Nature of the Product -Moment Correlation \\nCoefficient,” Academia Letters , Article 5313.  \\nMeucci, A., (2010a), “The Black -Litterman Approach: Original Model and Extensions,” The Encyclopedia \\nof Quanti tative Finance , Wiley, 2010  \\nMeucci, A., (2010b), “ Fully Flexible Views: Theory and Practice ,” arXiv:1012.2848v1  \\nMitchell, C., Ryne, R., and Hwang, K., (2022), “Using kernel -based statistical distance to study the \\ndynamics of charged particle beams in particle -based simulation codes,”  Phys. Rev. E , 106, 065302.  \\nMuirhead, R., (1982), Aspects of Multivariate Statistical Theory , Wiley Interscience, Hoboken, New Jersey.  \\nNawroth, A ., Anfuso, F . and Akesson, F ., (2014), “Correlation Breakdown and the Influence of Correlations \\non VaR,” https://ssrn.com/abstract=2425515  or http://dx.doi.org/10.2139/ssrn.2425515  \\nNg, F., Li, W., and Yu, P., (2014), “A Black -Litterman Approach to Correlation Stress Testing,” Quantitative \\nFinance, 14:9, 1643 -1649.  \\nNiu, L., Liu, X., and Zhao, J., (2020), “Robust Estimator of the Correlation Matrix with Sparse Kronecker \\nStructure for a High -Dimensional Matrix -variate,” Journal of Multivariate Analysis , 177, 104598.  \\nOpdyke, JD, (2022), “Beating the Correlation Breakdown: Robust Inference and Flexible Scenarios and \\nStress Testing for Financial Portfolios,” QuantMindsEdge: Alpha and Quant Investing: New Research: \\nApplying Machine Learning Techniques to Alpha Generation Models, June 6  (available at \\nwww.DataMineit.com) . \\nJD Opdyke, Chief Analytics Officer                  Page 85 of 88                      Beating the Correlation Breakdown  \\n Opdyke, JD, (2023), “Beating the Correlation Breakdown: Robust Inference and Flexible Scenarios and \\nStress Testing for Financial Portfolios,” Columbia University, NYC –School of Professional Studies: \\nMachine Learning for Risk Management, Invited Guest Lectu re, March 20  (available at \\nwww.DataMineit.com) . \\nOpdyke, JD, (2024 a), Keynote Presentation: “ Beating the Correlation Breakdown,  for Pearson’s and \\nBeyond: Robust Inference and Flexible Scenarios and Stress Testing for Financial Portfolios ,” \\nQuantStrats11, NYC, March 12 , 2024 (available at www.DataMineit.com) . \\nOpdyke, JD, (2024b), RoundTable Writeup: “Association -based vs Causal Research: the Hype, the \\nContrasts, and the Stronger -than-expected Complementary Overlaps,” QuantStrats11, NYC, March 12, \\n2024 (available at www.DataMineit.com) . \\nPackham, N., and Woebbeking, F., (2023), “Correlation scenarios and correlation stress testing,” Journal \\nof Economic Behavior & Organization , Vol 205, pp.55 -67. \\nPafka, S., and Kondor, I., (2004), “Estimated correlation matrices and portfolio optimization,” Physica A: \\nStatistical Mechanics and its Applications, Vol 343, 623 -634.  \\nPapenbrock, J., Schwendner, P., Jaeger, M., and Krugel, S., (2021), “Matrix Evolutions: Synthetic \\nCorrelations and Explainable Machine Learning for Constructing Robust Investment Portfolios,” Journal \\nof Financial Data Science , 51-69. \\nParlatore, C. and Philippon, T., (2024), “Designing Stress Scenarios,” NBER Working Paper No. w29901.  \\nPascual-Marqui, R., Kochi, K., and Kinoshita, T., (2024), “Distance -based Chatterjee correlation: a new \\ngeneralized robust measure of directed association for multivariate real and complex -valued data,: \\narXiv:2406.16458 [stat.ME].  \\nPearson, K., (1895), “VII. Note on regression and inheritance in the case of two parents,” Proceedings of \\nthe Royal Society of London , 58: 240–242.  \\nPham-Gia, T., Choulakian, V., (2014), “Distribution of the Sample Correlation Matrix and Applications,” \\nOpen Journal of Statistics , 4(5).  \\nPolakow, D., Gebbie, T., and Flint, E., (2023), “ Epistemic Limits of Empirical Finance: Causal \\nReductionism and  Self-Reference ,” arXiv:2311.16570v2 [q -fin.GN]  \\nPramanik, P., (2024), “Measuring Asymmetric Tails Under Copula Distributions,” European Journal of \\nStatistics , 4:7.  \\nPuccetti, G., (2022), “Measuring linear correlation between random vectors,” Information Sciences , Vol \\n607, 1328 -1347.  \\nQian, E. and Gorman, S. (2001). “Conditional Distribution in Portfolio Theory.” Financial Analysts Journal , \\n44-51. \\nJD Opdyke, Chief Analytics Officer                  Page 86 of 88                      Beating the Correlation Breakdown  \\n Qin, T., and Wei -Min, H., (2024), “ Epanechnikov Variational Autoencoder ,” arXiv:2405.12783v1 [stat.ML] \\n21 May 2024 . \\nRebonato , R., and P. Jäckel, P., (2000)  “The Most General Methodology to Create a Valid Correlation Matrix \\nfor Risk Management and Option Pricing Purposes,” Journal of Risk , 2(2), 17-27. \\nReddi, S., Ramdas, A., Poczos, B., Singh, A., and Wasserman, L., (2015), “On Decreasing Power of Kernel \\nand Distance based Nonparametric Hypothesis Tests in High Dimensions,” arXiv:1406.2083v2 [stat.ML].  \\nRodgers, J., and Nicewander, A., (1988), The American Statistician, 42(1), 59 -66. \\nRodriguez Dominguez, A., and Yadav, O., (2024), “Measuring causality with the variability of the largest \\neigenvalue,” Data Science in Finance and Economics . \\nRodriguez Dominguez, A., (2024), “ Geometric Spatial and Temporal Constraints in Dynamical Systems \\nand Their Relation to Causal Interactions between Time Series ,” \\nSSRN:  https://ssrn.com/abstract=4949383  or http://dx.doi.org/10.2139/ssrn.4949383  \\nRubsamen, Roman, (2023), “Random Correlation Matrices Generation,” \\nhttps://github.com/lequant40/random -correlation -matrices -generation  \\nSabato, S., Yom -Tov, E., Tsherniak, A., Rosset, S., (2007), “Analyzing systemlogs: A new view of what’s \\nimportant,” Proceedings, 2nd Workshop of Computing Systems ML , pp.1–7. \\nSaxena, S., Bhat, C., and Pinjari, A., (2023),  “Separation -based parameterization strategies for estimation \\nof restricted covariance matrices in multivariate model systems,” Journal of Choice Modelling , Vol. 47, \\n100411.  \\nSejdinovic, D., Sriperumbudur, B., Gretton, A., and Fukumizu, K., (2013) “Equivalence of Distance -Based \\nand RKHS -Based Statistics in Hypothesis Testing,” The Annals of Statistics , 41(5), 2263 -2291.  \\nSheppard, W. , (1899), “On the application of the theory of error to cases of normal distribution and \\nnormal correlation ,” Philosophical Transactions of the Royal Society of London (A) , 92, 101–167.  \\nSiburg, K., Strothmann, C., and Weiß, G., (2024), “Comparing and quantifying tail dependence,” \\nInsurance: Mathematics and Economics , Vol 118, 95 -103.  \\nSpearman, C., (1904), “’General Intelligence,’ Objectively Determined and Measured,” The American \\nJournal of Psychology , 15(2), 201 –292.  \\nSzekely, G., Rizzo, M., and Bakirov, N., (2007), “Measuring and Testing Dependence by Correlation of \\nDistances,” The Annals of Statistics , 35(6), pp2769 -2794.  \\nTaleb, N., (2007), The Black Swan: The Impact of the Highly Improbable , Random House, Inc, New York, \\nNew York.  \\nTaraldsen, G. (2021), “The Confidence Density for Correlation,” The Indian Journal of Statistics , 2021.  \\nJD Opdyke, Chief Analytics Officer                  Page 87 of 88                      Beating the Correlation Breakdown  \\n Thakkar, A., Patel, D., and Shah, P., (2021), “Pearson Correlation Coefficient -based performance \\nenhancement of Vanilla Neural Network for Stock Trend Prediction,” Neural Computing and Applications , \\n33:16985 -17000.  \\nTripathi, Y., Chatla, S., Chang, Y., Huang, L. , and Shiefh, G., (2022), “A Nonlinear Correlation Measure with \\nApplications to Gene Expression Data,” PLoS ONE , 17(6): e0270270.  \\nTumminello, M., Aset, T., Di Matteo, T., and Mantegna, R., (2005), “A tool for filtering information in \\ncomplex systems,” Proceedings of the National Academy of Sciences , 102(30), 10421 -10426.  \\nvan den Heuvel, E., and Zhan, Z., (2022), “Myths About Linear and Monotonic Associations: Pearson’s r, \\nSpearman’s ρ, and Kendall’s τ,” The American Statistician , 76:1, 44 -52. \\nVanni, F., Hitaj, A., and Mastrogiacomo, E., (2024), “Enhancing Portfolio Allocation: A Random Matrix \\nTheory Perspective,” Mathematics , 12(9), 1389  \\nVeleva, E., (2017), “Generation of Correlation Matrices,” AIP Conference Proceedings 1895 , 1230008, \\nhttps://doi.org/10.1063/1.5007425  \\nVorobets, A., (2024), “Sequential Entropy Pooling Heuristics,” \\nhttps://ssrn.com/abstract=3936392  or http://dx.doi.org/10.2139/ssrn.3936392  \\nVorobets, A., (202 5), “Portfolio Construction and Risk Management,” \\nhttps://ssrn.com/abstract=4807200  or http://dx.doi.org/10.2139/ssrn.4807200  \\nWahba, G., (2017), “Emanuel Parzen and a Tale of Two Kernels,” Technical Report No. 1183 , University of \\nWisconsin, Madison.  \\nWang, Z, Wu, Y., and Chu, H., (2018), “On equivalence of the LKJ distribution and the restricted Wishart \\ndistribution,” arXiv:1809.04746v1.  \\nWang, B., Xu, F., and Shu, L., (2019), “A Bayesian Approach to Diagnosing Covariance Matrix Shifts,” \\nQuality and Reliability Engineering International , 36(2).  \\nWang, J., Zhu, T., and Zhang, J., (2025), “Test of the Equality of Several High -Dimensional Covariance \\nMatrices: A Normal -Reference Approach,” MDPI: Mathematics , 13, 295.  \\nWelsch, R., and Zhou, X., (2007), “Application of Robust Statistics to Asset Allocation Models,” REVSTAT– \\nStatistical Journal , Volume 5 (1), 97–114. \\nWestfall, P., and Young, S., (1993), Resampling Based Multiple Testing , Wiley Series in Probability and \\nMathematical Statistics, John Wiley & Sons, Inc., New York, New York.  \\nXia, L., Cao, R., Du, J., and Chen, X., (2024), “The Improved Correlation Coefficient of Chatterjee,” Journal \\nof Nonparametric Statistics , pp1-17. \\nYu, P., Li, W., Ng, F., (2014), “Formulating Hypothetical Scenarios in Correlation Stress Testing via a \\nBayesian Framework,” The North Amer. J. of Econ. and Finance , Vol 27, 17 -33. \\nJD Opdyke, Chief Analytics Officer                  Page 88 of 88                      Beating the Correlation Breakdown  \\n Yu, S., Alesiani, F., Yu, X., Jenssen, R., and Principe, J., (2021), “Measuring Dependence with Matrix -based \\nEntropy Functional,” The Thirty -Fifth AAAI Conference on Artificial Intelligence (AAAI -21). \\nZar, J., (1999), Biostatistical Analysis , 4th Ed., Prentice Hall, New Jersey.  \\nZhang, Q., (2023), “On relationships between Chatterjee’s and Spearman’s correlation coefficients,” \\narXiv:2302.10131v1 [stat.ME]  \\nZhang, Y., (2022), “Angle Based Dependence Measures in Metric Space,” arXiv:2206.01459v1 [stat.ME].  \\nZhang, Y., and Yang, S., (2023), “Kernel Angle Dependence Measures for Complex Objects,” \\narXiv:2206.01459v2  \\nZhang, W., Leng, C., and Tang, Y., (2015), “A Joint Modeling Approach for Longitudinal Studies,” Journal of \\nthe Royal Stat. Society, Series B , 77(1), 219 -238.  \\nZhang, Y., Tao, J., Yin, Z., and Wang, G., (2022), “Improved Large Covariance Matrix Estimation Based on \\nEfficient Convex Combination and Its Application in Portfolio Optimization,” Mathematics , 10(22), 4282.  \\nZhang, Y., Tao, J., Lv, Y., and Wang, G., (2023), “An Improved DCC Model Based on Large -Dimensional \\nCovariance Matrices Estimation and Its Applications,” Symmetry , 15, 953.  \\nZhang, G., Jiang, D., and Yao, F., (2024), “Covariance Test and Universal Bootstrap by Operator Norm,” \\narXiv:2412.20019v1 [math.ST].  \\nZhangshuang, S., Gao, X., Luo, K., Bai, Y., Tao, J., and Wang, G., (2025), “Enhancing High -Dimensional \\nDynamic Conditional Angular Correlation Model Based on GARCH Family Models: Comparative \\nPerformance Analysis for Portfolio Optimization,” Finance Research Letters , 106808.  \\nZhao, T., Roeder, K., and Liu, H., (2014), “ Positive Semidefinite Rank -based Correlation Matrix Estimation \\nwith Application to Semiparametric Graph Estimation ,” Journal of Computational Graphics and Statistics , \\n23(4):895 –922.',\n",
       "  'structured_summary': {'objective': 'To develop a method for defining finite-sample distributions of dependence measures for financial portfolio analysis.',\n",
       "   'methods': ['Developed Nonparametric Angles-based Correlation (NAbC)',\n",
       "    'Ensured NAbC is valid under real-world data conditions',\n",
       "    'Applied NAbC to any positive definite dependence measure',\n",
       "    'Provided valid confidence intervals and p-values at matrix and cell levels',\n",
       "    'Enabled scenario-restricted analyses with frozen cells',\n",
       "    'Maintained validity for finite sample sizes'],\n",
       "   'results': ['NAbC allows for valid statistical inference under challenging conditions',\n",
       "    'Facilitated ceteris paribus comparisons across different dependence measures',\n",
       "    'Provided a unified method for defining finite-sample distributions',\n",
       "    'Enhanced robustness and accuracy in risk analysis and scenario testing',\n",
       "    'Demonstrated applicability to a wide range of dependence measures'],\n",
       "   'conclusions': ['NAbC addresses gaps in existing literature regarding dependence measures',\n",
       "    'Offers a comprehensive solution for real-world financial portfolio analysis',\n",
       "    'Enables rigorous inferential analyses without unrealistic assumptions',\n",
       "    'Improves decision-making and risk mitigation strategies in finance']},\n",
       "  'comprehensive_summary': \"JD Opdyke's paper introduces a novel method called Nonparametric Angles-based Correlation (NAbC) aimed at enhancing the analysis of financial portfolios by accurately modeling dependence structures among assets. The core objective is to define finite-sample distributions for various dependence measures, such as Pearson’s and Kendall’s correlations, under real-world data conditions, which are often complex and non-standard.\\n\\nThe methodology involves a robust statistical framework that remains valid across different estimators and allows for flexible scenario analysis, including stress testing. NAbC provides valid p-values and confidence intervals at both the matrix and individual cell levels, enabling granular attribution analyses and comparisons of dependence structures across different sectors.\\n\\nKey findings reveal that NAbC significantly improves the robustness and flexibility of correlation assessments, particularly during market upheavals when correlation breakdowns are most critical. The method facilitates accurate scenario modeling, allowing practitioners to freeze certain correlations while analyzing others, thus providing a comprehensive view of potential risks.\\n\\nPotential trading applications include enhanced portfolio construction and risk management strategies, enabling traders to better anticipate and mitigate the impacts of correlation breakdowns during volatile market conditions. By integrating NAbC into their analytical toolkit, quant traders can achieve more reliable and actionable insights for decision-making.\"},\n",
       " 'last_updated': datetime.datetime(2025, 4, 22, 2, 52, 36, 684234)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_papers[0].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StrategyOutput(python_code=\"import pandas as pd\\nimport numpy as np\\n\\n# Placeholder function for NAbC calculation\\ndef calculate_nabc(returns):\\n    # Implement NAbC logic here based on [ID: 2504.15268v1]\\n    pass\\n\\n# Load historical price data\\nasset_data = pd.read_csv('historical_prices.csv')\\n\\n# Calculate daily returns\\nreturns = asset_data.pct_change().dropna()\\n\\n# Compute NAbC matrix\\nnabc_matrix = calculate_nabc(returns)\\n\\n# Identify assets with high momentum\\npositive_momentum = nabc_matrix[nabc_matrix > 0.5]\\nnegative_momentum = nabc_matrix[nabc_matrix < -0.5]\\n\\n# Generate signals\\nbuy_signals = positive_momentum.columns.tolist()\\nsell_signals = negative_momentum.columns.tolist()\\n\\n# Execute trades (placeholder)\\nfor asset in buy_signals:\\n    # Execute buy order for asset\\n    print(f'Buying {asset}')\\nfor asset in sell_signals:\\n    # Execute sell order for asset\\n    print(f'Selling {asset}')\\n\\n# Note: This code is illustrative and not production-ready.\", pseudocode=\"['1. Load historical price data for a set of assets.', '2. Calculate daily returns for each asset.', '3. Compute the Nonparametric Angles-based Correlation (NAbC) matrix for the returns.', '4. Identify assets with high positive momentum (threshold > 0.5) and high negative momentum (threshold < -0.5).', '5. For each identified asset with positive momentum, generate a buy signal.', '6. For each identified asset with negative momentum, generate a sell signal.', '7. Execute trades based on the generated signals.', '8. Monitor performance and adjust thresholds as necessary.']\", strategy_description='This momentum-based trading strategy leverages the Nonparametric Angles-based Correlation (NAbC) framework to identify stocks that exhibit strong upward or downward trends. By analyzing the dependence structure of asset returns, the strategy aims to capture momentum effects in the market, as highlighted in paper [ID: 2504.15268v1]. The core logic is to buy assets with strong positive momentum (high correlation with previous returns) and sell (or short) assets with strong negative momentum (high negative correlation). This approach allows for robust inference and scenario analysis under real-world conditions, enhancing decision-making in momentum trading.', how_to_use='This strategy requires historical price data for the assets you wish to trade, ideally in a CSV format with date and price columns. Key parameters to tune include the momentum thresholds for buying and selling, which can be adjusted based on backtesting results. Assumptions include that the market conditions remain stable and that the NAbC method accurately captures the dependence structure. Limitations include potential overfitting to historical data and the risk of false signals in volatile markets. Regular performance monitoring and adjustments to the strategy are recommended.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy = await openai_service.generate_strategy(\n",
    "    strategy_prompt=\"\"\"\n",
    "    I want to create a momentum based trading strategy.\n",
    "    \"\"\",\n",
    "    papers=processed_papers\n",
    ")\n",
    "strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python_code': \"import pandas as pd\\nimport numpy as np\\n\\n# Placeholder function for NAbC calculation\\ndef calculate_nabc(returns):\\n    # Implement NAbC logic here based on [ID: 2504.15268v1]\\n    pass\\n\\n# Load historical price data\\nasset_data = pd.read_csv('historical_prices.csv')\\n\\n# Calculate daily returns\\nreturns = asset_data.pct_change().dropna()\\n\\n# Compute NAbC matrix\\nnabc_matrix = calculate_nabc(returns)\\n\\n# Identify assets with high momentum\\npositive_momentum = nabc_matrix[nabc_matrix > 0.5]\\nnegative_momentum = nabc_matrix[nabc_matrix < -0.5]\\n\\n# Generate signals\\nbuy_signals = positive_momentum.columns.tolist()\\nsell_signals = negative_momentum.columns.tolist()\\n\\n# Execute trades (placeholder)\\nfor asset in buy_signals:\\n    # Execute buy order for asset\\n    print(f'Buying {asset}')\\nfor asset in sell_signals:\\n    # Execute sell order for asset\\n    print(f'Selling {asset}')\\n\\n# Note: This code is illustrative and not production-ready.\", 'pseudocode': \"['1. Load historical price data for a set of assets.', '2. Calculate daily returns for each asset.', '3. Compute the Nonparametric Angles-based Correlation (NAbC) matrix for the returns.', '4. Identify assets with high positive momentum (threshold > 0.5) and high negative momentum (threshold < -0.5).', '5. For each identified asset with positive momentum, generate a buy signal.', '6. For each identified asset with negative momentum, generate a sell signal.', '7. Execute trades based on the generated signals.', '8. Monitor performance and adjust thresholds as necessary.']\", 'strategy_description': 'This momentum-based trading strategy leverages the Nonparametric Angles-based Correlation (NAbC) framework to identify stocks that exhibit strong upward or downward trends. By analyzing the dependence structure of asset returns, the strategy aims to capture momentum effects in the market, as highlighted in paper [ID: 2504.15268v1]. The core logic is to buy assets with strong positive momentum (high correlation with previous returns) and sell (or short) assets with strong negative momentum (high negative correlation). This approach allows for robust inference and scenario analysis under real-world conditions, enhancing decision-making in momentum trading.', 'how_to_use': 'This strategy requires historical price data for the assets you wish to trade, ideally in a CSV format with date and price columns. Key parameters to tune include the momentum thresholds for buying and selling, which can be adjusted based on backtesting results. Assumptions include that the market conditions remain stable and that the NAbC method accurately captures the dependence structure. Limitations include potential overfitting to historical data and the risk of false signals in volatile markets. Regular performance monitoring and adjustments to the strategy are recommended.'}\n"
     ]
    }
   ],
   "source": [
    "print(strategy.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
