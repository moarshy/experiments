{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual API endpoint tests for notebook use\n",
    "\n",
    "import httpx\n",
    "import json\n",
    "from IPython.display import display, JSON\n",
    "\n",
    "# Configuration - change this to match your API URL\n",
    "API_BASE_URL = \"http://localhost:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing search endpoint with query: 'momentum trading'\n",
      "Status code: 200\n",
      "Found 5 papers. First paper ID: http://arxiv.org/abs/2504.10789v1\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Search Papers\n",
    "async def test_search_papers(query=\"reinforcement learning trading\", max_results=5):\n",
    "    \"\"\"Test the search endpoint\"\"\"\n",
    "    print(f\"Testing search endpoint with query: '{query}'\")\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        response = await client.get(\n",
    "            f\"{API_BASE_URL}/search/\",\n",
    "            params={\"query\": query, \"max_results\": max_results}\n",
    "        )\n",
    "        \n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        response_data = response.json()\n",
    "        \n",
    "        # Display first paper details\n",
    "        if response.status_code == 200 and \"papers\" in response_data and len(response_data[\"papers\"]) > 0:\n",
    "            paper_id = response_data[\"papers\"][0][\"id\"]\n",
    "            print(f\"Found {len(response_data['papers'])} papers. First paper ID: {paper_id}\")\n",
    "            \n",
    "            # Return the paper ID for use in other tests\n",
    "            return paper_id, response_data\n",
    "        else:\n",
    "            print(\"No papers found or error in response\")\n",
    "            return None, response_data\n",
    "        \n",
    "results = await test_search_papers(query=\"momentum trading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http://arxiv.org/abs/2504.10789v1',\n",
       " {'papers': [{'id': 'http://arxiv.org/abs/2504.10789v1',\n",
       "    'title': 'Can Large Language Models Trade? Testing Financial Theories with LLM Agents in Market Simulations',\n",
       "    'authors': ['Alejandro Lopez-Lira'],\n",
       "    'summary': \"This paper presents a realistic simulated stock market where large language\\nmodels (LLMs) act as heterogeneous competing trading agents. The open-source\\nframework incorporates a persistent order book with market and limit orders,\\npartial fills, dividends, and equilibrium clearing alongside agents with varied\\nstrategies, information sets, and endowments. Agents submit standardized\\ndecisions using structured outputs and function calls while expressing their\\nreasoning in natural language. Three findings emerge: First, LLMs demonstrate\\nconsistent strategy adherence and can function as value investors, momentum\\ntraders, or market makers per their instructions. Second, market dynamics\\nexhibit features of real financial markets, including price discovery, bubbles,\\nunderreaction, and strategic liquidity provision. Third, the framework enables\\nanalysis of LLMs' responses to varying market conditions, similar to partial\\ndependence plots in machine-learning interpretability. The framework allows\\nsimulating financial theories without closed-form solutions, creating\\nexperimental designs that would be costly with human participants, and\\nestablishing how prompts can generate correlated behaviors affecting market\\nstability.\",\n",
       "    'published': '2025-04-15T01:18:36Z',\n",
       "    'link': 'http://arxiv.org/abs/2504.10789v1',\n",
       "    'category': 'q-fin.CP',\n",
       "    'pdf_url': 'http://arxiv.org/pdf/2504.10789v1'},\n",
       "   {'id': 'http://arxiv.org/abs/2504.06289v1',\n",
       "    'title': 'On the Efficacy of Shorting Corporate Bonds as a Tail Risk Hedging Solution',\n",
       "    'authors': ['Travis Cable',\n",
       "     'Amir Mani',\n",
       "     'Wei Qi',\n",
       "     'Georgios Sotiropoulos',\n",
       "     'Yiyuan Xiong'],\n",
       "    'summary': \"United States (US) IG bonds typically trade at modest spreads over US\\nTreasuries, reflecting the credit risk tied to a corporation's default\\npotential. During market crises, IG spreads often widen and liquidity tends to\\ndecrease, likely due to increased credit risk (evidenced by higher IG Credit\\nDefault Index spreads) and the necessity for asset holders like mutual funds to\\nliquidate assets, including IG credits, to manage margin calls, bolster cash\\nreserves, or meet redemptions. These credit and liquidity premia occur during\\nmarket drawdowns and tend to move non-linearly with the market. The research\\nherein refers to this non-linearity (during periods of drawdown) as downside\\nconvexity, and shows that this market behavior can effectively be captured\\nthrough a short position established in IG Exchange Traded Funds (ETFs).\\n  The following document details the construction of three signals: Momentum,\\nLiquidity, and Credit, that can be used in combination to signal entries and\\nexits into short IG positions to hedge a typical active bond portfolio (such as\\nPIMIX). A dynamic hedge initiates the short when signals jointly correlate and\\npoint to significant future hedged return. The dynamic hedge removes when the\\nshort position's predicted hedged return begins to mean revert. This systematic\\nhedge largely avoids IG Credit drawdowns, lowers absolute and downside risk,\\nincreases annualised returns and achieves higher Sortino ratios compared to the\\nbenchmark funds. The method is best suited to high carry, high active risk\\nfunds like PIMIX, though it also generalises to more conservative funds similar\\nto DODIX.\",\n",
       "    'published': '2025-04-03T20:33:30Z',\n",
       "    'link': 'http://arxiv.org/abs/2504.06289v1',\n",
       "    'category': 'q-fin.PM',\n",
       "    'pdf_url': 'http://arxiv.org/pdf/2504.06289v1'},\n",
       "   {'id': 'http://arxiv.org/abs/2503.09647v5',\n",
       "    'title': 'Leveraging LLMS for Top-Down Sector Allocation In Automated Trading',\n",
       "    'authors': ['Ryan Quek Wei Heng',\n",
       "     'Edoardo Vittori',\n",
       "     'Keane Ong',\n",
       "     'Rui Mao',\n",
       "     'Erik Cambria',\n",
       "     'Gianmarco Mengaldo'],\n",
       "    'summary': 'This paper introduces a methodology leveraging Large Language Models (LLMs)\\nfor sector-level portfolio allocation through systematic analysis of\\nmacroeconomic conditions and market sentiment. Our framework emphasizes\\ntop-down sector allocation by processing multiple data streams simultaneously,\\nincluding policy documents, economic indicators, and sentiment patterns.\\nEmpirical results demonstrate superior risk-adjusted returns compared to\\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\\nsuggest that LLM-based systematic macro analysis presents a viable approach for\\nenhancing automated portfolio allocation decisions at the sector level.',\n",
       "    'published': '2025-03-12T08:41:36Z',\n",
       "    'link': 'http://arxiv.org/abs/2503.09647v5',\n",
       "    'category': 'cs.CE',\n",
       "    'pdf_url': 'http://arxiv.org/pdf/2503.09647v5'},\n",
       "   {'id': 'http://arxiv.org/abs/2502.19349v3',\n",
       "    'title': 'CryptoPulse: Short-Term Cryptocurrency Forecasting with Dual-Prediction and Cross-Correlated Market Indicators',\n",
       "    'authors': ['Amit Kumar', 'Taoran Ji'],\n",
       "    'summary': \"Cryptocurrencies fluctuate in markets with high price volatility, posing\\nsignificant challenges for investors. To aid in informed decision-making,\\nsystems predicting cryptocurrency market movements have been developed,\\ntypically focusing on historical patterns. However, these methods often\\noverlook three critical factors influencing market dynamics: 1) the macro\\ninvesting environment, reflected in major cryptocurrency fluctuations affecting\\ncollaborative investor behaviors; 2) overall market sentiment, heavily\\ninfluenced by news impacting investor strategies; and 3) technical indicators,\\noffering insights into overbought or oversold conditions, momentum, and market\\ntrends, which are crucial for short-term price movements. This paper proposes a\\ndual prediction mechanism that forecasts the next day's closing price by\\nincorporating macroeconomic fluctuations, technical indicators, and individual\\ncryptocurrency price changes. Additionally, a novel refinement mechanism\\nenhances predictions through market sentiment-based rescaling and fusion.\\nExperiments demonstrate that the proposed model achieves state-of-the-art\\nperformance, consistently outperforming ten comparison methods.\",\n",
       "    'published': '2025-02-26T17:45:01Z',\n",
       "    'link': 'http://arxiv.org/abs/2502.19349v3',\n",
       "    'category': 'cs.LG',\n",
       "    'pdf_url': 'http://arxiv.org/pdf/2502.19349v3'},\n",
       "   {'id': 'http://arxiv.org/abs/2502.17777v1',\n",
       "    'title': 'Adaptive Nesterov Accelerated Distributional Deep Hedging for Efficient Volatility Risk Management',\n",
       "    'authors': ['Lei Zhao', 'Lin Cai', 'Wu-Sheng Lu'],\n",
       "    'summary': 'In the field of financial derivatives trading, managing volatility risk is\\ncrucial for protecting investment portfolios from market changes. Traditional\\nVega hedging strategies, which often rely on basic and rule-based models, are\\nhard to adapt well to rapidly changing market conditions. We introduce a new\\nframework for dynamic Vega hedging, the Adaptive Nesterov Accelerated\\nDistributional Deep Hedging (ANADDH), which combines distributional\\nreinforcement learning with a tailored design based on adaptive Nesterov\\nacceleration. This approach improves the learning process in complex financial\\nenvironments by modeling the hedging efficiency distribution, providing a more\\naccurate and responsive hedging strategy. The design of adaptive Nesterov\\nacceleration refines gradient momentum adjustments, significantly enhancing the\\nstability and speed of convergence of the model. Through empirical analysis and\\ncomparisons, our method demonstrates substantial performance gains over\\nexisting hedging techniques. Our results confirm that this innovative\\ncombination of distributional reinforcement learning with the proposed\\noptimization techniques improves financial risk management and highlights the\\npractical benefits of implementing advanced neural network architectures in the\\nfinance sector.',\n",
       "    'published': '2025-02-25T02:12:16Z',\n",
       "    'link': 'http://arxiv.org/abs/2502.17777v1',\n",
       "    'category': 'cs.LG',\n",
       "    'pdf_url': 'http://arxiv.org/pdf/2502.17777v1'}],\n",
       "  'total_count': 5,\n",
       "  'query': 'momentum trading'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing get paper endpoint for ID: 2204.11824\n",
      "Status code: 200\n",
      "Paper title: Semi-Parametric Neural Image Synthesis\n",
      "Authors: Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, Björn Ommer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'http://arxiv.org/abs/2204.11824v3',\n",
       " 'title': 'Semi-Parametric Neural Image Synthesis',\n",
       " 'authors': ['Andreas Blattmann',\n",
       "  'Robin Rombach',\n",
       "  'Kaan Oktay',\n",
       "  'Jonas Müller',\n",
       "  'Björn Ommer'],\n",
       " 'summary': 'Novel architectures have recently improved generative image synthesis leading\\nto excellent visual quality in various tasks. Much of this success is due to\\nthe scalability of these architectures and hence caused by a dramatic increase\\nin model complexity and in the computational resources invested in training\\nthese models. Our work questions the underlying paradigm of compressing large\\ntraining data into ever growing parametric representations. We rather present\\nan orthogonal, semi-parametric approach. We complement comparably small\\ndiffusion or autoregressive models with a separate image database and a\\nretrieval strategy. During training we retrieve a set of nearest neighbors from\\nthis external database for each training instance and condition the generative\\nmodel on these informative samples. While the retrieval approach is providing\\nthe (local) content, the model is focusing on learning the composition of\\nscenes based on this content. As demonstrated by our experiments, simply\\nswapping the database for one with different contents transfers a trained model\\npost-hoc to a novel domain. The evaluation shows competitive performance on\\ntasks which the generative model has not been trained on, such as\\nclass-conditional synthesis, zero-shot stylization or text-to-image synthesis\\nwithout requiring paired text-image data. With negligible memory and\\ncomputational overhead for the external database and retrieval we can\\nsignificantly reduce the parameter count of the generative model and still\\noutperform the state-of-the-art.',\n",
       " 'published': '2022-04-25T17:55:26Z',\n",
       " 'link': 'http://arxiv.org/abs/2204.11824v3',\n",
       " 'category': 'cs.CV',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2204.11824v3'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 2: Get Paper by ID\n",
    "async def test_get_paper(paper_id):\n",
    "    \"\"\"Test getting a specific paper by ID\"\"\"\n",
    "    print(f\"Testing get paper endpoint for ID: {paper_id}\")\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        response = await client.get(f\"{API_BASE_URL}/search/{paper_id}\")\n",
    "        \n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            paper = response.json()\n",
    "            print(f\"Paper title: {paper.get('title', 'N/A')}\")\n",
    "            print(f\"Authors: {', '.join(paper.get('authors', []))}\")\n",
    "            return paper\n",
    "        else:\n",
    "            print(f\"Error fetching paper: {response.text}\")\n",
    "            return None\n",
    "        \n",
    "results = await test_get_paper(paper_id=\"2204.11824\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing structure endpoint for paper ID: 2204.11824\n",
      "Status code: 200\n",
      "Paper structure sections:\n",
      "- objective: This paper aims to challenge the traditional appro...\n",
      "- methods: ['Semi-parametric approach combining diffusion or autoregressive models with an external image database', 'Nearest neighbor retrieval strategy for conditioning the generative model during training', 'Post-hoc model transfer by swapping the external database for different contents', 'Evaluation of performance on tasks not explicitly trained on, such as class-conditional synthesis and zero-shot stylization']...\n",
      "- results: ['Achieved competitive performance on novel tasks without requiring paired text-image data', 'Significantly reduced parameter count of the generative model while outperforming state-of-the-art methods', 'Demonstrated effective transfer of trained models to new domains with different content databases', 'Minimal memory and computational overhead associated with the external database and retrieval process']...\n",
      "- conclusions: ['The semi-parametric approach allows for greater flexibility and efficiency in generative modeling.', 'Retrieval strategies can enhance the performance of generative models without the need for extensive training data.', 'The ability to transfer models to new domains post-hoc opens up new avenues for application in various generative tasks.', 'Reducing model complexity while maintaining performance can lead to more accessible and scalable generative solutions.']...\n",
      "- trading_applications: ['Utilizing generative models for creating synthetic financial data to augment training datasets for trading algorithms.', 'Implementing retrieval strategies to adapt trading models to new market conditions or asset classes without extensive retraining.', 'Applying zero-shot learning techniques to generate trading signals based on unstructured data inputs, such as news articles or social media sentiment.', 'Leveraging the ability to transfer models to new domains for developing adaptive trading strategies that respond to changing market dynamics.']...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective': 'This paper aims to challenge the traditional approach of compressing large training data into complex parametric models by proposing a semi-parametric method that integrates a retrieval strategy with generative models. The goal is to enhance generative image synthesis while reducing model complexity and computational resources.',\n",
       " 'methods': ['Semi-parametric approach combining diffusion or autoregressive models with an external image database',\n",
       "  'Nearest neighbor retrieval strategy for conditioning the generative model during training',\n",
       "  'Post-hoc model transfer by swapping the external database for different contents',\n",
       "  'Evaluation of performance on tasks not explicitly trained on, such as class-conditional synthesis and zero-shot stylization'],\n",
       " 'results': ['Achieved competitive performance on novel tasks without requiring paired text-image data',\n",
       "  'Significantly reduced parameter count of the generative model while outperforming state-of-the-art methods',\n",
       "  'Demonstrated effective transfer of trained models to new domains with different content databases',\n",
       "  'Minimal memory and computational overhead associated with the external database and retrieval process'],\n",
       " 'conclusions': ['The semi-parametric approach allows for greater flexibility and efficiency in generative modeling.',\n",
       "  'Retrieval strategies can enhance the performance of generative models without the need for extensive training data.',\n",
       "  'The ability to transfer models to new domains post-hoc opens up new avenues for application in various generative tasks.',\n",
       "  'Reducing model complexity while maintaining performance can lead to more accessible and scalable generative solutions.'],\n",
       " 'trading_applications': ['Utilizing generative models for creating synthetic financial data to augment training datasets for trading algorithms.',\n",
       "  'Implementing retrieval strategies to adapt trading models to new market conditions or asset classes without extensive retraining.',\n",
       "  'Applying zero-shot learning techniques to generate trading signals based on unstructured data inputs, such as news articles or social media sentiment.',\n",
       "  'Leveraging the ability to transfer models to new domains for developing adaptive trading strategies that respond to changing market dynamics.']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def test_paper_structure(paper_id):\n",
    "    \"\"\"Test extracting paper structure\"\"\"\n",
    "    print(f\"Testing structure endpoint for paper ID: {paper_id}\")\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "        response = await client.get(f\"{API_BASE_URL}/structure/{paper_id}\")\n",
    "        \n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            structure = response.json()\n",
    "            print(\"Paper structure sections:\")\n",
    "            for section, content in structure.items():\n",
    "                print(f\"- {section}: {content[:50]}...\")\n",
    "            return structure\n",
    "        else:\n",
    "            print(f\"Error extracting structure: {response.text}\")\n",
    "            return None\n",
    "        \n",
    "results = await test_paper_structure(paper_id=\"2204.11824\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing summary endpoint for paper ID: 2204.11824\n",
      "Status code: 200\n",
      "Summary length: 1117\n",
      "Keywords: ['semi-parametric models', 'generative modeling', 'data retrieval', 'zero-shot learning', 'financial data synthesis', 'adaptive trading strategies', 'market condition adaptation', 'scene composition']\n",
      "Trading relevance: N/A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'paper_id': 'http://arxiv.org/abs/2204.11824v3',\n",
       " 'title': 'Semi-Parametric Neural Image Synthesis',\n",
       " 'sections': {'objective': 'This paper aims to challenge the traditional approach of compressing large training data into complex parametric models by proposing a semi-parametric method that integrates a retrieval strategy with generative models. The goal is to enhance generative image synthesis while reducing model complexity and computational resources.',\n",
       "  'methods': ['Semi-parametric approach combining diffusion or autoregressive models with an external image database',\n",
       "   'Nearest neighbor retrieval strategy for conditioning the generative model during training',\n",
       "   'Post-hoc model transfer by swapping the external database for different contents',\n",
       "   'Evaluation of performance on tasks not explicitly trained on, such as class-conditional synthesis and zero-shot stylization'],\n",
       "  'results': ['Achieved competitive performance on novel tasks without requiring paired text-image data',\n",
       "   'Significantly reduced parameter count of the generative model while outperforming state-of-the-art methods',\n",
       "   'Demonstrated effective transfer of trained models to new domains with different content databases',\n",
       "   'Minimal memory and computational overhead associated with the external database and retrieval process'],\n",
       "  'conclusions': ['The semi-parametric approach allows for greater flexibility and efficiency in generative modeling.',\n",
       "   'Retrieval strategies can enhance the performance of generative models without the need for extensive training data.',\n",
       "   'The ability to transfer models to new domains post-hoc opens up new avenues for application in various generative tasks.',\n",
       "   'Reducing model complexity while maintaining performance can lead to more accessible and scalable generative solutions.'],\n",
       "  'trading_applications': ['Utilizing generative models for creating synthetic financial data to augment training datasets for trading algorithms.',\n",
       "   'Implementing retrieval strategies to adapt trading models to new market conditions or asset classes without extensive retraining.',\n",
       "   'Applying zero-shot learning techniques to generate trading signals based on unstructured data inputs, such as news articles or social media sentiment.',\n",
       "   'Leveraging the ability to transfer models to new domains for developing adaptive trading strategies that respond to changing market dynamics.']},\n",
       " 'summary': 'The paper presents a semi-parametric approach to generative image synthesis that integrates a retrieval strategy with diffusion or autoregressive models. This method challenges the conventional paradigm of compressing large datasets into complex parametric representations, instead utilizing a smaller model complemented by an external image database. By retrieving nearest neighbors during training, the generative model can focus on scene composition while leveraging informative samples for content. The authors demonstrate that this approach not only reduces the parameter count of the model but also allows for effective transfer to new domains by simply swapping the external database. The experiments show competitive performance on tasks such as class-conditional synthesis and zero-shot stylization without requiring paired text-image data. This semi-parametric method offers greater flexibility and efficiency, making it applicable in various generative tasks, including potential applications in trading, where synthetic financial data and adaptive strategies can be developed without extensive retraining.',\n",
       " 'keywords': ['semi-parametric models',\n",
       "  'generative modeling',\n",
       "  'data retrieval',\n",
       "  'zero-shot learning',\n",
       "  'financial data synthesis',\n",
       "  'adaptive trading strategies',\n",
       "  'market condition adaptation',\n",
       "  'scene composition'],\n",
       " 'implementation_complexity': 'Medium',\n",
       " 'data_requirements': ['Historical financial data',\n",
       "  'External image databases for model training',\n",
       "  'Market condition data',\n",
       "  'Unstructured data inputs (e.g., news articles, social media sentiment)'],\n",
       " 'potential_performance': 'Expected to enhance trading performance by providing more robust synthetic data and adaptive strategies that can quickly respond to changing market conditions, potentially leading to improved trading signals and decision-making.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 4: Generate Paper Summary\n",
    "async def test_paper_summary(paper_id):\n",
    "    \"\"\"Test generating paper summary\"\"\"\n",
    "    print(f\"Testing summary endpoint for paper ID: {paper_id}\")\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "        response = await client.get(f\"{API_BASE_URL}/summarize/{paper_id}\")\n",
    "        \n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            summary = response.json()\n",
    "            print(f\"Summary length: {len(summary.get('summary', ''))}\")\n",
    "            print(f\"Keywords: {summary.get('keywords', [])}\")\n",
    "            print(f\"Trading relevance: {summary.get('trading_relevance', 'N/A')}\")\n",
    "            return summary\n",
    "        else:\n",
    "            print(f\"Error generating summary: {response.text}\")\n",
    "            return None\n",
    "\n",
    "results = await test_paper_summary(paper_id=\"2204.11824\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Ask Question\n",
    "async def test_qa(paper_id, question=\"What are the main methods used in this paper?\"):\n",
    "    \"\"\"Test asking a question about a paper\"\"\"\n",
    "    print(f\"Testing QA endpoint with question: '{question}'\")\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "        response = await client.post(\n",
    "            f\"{API_BASE_URL}/qa/\",\n",
    "            json={\"question\": question, \"paper_ids\": [paper_id]}\n",
    "        )\n",
    "        \n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            answer = response.json()\n",
    "            print(f\"Answer: {answer.get('answer', '')[:100]}...\")\n",
    "            print(f\"Confidence: {answer.get('confidence', 0)}\")\n",
    "            return answer\n",
    "        else:\n",
    "            print(f\"Error getting answer: {response.text}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Generate Strategy\n",
    "async def test_strategy(paper_id):\n",
    "    \"\"\"Test generating a trading strategy\"\"\"\n",
    "    print(f\"Testing strategy endpoint for paper ID: {paper_id}\")\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=120.0) as client:\n",
    "        response = await client.post(\n",
    "            f\"{API_BASE_URL}/strategy/\",\n",
    "            json={\n",
    "                \"paper_ids\": [paper_id],\n",
    "                \"market\": \"equities\",\n",
    "                \"timeframe\": \"daily\",\n",
    "                \"risk_profile\": \"moderate\",\n",
    "                \"additional_context\": \"Focus on low volatility stocks\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            strategy = response.json()\n",
    "            print(f\"Strategy name: {strategy.get('strategy_name', 'N/A')}\")\n",
    "            print(f\"Code length: {len(strategy.get('python_code', ''))}\")\n",
    "            return strategy\n",
    "        else:\n",
    "            print(f\"Error generating strategy: {response.text}\")\n",
    "            return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
