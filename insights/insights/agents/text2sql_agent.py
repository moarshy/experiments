import logging
import json
import pandas as pd
from typing import Dict, Any, List, Optional, Union, Tuple
from enum import Enum
from pydantic import BaseModel, Field

# Import Vanna components
from vanna.openai import OpenAI_Chat
from vanna.chromadb import ChromaDB_VectorStore

# Import from your existing modules
from database_summary_agent import DatabaseSummary

logger = logging.getLogger(__name__)

class SQLResult(BaseModel):
    """Model for SQL query execution result."""
    query: str
    success: bool
    error_message: Optional[str] = None
    data: Optional[List[Dict[str, Any]]] = None
    row_count: Optional[int] = None
    column_names: Optional[List[str]] = None
    execution_time: Optional[float] = None
    
class SQLQueryType(str, Enum):
    """Types of SQL queries."""
    SELECT = "select"
    INSERT = "insert"
    UPDATE = "update"
    DELETE = "delete"
    CREATE = "create"
    ALTER = "alter"
    DROP = "drop"
    OTHER = "other"

class VannaAgent(ChromaDB_VectorStore, OpenAI_Chat):
    """Custom Vanna agent that combines ChromaDB and OpenAI."""
    def __init__(self, config=None):
        ChromaDB_VectorStore.__init__(self, config=config)
        OpenAI_Chat.__init__(self, config=config)

class Text2SQLExecuteAgent:
    """
    Combined agent for generating SQL from natural language and executing it.
    
    This agent uses Vanna AI to:
    1. Generate SQL queries from natural language questions
    2. Execute the generated queries against a database
    3. Return the results in a structured format
    """
    
    def __init__(self, 
                 db_config: Dict[str, Any],
                 openai_api_key: str,
                 openai_model: str = "gpt-4o-mini",
                 collection_name: str = "insights_db",
                 persist_directory: str = "./vanna_data"):
        """
        Initialize the Text2SQL and Execute Agent.
        
        Args:
            db_config: Database connection configuration
            openai_api_key: OpenAI API key
            openai_model: OpenAI model to use
            collection_name: Name for the ChromaDB collection
            persist_directory: Directory to persist ChromaDB data
        """
        self.db_config = db_config
        
        # Configure Vanna agent
        self.vanna_config = {
            'api_key': openai_api_key,
            'model': openai_model,
            'collection_name': collection_name,
            'persist_directory': persist_directory
        }
        
        self.vn = VannaAgent(config=self.vanna_config)
        self.is_connected = False
        self.is_trained = False
        
    def connect_to_database(self) -> bool:
        """
        Connect to the database using the provided configuration.
        
        Returns:
            True if connection successful, False otherwise
        """
        try:
            db_type = self.db_config.get('type', 'sqlite')
            
            if db_type == 'sqlite':
                self.vn.connect_to_sqlite(self.db_config.get('database', ':memory:'))
            elif db_type == 'postgres':
                self.vn.connect_to_postgres(
                    host=self.db_config.get('host', 'localhost'),
                    dbname=self.db_config.get('dbname', ''),
                    user=self.db_config.get('user', ''),
                    password=self.db_config.get('password', ''),
                    port=self.db_config.get('port', '5432')
                )
            else:
                logger.error(f"Unsupported database type: {db_type}")
                return False
                
            self.is_connected = True
            logger.info(f"Successfully connected to {db_type} database")
            return True
            
        except Exception as e:
            logger.error(f"Failed to connect to database: {e}")
            self.is_connected = False
            return False
    
    def train_on_database_summary(self, db_summary: DatabaseSummary) -> bool:
        """
        Train Vanna AI on the database structure using the summary from DatabaseSummaryAgent.
        
        Args:
            db_summary: Summary generated by DatabaseSummaryAgent
            
        Returns:
            True if training successful, False otherwise
        """
        if not self.is_connected:
            if not self.connect_to_database():
                return False
                
        try:
            # Generate DDL statements from the database summary
            tables = db_summary.tables
            
            for table in tables:
                # Create a DDL statement for each table
                ddl = self._create_ddl_from_table_summary(table)
                self.vn.train(ddl=ddl)
                
                # Add table documentation if available
                doc = f"Table {table.name} contains {table.row_count} rows."
                if hasattr(table, 'description') and table.description:
                    doc += f" {table.description}"
                self.vn.train(documentation=doc)
                
                # Add column documentation
                for column in table.columns:
                    col_doc = f"Column {column.name} in table {table.name} has data type {column.data_type}."
                    
                    if column.is_primary_key:
                        col_doc += " This is a primary key."
                    if column.is_foreign_key and column.references:
                        col_doc += f" This is a foreign key referencing {column.references}."
                        
                    if hasattr(column, 'description') and column.description:
                        col_doc += f" {column.description}"
                        
                    self.vn.train(documentation=col_doc)
            
            # Add relationship documentation
            for relationship in db_summary.relationships:
                rel_doc = (f"There is a relationship between {relationship['from_table']}.{relationship['from_column']} "
                           f"and {relationship['to_table']}.{relationship['to_column']}.")
                self.vn.train(documentation=rel_doc)
                
            # Get information schema for additional training
            try:
                df_information_schema = self.vn.run_sql("SELECT * FROM INFORMATION_SCHEMA.COLUMNS")
                plan = self.vn.get_training_plan_generic(df_information_schema)
                self.vn.train(plan=plan)
            except Exception as e:
                logger.warning(f"Could not train on information schema: {e}")
                # This is not critical, so we can continue
            
            self.is_trained = True
            logger.info("Successfully trained Vanna AI on database structure")
            return True
            
        except Exception as e:
            logger.error(f"Failed to train Vanna AI: {e}")
            return False
    
    def _create_ddl_from_table_summary(self, table_summary) -> str:
        """
        Create a DDL statement from a table summary.
        
        Args:
            table_summary: Summary of a database table
            
        Returns:
            DDL statement as a string
        """
        ddl = f"CREATE TABLE {table_summary.name} (\n"
        
        # Add columns
        columns = []
        for column in table_summary.columns:
            col_def = f"  {column.name} {column.data_type}"
            
            if column.is_primary_key:
                col_def += " PRIMARY KEY"
            if not column.nullable:
                col_def += " NOT NULL"
                
            columns.append(col_def)
            
        # Add foreign keys
        for fk in table_summary.foreign_keys:
            if 'column' in fk and 'references' in fk:
                fk_def = f"  FOREIGN KEY ({fk['column']}) REFERENCES {fk['references']}"
                columns.append(fk_def)
                
        ddl += ",\n".join(columns)
        ddl += "\n);"
        
        return ddl
    
    def generate_sql(self, question: str, db_context: Optional[str] = None) -> str:
        """
        Generate SQL from a natural language question.
        
        Args:
            question: Natural language question
            db_context: Optional additional context about the database
            
        Returns:
            Generated SQL query
        """
        if not self.is_connected:
            if not self.connect_to_database():
                raise ConnectionError("Not connected to database")
                
        if not self.is_trained:
            logger.warning("Vanna AI has not been trained on the database structure")
            
        # Add context to the question if provided
        enhanced_question = question
        if db_context:
            enhanced_question = f"{db_context}\n\nQuestion: {question}"
            
        # Generate SQL using Vanna AI
        try:
            sql = self.vn.generate_sql(question=enhanced_question)
            logger.info(f"Generated SQL: {sql}")
            return sql
        except Exception as e:
            logger.error(f"Failed to generate SQL: {e}")
            raise
    
    def execute_sql(self, sql: str) -> SQLResult:
        """
        Execute a SQL query and return the results.
        
        Args:
            sql: SQL query to execute
            
        Returns:
            Result of the SQL execution
        """
        import time
        
        if not self.is_connected:
            if not self.connect_to_database():
                return SQLResult(
                    query=sql,
                    success=False,
                    error_message="Not connected to database"
                )
        
        # Identify query type
        query_type = self._identify_query_type(sql)
        
        # Execute the query
        try:
            start_time = time.time()
            
            if query_type == SQLQueryType.SELECT:
                # For SELECT queries, use Vanna's run_sql which returns a pandas DataFrame
                df = self.vn.run_sql(sql)
                end_time = time.time()
                
                # Convert DataFrame to list of dictionaries
                data = df.to_dict(orient='records')
                
                return SQLResult(
                    query=sql,
                    success=True,
                    data=data,
                    row_count=len(data),
                    column_names=list(df.columns),
                    execution_time=end_time - start_time
                )
            else:
                # For non-SELECT queries, use Vanna's execute_sql
                self.vn.run_sql(sql)
                end_time = time.time()
                
                return SQLResult(
                    query=sql,
                    success=True,
                    execution_time=end_time - start_time
                )
                
        except Exception as e:
            logger.error(f"SQL execution error: {e}")
            return SQLResult(
                query=sql,
                success=False,
                error_message=str(e)
            )
    
    def _identify_query_type(self, sql: str) -> SQLQueryType:
        """
        Identify the type of SQL query.
        
        Args:
            sql: SQL query
            
        Returns:
            Type of SQL query
        """
        sql_lower = sql.lower().strip()
        
        if sql_lower.startswith("select"):
            return SQLQueryType.SELECT
        elif sql_lower.startswith("insert"):
            return SQLQueryType.INSERT
        elif sql_lower.startswith("update"):
            return SQLQueryType.UPDATE
        elif sql_lower.startswith("delete"):
            return SQLQueryType.DELETE
        elif sql_lower.startswith("create"):
            return SQLQueryType.CREATE
        elif sql_lower.startswith("alter"):
            return SQLQueryType.ALTER
        elif sql_lower.startswith("drop"):
            return SQLQueryType.DROP
        else:
            return SQLQueryType.OTHER
    
    def process_question(self, question: str, db_context: Optional[str] = None) -> Dict[str, Any]:
        """
        Process a question from start to finish: generate SQL, execute it, and return results.
        
        Args:
            question: Natural language question
            db_context: Optional additional context about the database
            
        Returns:
            Dictionary containing the question, SQL, execution status, and results
        """
        try:
            # Generate SQL
            sql = self.generate_sql(question, db_context)
            
            # Execute SQL
            result = self.execute_sql(sql)
            
            # Return combined result
            return {
                "question": question,
                "sql": sql,
                "execution_result": result.model_dump()
            }
            
        except Exception as e:
            logger.error(f"Error processing question: {e}")
            return {
                "question": question,
                "error": str(e),
                "success": False
            }
    
    def process_analysis_questions(self, 
                                  analysis_questions: List[Dict[str, Any]], 
                                  db_summary: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Process a batch of analysis questions.
        
        Args:
            analysis_questions: List of analysis questions
            db_summary: Database summary
            
        Returns:
            List of results for each question
        """
        results = []
        
        # Extract relevant database context from the summary
        db_context = db_summary.get("natural_language_summary", "")
        
        for question_data in analysis_questions:
            question_text = question_data.get("question_text", "")
            
            # Get related tables and columns to provide as context
            related_tables = question_data.get("related_tables", [])
            related_columns = question_data.get("related_columns", [])
            
            # Build specific context for this question
            specific_context = f"{db_context}\n\n"
            if related_tables:
                specific_context += f"This question relates to the following tables: {', '.join(related_tables)}.\n"
            if related_columns:
                specific_context += f"Consider using these columns: {', '.join(related_columns)}.\n"
                
            # Process the question
            result = self.process_question(question_text, specific_context)
            
            # Add question metadata to the result
            result["question_id"] = question_data.get("question_id", "")
            result["category"] = question_data.get("category", "")
            result["relevance_score"] = question_data.get("relevance_score", 0.0)
            
            results.append(result)
            
        return results

def main(db_config_path: str, db_summary_path: str, questions_path: str, output_path: str, openai_api_key: str):
    """
    Entry point function to run the Text2SQL and Execute Agent.
    
    Args:
        db_config_path: Path to database configuration file
        db_summary_path: Path to database summary file
        questions_path: Path to analysis questions file
        output_path: Path to save results
        openai_api_key: OpenAI API key
    """
    # Load database configuration
    with open(db_config_path, 'r') as f:
        db_config = json.load(f)
    
    # Load database summary
    with open(db_summary_path, 'r') as f:
        db_summary = json.load(f)
    
    # Load analysis questions
    with open(questions_path, 'r') as f:
        questions_data = json.load(f)
        analysis_questions = questions_data.get("questions", [])
    
    # Create and initialize the agent
    agent = Text2SQLExecuteAgent(
        db_config=db_config,
        openai_api_key=openai_api_key
    )
    
    # Connect to the database
    if not agent.connect_to_database():
        logger.error("Failed to connect to database. Exiting.")
        return
    
    # Train on database structure
    if not agent.train_on_database_summary(db_summary):
        logger.warning("Training incomplete. Proceeding with limited knowledge.")
    
    # Process all questions
    results = agent.process_analysis_questions(analysis_questions, db_summary)
    
    # Save results
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Processed {len(results)} questions. Results saved to {output_path}")

if __name__ == "__main__":
    import sys
    import os
    
    if len(sys.argv) != 6:
        print("Usage: python text2sql_execute_agent.py <db_config_path> <db_summary_path> <questions_path> <output_path> <openai_api_key>")
        sys.exit(1)
    
    db_config_path = sys.argv[1]
    db_summary_path = sys.argv[2]
    questions_path = sys.argv[3]
    output_path = sys.argv[4]
    openai_api_key = sys.argv[5]
    
    main(db_config_path, db_summary_path, questions_path, output_path, openai_api_key)